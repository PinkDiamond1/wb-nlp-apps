{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sudo apt-get install default-jdk\n",
    "sudo apt-get install ant\n",
    "git clone git@github.com:mimno/Mallet.git\n",
    "cd Mallet/\n",
    "ant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import gc\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# LdaMallet\n",
    "# Dictionary (gensim)\n",
    "# build_docs\n",
    "# transform_dt\n",
    "# get_tw\n",
    "# get_top_words\n",
    "%run ./LDAModule.ipynb\n",
    "\n",
    "# DocsManager\n",
    "# build_docs\n",
    "%run ../../DocsManager.ipynb\n",
    "## Jupyter.notebook.save_checkpoint()\n",
    "\n",
    "# get_corpus_path\n",
    "# get_txt_clean_path\n",
    "%run ../../path_manager.ipynb\n",
    "\n",
    "# CorpusCleaner\n",
    "%run ../../DataCleanerModule.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.wrappers.ldamallet import malletmodel2ldamodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/wb536061/wb-nlp/CORPUS/IMF'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_corpus_path('IMF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_PATH = get_models_path('LDA').replace('MODELS', 'MODELS.pos')\n",
    "MALLET_BINARY_PATH = \"../Mallet/bin/mallet\"\n",
    "\n",
    "NUM_WORKERS = get_workers()\n",
    "NUM_ITERS = 196\n",
    "MIN_TOKEN_COUNT = 50\n",
    "NGRAM_FILE =  '../../whitelists/whitelist_ngrams_cleaned.csv'  # '../../whitelists/whitelist_ngrams_truncated_cleaned.csv'\n",
    "DOC_PROCESSING_WORKERS = 2 * max(1, os.cpu_count() - 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/wb536061/wb-nlp/MODELS.pos/LDA'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODELS_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(MODELS_PATH):\n",
    "    os.makedirs(MODELS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import gc\n",
    "\n",
    "TRAINING_MODEL_ID = 'LDA'\n",
    "\n",
    "logging.basicConfig(filename=f'./{TRAINING_MODEL_ID.lower()}-iters_{NUM_ITERS}.log', format='%(asctime)s - %(levelname)s - %(message)s', level=logging.INFO)\n",
    "\n",
    "logger = logging.getLogger(f'{TRAINING_MODEL_ID.lower()}-logger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(203876, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wb536061/anaconda3/envs/wb-nlp/lib/python3.7/site-packages/ipykernel_launcher.py:90: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "/home/wb536061/anaconda3/envs/wb-nlp/lib/python3.7/site-packages/ipykernel_launcher.py:90: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    }
   ],
   "source": [
    "for CORPUS_ID in ['WB']:\n",
    "    if CORPUS_ID == 'WB':\n",
    "        region_partitions = []  # ['AFR', 'EAP', 'ECA', 'LAC', 'MENA', 'RoW', 'SAR', 'WLD']\n",
    "        doctype_partitions = ['PD', 'PR']  # ['BD', 'CF', 'ESW', 'PD', 'PR']\n",
    "        doctype_partitions = doctype_partitions[::-1]\n",
    "        corpus_partitions = ['ALL'] + doctype_partitions + region_partitions\n",
    "    else:\n",
    "        corpus_partitions = ['ALL']\n",
    "\n",
    "    num_topics = [25, 50, 100, 200]\n",
    "\n",
    "    for CORPUS_PART in corpus_partitions:\n",
    "        docs = build_docs(\n",
    "            metadata_filename=os.path.join(get_corpus_path(CORPUS_ID), f'{CORPUS_ID.lower()}_pos_metadata_complete.csv'),\n",
    "            cleaned_files_dir=get_txt_clean_pos_path(CORPUS_ID),\n",
    "            model_output_dir=MODELS_PATH  # Use flat directory as discussed...\n",
    "        )\n",
    "\n",
    "        logger.info(f'Creating partitioned docs and loading files for {CORPUS_ID}-{CORPUS_PART}...')\n",
    "\n",
    "        # docs.set_ngram_mapper('../../whitelists/whitelist_ngrams_cleaned.csv', cleaner=None)\n",
    "        docs.set_ngram_mapper(NGRAM_FILE, cleaner=None)\n",
    "        \n",
    "        docs.set_min_token_count(MIN_TOKEN_COUNT)\n",
    "        docs_filtered, meta = docs.filter_doclist(CORPUS_PART, corpus_id=CORPUS_ID, save=True, return_meta=True, pool_workers=DOC_PROCESSING_WORKERS)\n",
    "\n",
    "        print(docs_filtered.shape)\n",
    "        logger.info(f'Building model for {docs_filtered.shape[0]} documents...')\n",
    "        if docs_filtered.empty:\n",
    "            continue\n",
    "\n",
    "        logger.info('Building dictionary...')\n",
    "        g_dict = Dictionary(docs_filtered.text.str.split())\n",
    "        g_dict.filter_extremes(no_below=10, no_above=0.99, keep_n=200000, keep_tokens=None)  # Exclude words appearing in less than 10 docs.\n",
    "        g_dict.id2token = {id: token for token, id in g_dict.token2id.items()}\n",
    "        \n",
    "        logger.info('Performing doc2bow...')\n",
    "        # corpus = [g_dict.doc2bow(c.split()) for c in docs_filtered.text]\n",
    "\n",
    "        with mp.Pool(NUM_WORKERS) as pool:\n",
    "            # pool = mp.Pool(processes=10)\n",
    "            logger.info('Performing parallel doc2bow...')\n",
    "            corpus = pool.map(g_dict.doc2bow, docs_filtered.text.str.split())\n",
    "            logger.info('Completed parallel doc2bow...')\n",
    "            # pool.close()\n",
    "            # pool.join()\n",
    "\n",
    "        for NUM_TOPICS in num_topics:\n",
    "\n",
    "            MODEL_ID = f\"{CORPUS_PART}_{NUM_TOPICS}\"\n",
    "            logger.info(f'Starting process for {MODEL_ID}...')\n",
    "\n",
    "            MODEL_FOLDER = os.path.join(MODELS_PATH, f'{CORPUS_ID}-{MODEL_ID}')\n",
    "\n",
    "            MODEL_DATA_FOLDER = os.path.join(MODEL_FOLDER, 'data')\n",
    "            MODEL_MALLET_FOLDER = os.path.join(MODEL_FOLDER, 'mallet')\n",
    "\n",
    "            if not os.path.isdir(MODEL_DATA_FOLDER):\n",
    "                os.makedirs(MODEL_DATA_FOLDER)\n",
    "\n",
    "            if not os.path.isdir(MODEL_MALLET_FOLDER):\n",
    "                os.makedirs(MODEL_MALLET_FOLDER)\n",
    "\n",
    "            # Set logging\n",
    "            lfh = logging.FileHandler(f'./{CORPUS_ID.lower()}-iters_{NUM_ITERS}-{MODEL_ID}.log')\n",
    "            lfh.setLevel(logging.INFO)\n",
    "            formatter = logging.Formatter('%(asctime)s %(levelname)-8s %(message)s')\n",
    "            lfh.setFormatter(formatter)\n",
    "            logger.addHandler(lfh)\n",
    "            # End logging setup\n",
    "\n",
    "            logger.info('Training mallet LDA model...')\n",
    "            model = LdaMallet(\n",
    "                MALLET_BINARY_PATH, corpus=corpus, num_topics=NUM_TOPICS, prefix=f'{MODEL_MALLET_FOLDER}/{CORPUS_ID}-{MODEL_ID}_',\n",
    "                id2word=g_dict.id2token, workers=NUM_WORKERS, iterations=NUM_ITERS,\n",
    "                random_seed=1029\n",
    "            )\n",
    "            logger.info('Completed training mallet LDA model...')\n",
    "\n",
    "            dt = pd.read_csv(\n",
    "                model.fdoctopics(), delimiter='\\t', header=None,\n",
    "                names=[i for i in range(model.num_topics)], index_col=None,\n",
    "                usecols=[i + 2 for i in range(model.num_topics)],\n",
    "            )\n",
    "\n",
    "            dt.index = docs_filtered['id']\n",
    "            dt = dt.divide(dt.min(axis=1), axis=0).astype(int) - 1\n",
    "\n",
    "            logger.info('Generating dfr-browser data...')\n",
    "            ddt = transform_dt(dt.as_matrix().T)\n",
    "            ttw = get_tw(model)\n",
    "\n",
    "            with open(os.path.join(MODEL_DATA_FOLDER, 'tw.json'), 'w') as fl:\n",
    "                json.dump(ttw, fl)\n",
    "\n",
    "            with open(os.path.join(MODEL_DATA_FOLDER, 'dt.json'), 'w') as fl:\n",
    "                json.dump(ddt, fl)\n",
    "\n",
    "            info_json = {\n",
    "                \"title\": f\"Topics in <em>{CORPUS_ID} {MODEL_ID}<\\/em>\",\n",
    "                \"meta_info\": \"This site is the working demo for <a href=\\\"/\\\">dfr-browser</a>, a browsing interface for topic models of journal articles or other text.\",\n",
    "                \"VIS\": {\n",
    "                    \"condition\": {\n",
    "                        \"type\": \"time\",\n",
    "                        \"spec\": {\n",
    "                            \"unit\": \"year\",\n",
    "                            \"n\": 1\n",
    "                        }\n",
    "                    },\n",
    "                    \"bib_sort\": {\n",
    "                        \"major\": \"year\",\n",
    "                        \"minor\": \"alpha\"\n",
    "                    },\n",
    "                    \"model_view\": {\n",
    "                        \"plot\": {\n",
    "                            \"words\": 6,\n",
    "                            \"size_range\": [6, 14]\n",
    "                        } \n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "\n",
    "            with open(os.path.join(MODEL_DATA_FOLDER, 'info.json'), 'w') as fl:\n",
    "                json.dump(info_json, fl)\n",
    "\n",
    "            # Generation of key LDA files\n",
    "            # doc_topics\n",
    "            logger.info('Storing doc_topics...')\n",
    "            dt.to_csv(\n",
    "                os.path.join(MODEL_DATA_FOLDER, f'doc_topics_{MODEL_ID}.csv'), \n",
    "                header=False,  # Change to True if topic id should be present as the header\n",
    "                index=False  # Change to True if the uid should be present as the index\n",
    "            )\n",
    "            dt.to_csv(\n",
    "                os.path.join(MODEL_DATA_FOLDER, f'doc_topics_{MODEL_ID}_with_details.csv'), \n",
    "                header=True,  # Change to True if topic id should be present as the header\n",
    "                index=True  # Change to True if the uid should be present as the index\n",
    "            )\n",
    "\n",
    "            # topic_words\n",
    "            word_topics = pd.DataFrame(model.word_topics, columns=range(model.word_topics.shape[1]), index=range(1, model.word_topics.shape[0] + 1))\n",
    "            word_topics = word_topics.rename(columns=model.id2word)\n",
    "\n",
    "            logger.info('Storing word_topics...')\n",
    "            word_topics.astype(int).to_csv(\n",
    "                os.path.join(MODEL_DATA_FOLDER, f'topic_words_{MODEL_ID}.csv'), \n",
    "                header=False,  # Change to True if actual word should be present as the header\n",
    "                index=False  # Sorted order by topic id\n",
    "            )\n",
    "            word_topics.astype(int).to_csv(\n",
    "                os.path.join(MODEL_DATA_FOLDER, f'topic_words_{MODEL_ID}_with_details.csv'), \n",
    "                header=True,  # Change to True if actual word should be present as the header\n",
    "                index=False  # Sorted order by topic id\n",
    "            )\n",
    "\n",
    "            logger.info('Storing top_words...')\n",
    "            top_words = get_top_words(word_topics, topic=None, topn=NUM_TOPICS)\n",
    "            top_words.to_csv(\n",
    "                os.path.join(MODEL_DATA_FOLDER, f'top_words_{MODEL_ID}.csv'), \n",
    "                index=False\n",
    "            )\n",
    "\n",
    "            logger.info('Saving mallet lda model...')\n",
    "            model.save(os.path.join(MODEL_DATA_FOLDER, f'{CORPUS_ID}_lda_model_{MODEL_ID}.mallet.lda'))\n",
    "\n",
    "            logger.info('Converting mallet lda to gensim lda model...')\n",
    "            gensim_lda = malletmodel2ldamodel(model, gamma_threshold=0.000001, iterations=1000)\n",
    "            gensim_lda.minimum_probability = 0.000001\n",
    "\n",
    "            logger.info('Saving mallet.gensim lda model...')\n",
    "            gensim_lda.save(os.path.join(MODEL_DATA_FOLDER, f'{CORPUS_ID}_lda_model_{MODEL_ID}.mallet.gensim.lda'))\n",
    "            \n",
    "            logger.info(f'lda model for {CORPUS_ID} completed: {MODEL_ID}...')\n",
    "            logger.removeHandler(lfh)\n",
    "\n",
    "            del(model)\n",
    "            gc.collect()\n",
    "\n",
    "        del(docs_filtered)\n",
    "        del(docs.doclist)\n",
    "        del(docs)\n",
    "        del(meta)\n",
    "        del(g_dict)\n",
    "        del(corpus)\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PD'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CORPUS_PART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# !../Mallet/bin/mallet import-file --preserve-case --keep-sequence --remove-stopwords --token-regex \"\\S+\" --input /R/NLP/MODELS/LDA/IMF-ALL_20/mallet/IMF-ALL_20_corpus.txt --output /R/NLP/MODELS/LDA/IMF-ALL_20/mallet/IMF-ALL_20_corpus.mallet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# http://microdatahub.com/topicsmodeling/dfr/topic_browser/browser.php?model=data50_SAR&type=SAR&topic_count=50#/doc/9622\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}