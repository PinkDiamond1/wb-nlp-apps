#!/usr/bin/env python
# -*- coding: utf-8 -*-
'''
This script processes the text data to generate candidate phrases based on part-of-speech patterns.
'''
import logging
from pathlib import Path

import json
import re

import click

import nltk
from gensim import utils
from joblib import Parallel, delayed
import joblib
import wb_nlp
from wb_nlp.utils.scripts import configure_logger, create_dask_cluster

# logging.basicConfig(stream=sys.stdout,
#                     level=logging.INFO,
#                     datefmt='%Y-%m-%d %H:%M',
#                     format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

MAX_LENGTH = 1000000


'''
Notes:
1. Download https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2 and save in /data/external/wikipedia/enwiki-latest-pages-articles.xml.bz2
2. Run `python -m gensim.scripts.segment_wiki -i -f /data/external/wikipedia/enwiki-latest-pages-articles.xml.bz2 -o /data/external/wikipedia/enwiki-latest.json.gz`
3. Run `python -u ./scripts/wikipedia/extract_wikipedia_titles.py --input-file ./data/external/wikipedia/enwiki-latest.json.gz --output-file ./data/external/wikipedia/enwiki-latest.titles-intro.json  |& tee ./logs/extract_wikipedia_titles.py.log

Use all words in wikipedia that is at least 4 characters.

'''

P_INTRO_NOISE_PATTERN = re.compile(r'[^a-zA-Z0-9\-., ]')
P_BDAY = re.compile(
    '(?:([A-Z][a-z]+ [0-9]{1,2}, [0-9]{1,4})|([0-9]{1,2} [A-Z][a-z]+ [0-9]{1,4}))')
P_ASCII = re.compile('^[ -~]+$')
GENSIM_BOLD_PATTERN = "'''"


def process_article_line(article_line):
    '''
    This function extracts the title and first line of the intro of the wiki article.
    There's an assumption that the first line of a wiki intro succinctly describes the article itself.
    This succinct description, together with the title, may be used as reference to filter useful phrases.

    The return format is a tuple. The first value is the title and the second is the intro. This is designed
    so that it's easy to generate a dictionary from the list of such tuples, i.e., result of joblib parallel processing.
    '''
    article = json.loads(article_line)

    title = article['title']
    section_texts = article['section_texts']

    if (title.isdigit() or title.isupper() or title[0].isdigit() or len(title) <= 2 or len(section_texts) == 0 or P_ASCII.search(title) is None):
        return (title, None)

    intro = section_texts[0]

    # Use the ''' convention in gensim processing to identify the general start of intro.
    # Example, the first line of the entry for "Albedo" is the description of an image. We use
    # The above convention to make sure we get the correct first line.

    intro = P_INTRO_NOISE_PATTERN.sub(
        ' ', GENSIM_BOLD_PATTERN.join(intro.split(GENSIM_BOLD_PATTERN)[1:]))
    sents = nltk.sent_tokenize(intro)

    intro = None

    if sents:
        intro = sents[0]
        if P_BDAY.search(intro):
            return (title, None)

    return (title, intro)


_logger = logging.getLogger(__file__)


@click.command()
@click.option('--input-file', 'input_file', required=True,
              type=click.Path(exists=True), help='path to wiki articles generated by gensim.scripts.segment_wiki')
@click.option('--output-file', 'output_file', required=True,
              type=click.Path(exists=False), help='path to output json file of article titles and intros')
@click.option('--quiet', 'log_level', flag_value=logging.WARNING, default=True)
@click.option('-v', '--verbose', 'log_level', flag_value=logging.INFO)
@click.option('-vv', '--very-verbose', 'log_level', flag_value=logging.DEBUG)
@click.version_option(wb_nlp.__version__)
def main(input_file: Path, output_file: Path, log_level: int):
    '''
    Entry point for wikipedia article title and intro extraction script.
    '''
    configure_logger(log_level)

    # YOUR CODE GOES HERE! Keep the main functionality in src/wb_nlp
    input_file = Path(input_file)
    output_file = Path(output_file)

    logging.info('Checking files...')
    if not input_file.exists():
        raise ValueError("Input file doesn't exist!")

    if not output_file.resolve().parent.exists():
        output_file.resolve().parent.mkdir(parents=True)

    client = create_dask_cluster(_logger)
    _logger.info(client)

    _logger.info('Starting joblib tasks...')

    with utils.open(input_file, 'rb') as wiki_gz:
        with joblib.parallel_backend('dask'):
            res = Parallel(verbose=10)(
                delayed(process_article_line)(line) for line in wiki_gz)

    with open(output_file, 'w') as out:
        json.dump(dict(filter(lambda x: x[1] is not None, res)), out)

    _logger.info('Processed all: %s', all(res))

# Parameters:
# - Location of input data
# - Directory of * .txt files
# - MongoDB database


if __name__ == '__main__':
    # python -u ./scripts/wikipedia/extract_wikipedia_titles.py --input-file ./data/external/wikipedia/enwiki-latest.json.gz --output-file ./data/external/wikipedia/enwiki-latest.titles-intro.json  |& tee ./logs/extract_wikipedia_titles.py.log
    main()


# '''
# https://www.wikidata.org/w/index.php?title=Special:WhatLinksHere/Q5&limit=500
# https://www.wikidata.org/w/index.php?title=Special:WhatLinksHere/Q5&namespace=0&limit=500&from=7130&back=4035


# https://www.wikidata.org/wiki/Property:P495 - country of origin
# https://www.wikidata.org/wiki/Property:P21 - sex or gender
# https://www.wikidata.org/wiki/Property:P27 - country of citizenship
# https://www.wikidata.org/wiki/Property:P276 - location

# https://www.wikidata.org/wiki/Q23810017 - decree
# https://www.wikidata.org/wiki/Q35749 - parliament

# '''
