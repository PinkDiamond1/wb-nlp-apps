{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "import cPickle\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/'\n",
    "pretrained_dir = '/home/avsolatorio/WORK/kaggle/pre-trained-models/'\n",
    "\n",
    "EMBEDDING_FILE_GLOVE = os.path.join(pretrained_dir, 'glove.840B.300d.txt')  # 'crawl-300d-2M.vec')\n",
    "EMBEDDING_FILE_GLOVE_W2V = os.path.join(pretrained_dir, 'glove.840B.300d.w2vformat.txt')\n",
    "EMBEDDING_FILE_FASTTEXT = os.path.join(pretrained_dir, 'crawl-300d-2M.vec')\n",
    "\n",
    "train = pd.read_csv(os.path.join(data_dir, 'train.csv'))\n",
    "test = pd.read_csv(os.path.join(data_dir, 'test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, return_splits=True):\n",
    "    text = text.fillna('fillna')\n",
    "    text = text.str.lower()\n",
    "    text = text.str.strip()\n",
    "    \n",
    "    # Datetime formats\n",
    "    text = text.str.replace('[0-9]{2}:[0-9]{2}, [a-z]+ [0-9]{1,}, [0-9]{4} \\(utc\\)', ' ')\n",
    "    text = text.str.replace('[0-9]{2}:[0-9]{2}, [0-9]{1,} [a-z]+ [0-9]{1,} \\(utc\\)', ' ')\n",
    "    \n",
    "    # Web links\n",
    "    text = text.str.replace('http[s]{0,1}://[\\S]+ ', ' ')\n",
    "\n",
    "    text = text.str.replace(\"\\'\", '')\n",
    "    text = text.str.replace(\"!{1,}\", ' xexclamx ')\n",
    "    text = text.str.replace('[\\(\\)]', '')\n",
    "    text = text.str.replace(',', ' ')\n",
    "    text = text.str.replace('[\\!\\@\\#\\$\\%\\^\\&\\*\\.\\?]{2,}', ' xrepsx ')\n",
    "    text = text.str.replace('[\\n]{1,}', ' ')\n",
    "    text = text.str.replace('[^a-zA-Z ]', ' ')\n",
    "    \n",
    "    if return_splits:\n",
    "        text = text.str.split()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toline(word, w2v_model):\n",
    "    return '{} {}'.format(word, ' '.join(w2v_model[word].astype('str')))\n",
    "\n",
    "\n",
    "def train_word2vec(\n",
    "    tokenized_questions,\n",
    "    pre_trained_model='GoogleNews-vectors-negative300.bin.gz',\n",
    "    size=300,\n",
    "    iter=10,\n",
    "    min_count=2,\n",
    "    negative=10,\n",
    "    workers=7,\n",
    "    min_alpha=0.0001,\n",
    "    window=5,\n",
    "    binary=True,\n",
    "    save_file=None,\n",
    "    seed=1029,\n",
    "    lockf=1\n",
    "):\n",
    "    # https://github.com/RaRe-Technologies/gensim/issues/1245\n",
    "    # List of tokenized questions.\n",
    "    # e.g. ['What', 'is', 'the', 'step', 'by', 'step', 'guide', 'to', 'invest', 'in', 'share', 'market', 'in', 'india']\n",
    "    # pre_trained_model can be any pre trained model that gensim accepts, e.g., Glove or GoogleNews word2vec\n",
    "\n",
    "    # Initialize model\n",
    "    word_vectors = Word2Vec(\n",
    "        size=size, iter=iter, min_count=min_count, negative=negative, workers=workers,\n",
    "        min_alpha=min_alpha, window=window,\n",
    "        seed=seed\n",
    "    )\n",
    "\n",
    "    # Initialize vocab\n",
    "    print('building vocab...')\n",
    "    word_vectors.build_vocab(tokenized_questions)\n",
    "\n",
    "    # Initialize vectors in local model with with vectors from pre-trained model with overlapping vocabulary.\n",
    "    # Set `lockf` to 1 for re-training\n",
    "    print('injecting pre-trained vectors...')\n",
    "    word_vectors.intersect_word2vec_format(pre_trained_model, lockf=lockf, binary=binary)\n",
    "\n",
    "    # Adjust pre-trained vectors to adapt its distribution with that of the local data via retraining.\n",
    "    print('start training...')\n",
    "    word_vectors.train(\n",
    "        tokenized_questions,\n",
    "        total_examples=word_vectors.corpus_count,\n",
    "        epochs=word_vectors.iter\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        save_file = '{}-{}'.format(save_file, datetime.now()) if save_file is not None else 'custom-w2v-{}'.format(datetime.now())\n",
    "        \n",
    "        print('saving model to file: {}...'.format(save_file))\n",
    "        word_vectors.save('{}.w2vmodel'.format(save_file))\n",
    "        \n",
    "        print('saving vectors to file: {}...'.format())\n",
    "        with open('{}.w2v.vectors.txt'.format(save_file), 'w') as f:\n",
    "            for word in word_vectors.wv.vocab:\n",
    "                entry = toline(word, word_vectors)\n",
    "                f.write('{}\\n'.format(entry))\n",
    "    except:\n",
    "        print('Failed saving...')\n",
    "        pass\n",
    "\n",
    "    return word_vectors, save_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.7 s, sys: 266 ms, total: 12 s\n",
      "Wall time: 11.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train = tokenize(train[\"comment_text\"], return_splits=True).values\n",
    "X_test = tokenize(test[\"comment_text\"], return_splits=True).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_comments = [i for i in X_train if len(i) > 0] + [i for i in X_test if len(i) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building vocab...\n",
      "injecting pre-trained vectors...\n",
      "start training...\n",
      "saving model to file: custom-w2v-with-fasttext-vecs-locked-2018-03-18 19:57:58.436226...\n",
      "Failed saving...\n",
      "CPU times: user 21min, sys: 2.87 s, total: 21min 2s\n",
      "Wall time: 7min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "start = datetime.now()\n",
    "word_vectors_vec, save_file = train_word2vec(\n",
    "    tokenized_comments,\n",
    "    pre_trained_model=EMBEDDING_FILE_FASTTEXT,\n",
    "    binary=False,\n",
    "    save_file='custom-w2v-with-fasttext-vecs-locked',\n",
    "    iter=10,\n",
    "    lockf=0,\n",
    "    min_count=10\n",
    ")\n",
    "\n",
    "with open('word_vectors_fasttext.lock', 'w') as fl:\n",
    "    fl.write('{}'.format((datetime.now() - start).total_seconds()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('fucking', 0.8198376893997192), ('fucks', 0.7554321885108948), ('fucked', 0.751573920249939), ('fucker', 0.7173537611961365), ('fuckin', 0.698074996471405), ('fuckers', 0.690087616443634), ('shit', 0.6870328783988953), ('fuk', 0.6845501661300659), ('ashol', 0.6474372148513794), ('fucken', 0.6414185166358948)]\n"
     ]
    }
   ],
   "source": [
    "print word_vectors_vec.most_similar('fuck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('{}.w2v.vectors.txt'.format(save_file), 'w') as f:\n",
    "    for word in word_vectors_vec.wv.vocab:\n",
    "        entry = toline(word, word_vectors_vec)\n",
    "        f.write('{}\\n'.format(entry))\n",
    "\n",
    "word_vectors_vec.save('{}.w2vmodel'.format(save_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('thank', 0.4745348393917084), ('lol', 0.4688342809677124), ('congrats', 0.45453837513923645), ('haha', 0.45355671644210815), ('yay', 0.45155268907546997), ('hello', 0.45108360052108765), ('freakin', 0.4448145031929016), ('hahaha', 0.44444596767425537), ('awesome', 0.44306883215904236), ('happy', 0.4410898685455322)]\n"
     ]
    }
   ],
   "source": [
    "print word_vectors_glove_vec.most_similar('xexclamx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('thank', 0.492216557264328), ('happy', 0.4738181233406067), ('lol', 0.47110986709594727), ('awesome', 0.47050565481185913), ('haha', 0.45867347717285156), ('hello', 0.45843008160591125), ('yay', 0.45154261589050293), ('congrats', 0.45094528794288635), ('oh', 0.4467509090900421), ('freakin', 0.44231390953063965)]\n"
     ]
    }
   ],
   "source": [
    "print word_vectors_glove_vec.most_similar('xexclamx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('fucking', 0.9117226004600525), ('ass', 0.8491613268852234), ('fucked', 0.8409990072250366), ('bitch', 0.8190275430679321), ('dick', 0.8054120540618896), ('pussy', 0.800691545009613), ('suck', 0.796139657497406), ('fucks', 0.7924858331680298), ('fucker', 0.779781699180603), ('fuckin', 0.7665546536445618)]\n"
     ]
    }
   ],
   "source": [
    "print word_vectors_glove_vec.most_similar('fuck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file = 'custom-w2v-with-glove-vecs-locked-2018-03-18 18:57:06.486222'\n",
    "with open('{}.w2v.vectors.txt'.format(save_file), 'w') as f:\n",
    "    for word in word_vectors_glove_vec.wv.vocab:\n",
    "        entry = toline(word, word_vectors_glove_vec)\n",
    "        f.write('{}\\n'.format(entry))\n",
    "\n",
    "word_vectors_glove_vec.save('{}.w2vmodel'.format(save_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('yawa', 0.8797650933265686),\n",
       " ('nalang', 0.8796565532684326),\n",
       " ('hayop', 0.8790163993835449),\n",
       " ('baboy', 0.8676713705062866),\n",
       " ('offff', 0.8663419485092163),\n",
       " ('lanwi', 0.864619255065918),\n",
       " ('bacha', 0.8645554780960083),\n",
       " ('blahhhhh', 0.8591206073760986),\n",
       " ('poljske', 0.8590113520622253),\n",
       " ('kirchen', 0.8589603304862976)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors_glove.most_similar('putah')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file = 'custom-w2v-2018-03-18 01:51:37.402106'\n",
    "with open('{}.w2v.vectors.txt'.format(save_file), 'w') as f:\n",
    "    for word in word_vectors_glove.wv.vocab:\n",
    "        entry = toline(word, word_vectors_glove)\n",
    "        f.write('{}\\n'.format(entry))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors_glove.save('{}.w2vmodel'.format(save_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kamo', 0.33765679597854614),\n",
       " ('hayop', 0.33685269951820374),\n",
       " ('nawa', 0.3262745440006256),\n",
       " ('mumu', 0.32459402084350586),\n",
       " ('tubig', 0.31144294142723083),\n",
       " ('ihh', 0.29867127537727356),\n",
       " ('kuwa', 0.29780203104019165),\n",
       " ('oooooooh', 0.29376742243766785),\n",
       " ('saare', 0.2930316627025604),\n",
       " ('celaka', 0.2916872799396515)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fuck 2.017783 1.7786797 0.89188445 0.33717886 -1.0747464 -1.3682405 0.24001393 2.106605 -0.6743231 0.38741764 -2.5176349 1.5692208 1.2137522 -1.0351607 0.16773722 -0.65743643 -1.342997 1.3867356 -0.23306178 -0.5098565 0.94252956 1.0163538 1.5931997 -0.44192076 0.13647778 -0.4049346 -0.5950908 0.65403336 -0.37482566 0.057701822 0.18711543 -0.9583984 -0.959869 -0.81852007 0.6665856 0.18213916 -1.2549973 0.7778149 0.673082 -1.4108119 -0.8555216 0.52855974 -1.8154352 1.0716504 -1.1520269 1.550046 -0.10887948 -1.9119035 -1.3034973 0.7408836 2.1605446 -1.104308 -0.42523685 -0.87536865 -0.38826787 0.37005678 0.029779159 -0.28320637 0.5889931 0.88086516 -0.15481481 -0.52914613 0.79724354 -0.23508072 0.21692257 0.96935815 -0.98359966 -1.5806046 -0.44355845 -0.08708171 0.019584794 0.6108139 -0.036803618 1.0005672 -0.67962605 1.6407994 0.43595326 -0.8629202 0.5020631 2.167295 0.764724 -0.96828157 0.22215915 -0.49856928 -0.09789388 -1.9419558 0.65986735 0.76987594 0.09665694 -1.4868082 0.13182876 1.1567365 0.12381404 2.7866085 -0.06887858 -0.36045322 -0.2616117 -0.9546936 -0.9570243 -0.052281916 -0.004287556 -0.90752506 0.54278195 -0.23259054 -0.69170374 1.526837 -0.39690527 0.09711815 -1.9423848 -1.8900807 1.8691906 1.3872099 -1.7761767 0.570055 -0.7654464 -0.32749325 0.28444526 0.61675495 1.1119225 1.4086262 -0.16530272 -0.535269 1.4459479 0.17257069 0.065448 -0.112627275 0.67849404 -1.1514391 -1.5367683 0.16370322 -0.10051129 0.17300314 1.6516653 -0.96741575 1.8067001 -1.1519572 -0.6090305 0.90022737 -1.2472683 -0.14067009 0.8044819 -0.26167423 0.11036888 -2.24186 0.58221644 0.16040498 -0.58265865 -0.6086846 0.49324873 -0.9668993 -1.3120824 -1.1947145 0.97879905 1.0444556 -0.3246319 -0.73008305 1.755455 1.0841258 -0.97492564 -0.27624097 0.5014476 -0.105314516 -0.37393817 -1.1775112 0.37870893 -1.345509 0.36920673 -0.8541484 -0.4526572 -0.28565526 -0.37631574 0.08588554 -1.4709405 -1.0110871 1.3734657 -0.1123042 -0.6863378 0.03140833 1.1942129 0.9739598 1.9463863 -0.5095479 0.5982598 0.18599585 -0.094610475 -0.38671234 -0.17390631 -0.1398534 -0.8803352 0.24325226 0.24018055 1.2691675 0.87181354 0.00046132703 0.35348555 1.5706701 1.076452 1.39946 0.4098426 0.78107256 -0.64370877 0.25056174 0.38147363 0.64557105 0.4132185 0.60587066 -0.6639271 2.182555 1.5484861 0.84690934 -0.9597281 1.7943598 -0.62912536 -0.5771089 0.4690569 -0.46010596 -1.1305724 -1.1459504 -0.23403658 -0.054988675 0.6892933 -0.14373523 -2.479406 1.8663138 -0.6318484 0.75684273 0.20037119 -0.84941846 1.3164889 -1.2200176 0.6191614 -1.2592782 -0.20509711 0.058702797 -0.24932154 -0.14384891 0.9398345 0.533538 -0.7049534 -0.23330975 0.91801876 0.5979487 0.21069513 0.29105467 2.4176655 0.7372098 -1.4278368 -1.7313899 -0.6014411 0.6654981 -2.1044888 -1.5946032 0.13847178 -0.21520598 -0.17462878 -0.27432656 0.13102283 1.6411058 -1.0464805 0.78021747 0.48858026 0.22661366 -0.72572744 0.8075193 0.7302393 -1.4167261 0.6538565 -0.9570806 0.6833109 3.4734502 -0.69927996 2.1385489 0.35023782 -0.8828952 -1.0788709 -1.0020157 0.6770872 -0.6950683 0.01605315 -0.98466706 -1.0744271 0.005925141 0.3537179 -0.04468761 0.36817303 -0.30753887 2.3494973 -1.0057036 -0.47148913 -0.72346 1.7433869 -0.7318334 1.5633008 -0.34546107 0.50880426 0.48475882 0.5866188 -1.2213233 1.1171027 0.72246873'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toline(word, w2v_model):\n",
    "    return '{} {}'.format(word, ' '.join(w2v_model[word].astype('str')))\n",
    "\n",
    "\n",
    "def train_word2vec(\n",
    "    tokenized_docs,\n",
    "    pre_trained_model='GoogleNews-vectors-negative300.bin.gz',\n",
    "    size=300,\n",
    "    iter=10,\n",
    "    min_count=2,\n",
    "    negative=10,\n",
    "    workers=7,\n",
    "    min_alpha=0.0001,\n",
    "    window=5,\n",
    "    binary=True,\n",
    "    save_file=None,\n",
    "    seed=1029,\n",
    "    lockf=1\n",
    "):\n",
    "    # https://github.com/RaRe-Technologies/gensim/issues/1245\n",
    "    # List of tokenized questions.\n",
    "    # e.g. ['What', 'is', 'the', 'step', 'by', 'step', 'guide', 'to', 'invest', 'in', 'share', 'market', 'in', 'india']\n",
    "    # pre_trained_model can be any pre trained model that gensim accepts, e.g., Glove or GoogleNews word2vec\n",
    "\n",
    "    # Initialize model\n",
    "    word_vectors = Word2Vec(\n",
    "        size=size, iter=iter, min_count=min_count, negative=negative, workers=workers,\n",
    "        min_alpha=min_alpha, window=window,\n",
    "        seed=seed\n",
    "    )\n",
    "\n",
    "    # Initialize vocab\n",
    "    print('building vocab...')\n",
    "    word_vectors.build_vocab(tokenized_docs)\n",
    "\n",
    "    # Initialize vectors in local model with with vectors from pre-trained model with overlapping vocabulary.\n",
    "    # Set `lockf` to 1 for re-training\n",
    "    print('injecting pre-trained vectors...')\n",
    "    word_vectors.intersect_word2vec_format(pre_trained_model, lockf=lockf, binary=binary)\n",
    "\n",
    "    # Adjust pre-trained vectors to adapt its distribution with that of the local data via retraining.\n",
    "    print('start training...')\n",
    "    word_vectors.train(\n",
    "        tokenized_docs,\n",
    "        total_examples=word_vectors.corpus_count,\n",
    "        epochs=word_vectors.iter\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        save_file = '{}-{}'.format(save_file, datetime.now()) if save_file is not None else 'custom-w2v-{}'.format(datetime.now())\n",
    "        \n",
    "        print('saving model to file: {}...'.format(save_file))\n",
    "        word_vectors.save('{}.w2vmodel'.format(save_file))\n",
    "        \n",
    "        print('saving vectors to file: {}...'.format())\n",
    "        with open('{}.w2v.vectors.txt'.format(save_file), 'w') as f:\n",
    "            for word in word_vectors.wv.vocab:\n",
    "                entry = toline(word, word_vectors)\n",
    "                f.write('{}\\n'.format(entry))\n",
    "    except:\n",
    "        print('Failed saving...')\n",
    "        pass\n",
    "\n",
    "    return word_vectors, save_file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}