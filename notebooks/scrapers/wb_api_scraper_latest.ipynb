{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# World Bank Documents and Reports API Scraper\n",
    "\n",
    "This notebook contains all the necessary scripts to connect and scrape document metadata from the World Bank Documents and Reports API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 258 ms\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 169 ms\n"
     ]
    }
   ],
   "source": [
    "from wb_nlp import dir_manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are the proper versions to be installed to prevent incompatibility issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requests : 2.24.0\n",
      "json : 2.0.9\n",
      "pandas : 1.1.2\n",
      "time: 553 µs\n"
     ]
    }
   ],
   "source": [
    "for m in [requests, json, pd]:\n",
    "    print(m.__name__, ':', m.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping methods for the World Bank Document and Reports API\n",
    "\n",
    "The API allows customization of the kind of returned data. The `fl_param` can be adjusted to a specific subset of values if only certain fields are needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 457 µs\n"
     ]
    }
   ],
   "source": [
    "fl_params = [\n",
    "    'guid', 'abstracts', 'admreg', 'alt_title', 'authr', 'available_in',\n",
    "    'bdmdt', 'chronical_docm_id', 'closedt', 'colti', 'count', 'credit_no',\n",
    "    'disclosure_date', 'disclosure_type', 'disclosure_type_date', 'disclstat',\n",
    "    'display_title', 'docdt', 'docm_id', 'docna', 'docty', 'dois', 'entityid',\n",
    "    'envcat', 'geo_reg', 'geo_reg_and_mdk', 'historic_topic', 'id',\n",
    "    'isbn', 'issn', 'keywd', 'lang', 'listing_relative_url', 'lndinstr', 'loan_no',\n",
    "    'majdocty', 'majtheme', 'ml_abstract', 'ml_display_title', 'new_url', 'owner',\n",
    "    'pdfurl', 'prdln', 'projn', 'publishtoextweb_dt', 'repnb', 'repnme', 'seccl',\n",
    "    'sectr', 'src_cit', 'subsc', 'subtopic', 'teratopic', 'theme', 'topic', 'topicv3',\n",
    "    'totvolnb', 'trustfund', 'txturl', 'unregnbr', 'url_friendly_title', 'versiontyp',\n",
    "    'versiontyp_key', 'virt_coll', 'vol_title', 'volnb', 'projectid',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstracts\n",
      "admreg\n",
      "alt_title\n",
      "authr\n",
      "available_in\n",
      "bdmdt\n",
      "chronical_docm_id\n",
      "closedt\n",
      "colti\n",
      "count\n",
      "credit_no\n",
      "disclosure_date\n",
      "disclosure_type\n",
      "disclosure_type_date\n",
      "disclstat\n",
      "display_title\n",
      "docdt\n",
      "docm_id\n",
      "docna\n",
      "docty\n",
      "dois\n",
      "entityid\n",
      "envcat\n",
      "geo_reg\n",
      "geo_reg_and_mdk\n",
      "guid\n",
      "historic_topic\n",
      "id\n",
      "isbn\n",
      "issn\n",
      "keywd\n",
      "lang\n",
      "listing_relative_url\n",
      "lndinstr\n",
      "loan_no\n",
      "majdocty\n",
      "majtheme\n",
      "ml_abstract\n",
      "ml_display_title\n",
      "new_url\n",
      "owner\n",
      "pdfurl\n",
      "prdln\n",
      "projectid\n",
      "projn\n",
      "publishtoextweb_dt\n",
      "repnb\n",
      "repnme\n",
      "seccl\n",
      "sectr\n",
      "src_cit\n",
      "subsc\n",
      "subtopic\n",
      "teratopic\n",
      "theme\n",
      "topic\n",
      "topicv3\n",
      "totvolnb\n",
      "trustfund\n",
      "txturl\n",
      "unregnbr\n",
      "url_friendly_title\n",
      "versiontyp\n",
      "versiontyp_key\n",
      "virt_coll\n",
      "vol_title\n",
      "volnb\n",
      "time: 1.86 ms\n"
     ]
    }
   ],
   "source": [
    "for i in sorted(fl_params):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/wb536061/wb_nlp/data/corpus/WB'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.86 ms\n"
     ]
    }
   ],
   "source": [
    "dir_manager.get_data_dir('corpus', 'WB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/wb536061/wb_nlp/data/corpus/WB\n",
      "/home/wb536061/wb_nlp/data/corpus/WB/tmp_api_json\n",
      "time: 642 µs\n"
     ]
    }
   ],
   "source": [
    "SCRAPER_DIR = dir_manager.get_data_dir('corpus', 'WB')\n",
    "API_JSON_DIR = os.path.join(SCRAPER_DIR, 'tmp_api_json')\n",
    "print(SCRAPER_DIR)\n",
    "print(API_JSON_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 557 µs\n"
     ]
    }
   ],
   "source": [
    "def download_with_retry(url, params=None, max_retries=10):\n",
    "    retry_count = 0\n",
    "\n",
    "    while retry_count < max_retries:\n",
    "        try:\n",
    "            response = requests.get(url, params=params)\n",
    "            if response.status_code != 200:\n",
    "                retry_count += 1\n",
    "                time.sleep(1)\n",
    "                continue\n",
    "            break\n",
    "        except:\n",
    "            retry_count += 1\n",
    "            time.sleep(1)\n",
    "\n",
    "    if retry_count >= max_retries:\n",
    "        return\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.53 ms\n"
     ]
    }
   ],
   "source": [
    "def request_worldbank_api(fl_params=None, offset=0, limit=1, max_retries=10):\n",
    "    '''\n",
    "    fl_params: list of values to return per row\n",
    "    offset: parameter corresponding to the start page\n",
    "    limit: maximum number of rows returned by the api call\n",
    "    '''\n",
    "\n",
    "    if fl_params is None:\n",
    "        fl_params = ['guid']\n",
    "\n",
    "    api_url = 'http://search.worldbank.org/api/v2/wds'\n",
    "    api_params = dict(\n",
    "        format='json',\n",
    "        fl=','.join(fl_params),\n",
    "        lang_exact='English',\n",
    "        disclstat='Disclosed',\n",
    "        srt='docdt',\n",
    "        order='desc',  # Use asc such that pages already downloaded can still be used\n",
    "        os=offset,\n",
    "        rows=limit,\n",
    "        # frmdisclosuredate='',  # '2018-09-12'\n",
    "        # todisclosuredate='',  # '2018-09-13'\n",
    "    )\n",
    "\n",
    "    response = download_with_retry(url=api_url, params=api_params)\n",
    "\n",
    "    if (response is None) or (response.status_code != 200):\n",
    "        return {}\n",
    "\n",
    "    json_content = response.json()\n",
    "\n",
    "    return json_content\n",
    "\n",
    "\n",
    "def get_total_documents():\n",
    "    # This method solves the problem of determination of\n",
    "    # the total pages in the database automatically.\n",
    "\n",
    "    poll_request = request_worldbank_api()\n",
    "    total_documents = poll_request['total']\n",
    "\n",
    "    return int(total_documents)\n",
    "\n",
    "\n",
    "def scrape_page(fl_params, page, limit=500, verbose=True):\n",
    "    offset = page * limit\n",
    "    page_content = request_worldbank_api(fl_params=fl_params, offset=offset, limit=limit)\n",
    "    page_content = page_content['documents']\n",
    "    func_params = {'page': page}\n",
    "\n",
    "    # Remove extraneous key\n",
    "    page_content.pop('facets')\n",
    "\n",
    "    if not os.path.isdir(API_JSON_DIR):\n",
    "        os.makedirs(API_JSON_DIR)\n",
    "\n",
    "    page_file = os.path.join(API_JSON_DIR, 'data-{page}.json'.format(**func_params))\n",
    "\n",
    "    with open(page_file, 'w') as fl:\n",
    "        json.dump(page_content, fl)\n",
    "\n",
    "    if verbose:\n",
    "        print('Completed scraping of page {page}.'.format(**func_params), flush=True)\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "\n",
    "def scrape_worldbank_operational_docs_api(fl_params, limit=500, max_pages=5, n_jobs=1, verbose=True):\n",
    "    '''\n",
    "    Note:\n",
    "        Parallelization of API access is discouraged for large limit size.\n",
    "        It could result to throttling or failed return values.\n",
    "    '''\n",
    "    func_params = {}\n",
    "    total_documents = get_total_documents()\n",
    "\n",
    "    total_pages = (total_documents // limit) + 1\n",
    "    func_params['total_pages'] = total_pages\n",
    "\n",
    "    scrape_params = []\n",
    "\n",
    "    for page in range(total_pages):\n",
    "        func_params['page'] = page + 1\n",
    "\n",
    "        if (max_pages is not None) and (page > max_pages):\n",
    "            print('Terminating scraping for remaining pages...')\n",
    "            break\n",
    "\n",
    "        if not verbose:\n",
    "            # Print this only if scrape_params verbosity is False...\n",
    "            print('Scraping page {page} / {total_pages}'.format(**func_params))\n",
    "\n",
    "        scrape_params.append(dict(fl_params=fl_params, page=page, limit=limit, verbose=verbose))\n",
    "\n",
    "    Parallel(n_jobs=n_jobs)(delayed(scrape_page)(**sp) for sp in scrape_params)\n",
    "    # scrape_page(fl_params, page, limit, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 245 µs\n"
     ]
    }
   ],
   "source": [
    "# # Check if ids are sorted by disclosure date\n",
    "\n",
    "# sample_request_data = request_worldbank_api(fl_params, offset=100000, limit=4)\n",
    "# sample_request_keys = sorted(sample_request_data['documents'].keys())\n",
    "# sample_request_keys.pop(sample_request_keys.index('facets'))\n",
    "# sample_request_disclosure_dates = sorted([sample_request_data['documents'][uid]['disclosure_date'] for uid in sample_request_keys])\n",
    "\n",
    "# for ix, i in enumerate(sample_request_keys):\n",
    "#     # Assuming that the document ids are not sequentially assigned by disclosure_date,\n",
    "#     # then it is likely that if we sort the ids and the disclosure date and check the equality of the\n",
    "#     # actual disclosure_date for the id vs the sorted disclosure_date that it will not match.\n",
    "#     assert(sample_request_data['documents'][i]['disclosure_date'] == sample_request_disclosure_dates[ix])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "593"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 55.2 ms\n"
     ]
    }
   ],
   "source": [
    "get_total_documents() // 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "scrape_worldbank_operational_docs_api(fl_params=fl_params, limit=500, max_pages=None, n_jobs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(API_JSON_DIR, \"data-100.json\")) as fl:\n",
    "    nd = json.load(fl)\n",
    "    nd = pd.DataFrame(nd).T\n",
    "    nd.index.name = 'uid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = nd.dropna(subset=[\"authors\"])[nd.dropna(subset=[\"authors\"])[\"authors\"].map(len) > 3].iloc[0].to_dict()\n",
    "for k, v in s.items():\n",
    "    if isinstance(v, dict):\n",
    "        print(k, v)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing and normalization of scraped document metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = set(['Publications & Research', 'Publications'])\n",
    "# s = set([\"Country Focus\", \"Country Focus\"])\n",
    "# s = set(\"Publications,Publications & Research,Publications,Publications & Research\".split(','))\n",
    "\n",
    "def normalize_set(s):\n",
    "    # s = set(['Publications & Research', 'Publications'])\n",
    "\n",
    "    l = sorted(s)\n",
    "    remove_index = set()\n",
    "    \n",
    "    for i in range(len(l) - 1):\n",
    "        for j in range(1, len(l)):\n",
    "            if l[i] in l[j]:\n",
    "                remove_index.add(i)\n",
    "    \n",
    "    for k in sorted(remove_index, reverse=True):\n",
    "        l.pop(k)\n",
    "\n",
    "    return l\n",
    "\n",
    "\n",
    "def make_unique_entry(series):\n",
    "    # This will remove duplicate entries in fields: `majdocty` (`majdoctype` : normalized) and `admreg`\n",
    "    series = series.fillna('')\n",
    "    series = series.str.split(',').map(set).map(lambda vals: ', '.join(normalize_set(vals)))\n",
    "    return series.replace('', None)\n",
    "\n",
    "\n",
    "def collapse_array(data, connector=None):\n",
    "    # Assume that array is of type list\n",
    "    value = []\n",
    "    \n",
    "    if isinstance(data, list):        \n",
    "        for d in data:\n",
    "            if isinstance(d, dict):\n",
    "                value.append(collapse_nested_dict(d, connector=connector))\n",
    "            else:\n",
    "                value.extend(collapse_array(d))\n",
    "#     elif isinstance(data, dict):\n",
    "#         data = collapse_nested_dict(data, connector=connector)\n",
    "#         value.append(data)\n",
    "    else:\n",
    "        value.append(data)\n",
    "    \n",
    "    try:\n",
    "        if connector:\n",
    "            # `connector` is only used in the root function call so it is safe\n",
    "            # to assume that in cases where the original value is not an array or nested array,\n",
    "            # we can just retrieve and return the original value\n",
    "            if len(value) > 1:\n",
    "                # This means that the data is an array and possibly nested\n",
    "                value = connector.join(value)\n",
    "            else:\n",
    "                value = value[0]\n",
    "    except Exception as e:\n",
    "        print(data)\n",
    "        print(value)\n",
    "        raise(e)\n",
    "        \n",
    "    return value\n",
    "\n",
    "\n",
    "# line_break_pattern = re.compile('\\r?\\n|\\r')\n",
    "whitespace_pattern = re.compile('\\s+')\n",
    "hanging_dash_pattern = re.compile('\\S+- ')\n",
    "\n",
    "\n",
    "def extract_formatted_authors(authors, delimiter=\";\"):\n",
    "    # {'0': {'author': 'Sjoberg,Fredrik Matias'},\n",
    "    #  '1': {'author': 'Mellon,Jonathan'},\n",
    "    #  '2': {'author': 'Peixoto,Tiago Carneiro'},\n",
    "    #  '3': {'author': 'Hemker,Johannes Zacharias'},\n",
    "    #  '4': {'author': 'Tsai,Lily Lee'}}\n",
    "\n",
    "    authors_value = None\n",
    "    if pd.notna(authors):\n",
    "        authors_value = delimiter.join([author[\"author\"] for author in authors.values()])\n",
    "\n",
    "    return authors_value\n",
    "\n",
    "def normalize_hanging_dash(t):\n",
    "    for p in hanging_dash_pattern.findall(t):\n",
    "        t = t.replace(p, p.replace('- ', ' - '))\n",
    "\n",
    "    return t\n",
    "\n",
    "\n",
    "def normalize_str_col(ser):\n",
    "    return ser.map(lambda x: normalize_hanging_dash(whitespace_pattern.sub(' ', x)) if isinstance(x, str) else x)\n",
    "\n",
    "\n",
    "def normalize_geo_regions(x, connector='|'):\n",
    "    # geo_regions has this assumed format: {'0': {'geo_region': 'Europe'}, '1': {'geo_region': 'Europe'}}\n",
    "    if isinstance(x, dict):\n",
    "        x = connector.join(set(i['geo_region'] for i in x.values()))\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def collapse_nested_dict(x, connector=None):\n",
    "    value = []\n",
    "    \n",
    "    if isinstance(x, dict):\n",
    "        for val in x.values():\n",
    "            value.extend(collapse_nested_dict(val))\n",
    "    elif isinstance(x, list):\n",
    "        x = collapse_array(x, connector=connector)\n",
    "        value.append(x)\n",
    "    else:\n",
    "        value.append(x)\n",
    "    \n",
    "    if connector:\n",
    "        if len(value) > 1:\n",
    "            value = connector.join(value)\n",
    "        else:\n",
    "            value = value[0]\n",
    "\n",
    "    return value\n",
    "\n",
    "def process_uid(uid):\n",
    "    # Implement this function to easily process the creation or update how the ids are constructed.\n",
    "    # Take note that this is important because the original API changed how the ids are rendered from <id> to D<id> format.\n",
    "\n",
    "    ## No need to do this since all uids are already standardized\n",
    "    # if uid.startswith('D'):\n",
    "    #     uid = uid[1:]\n",
    "\n",
    "    # if not uid.isdigit():\n",
    "    #     raise ValueError(f'Unexpected document id format: {uid}...')\n",
    "\n",
    "    return uid\n",
    "\n",
    "def normalize_document_data(use_short_columns=True, fname=None, data_dir=None, save_data=True):\n",
    "#     if use_short_columns:\n",
    "#         columns = ['guid', 'docyear', 'majdoctype', 'doctype', 'authors', 'colti', 'display_title', 'docdt', 'docm_id', 'historic_topic', 'pdfurl', 'seccl', 'txturl', 'language', 'admreg', 'country', 'txtfilename']\n",
    "#     else:\n",
    "#         columns = ['authors', 'abstracts', 'admreg', 'alt_title', 'available_in', 'bdmdt', 'chronical_docm_id', 'closedt', 'colti', 'count', 'credit_no', 'disclosure_date', 'disclosure_type', 'disclosure_type_date', 'disclstat', 'display_title', 'docdt', 'doc_year', 'docm_id', 'docna', 'docty', 'dois', 'entityids', 'envcat', 'geo_regions', 'geo_region_mdks', 'historic_topic', 'id', 'isbn', 'issn', 'keywd', 'lang', 'listing_relative_url', 'lndinstr', 'loan_no', 'majdocty', 'majtheme', 'ml_abstract', 'ml_display_title', 'new_url', 'owner', 'pdfurl', 'prdln', 'projn', 'publishtoextweb_dt', 'repnb', 'repnme', 'seccl', 'sectr', 'src_cit', 'subsc', 'subtopic', 'teratopic', 'theme', 'topic', 'topicv3', 'totvolnb', 'trustfund', 'txturl', 'unregnbr', 'url_friendly_title', 'versiontyp', 'versiontyp_key', 'virt_coll', 'vol_title', 'volnb']\n",
    "\n",
    "\n",
    "    normalized_df = pd.DataFrame()\n",
    "\n",
    "    for json_file in glob.iglob(os.path.join(API_JSON_DIR, '*.json')):\n",
    "        print(json_file)\n",
    "\n",
    "        with open(json_file) as fl:\n",
    "            normalized_data = json.load(fl)\n",
    "            normalized_data = pd.DataFrame(normalized_data).T\n",
    "            normalized_data.index.name = 'uid'\n",
    "            \n",
    "            normalized_data.index = normalized_data.index.map(process_uid)\n",
    "            # normalized_data.index = normalized_data.index.astype(int)\n",
    "\n",
    "        rename_cols = {\n",
    "            'docty': 'doc_type',\n",
    "            'lang': 'language',\n",
    "            'majdocty': 'majdoctype',\n",
    "            'count': 'country'\n",
    "        }\n",
    "\n",
    "        normalized_data = normalized_data.rename(columns=rename_cols)\n",
    "        try:\n",
    "            normalized_data['authors'] = normalized_data['authors'].map(authors_value)\n",
    "        except KeyError:\n",
    "            # This means that the metadata doesn't have an author field\n",
    "            normalized_data['authors'] = None\n",
    "\n",
    "        # Assume that the `display_title` field follows a standard format: list -> dict\n",
    "        # [{'display_title': 'Voice and Punishment : A Global\\n            Survey Experiment on Tax Morale'}]\n",
    "        normalized_data['display_title'] = normalized_data['display_title'].map(lambda dt: dt[0].get('display_title') if len(dt) else None)\n",
    "\n",
    "        for col in normalized_data.columns:\n",
    "            try:\n",
    "                # Normalize line breaks for string data\n",
    "                normalized_data[col] = normalize_str_col(normalized_data[col])\n",
    "                normalized_data[col] = normalized_data[col].map(lambda x: collapse_array(x, '|'))\n",
    "                normalized_data[col] = normalized_data[col].map(lambda x: collapse_nested_dict(x, '|'))\n",
    "\n",
    "            except AttributeError:\n",
    "                # column is not a string type\n",
    "                continue\n",
    "\n",
    "        normalized_data['majdoctype'] = make_unique_entry(normalized_data['majdoctype'])\n",
    "        normalized_data['admreg'] = make_unique_entry(normalized_data['admreg'])\n",
    "        normalized_data['geo_regions'] = normalized_data['geo_regions'].map(normalize_geo_regions)\n",
    "\n",
    "        normalized_data['docyear'] = pd.to_datetime(normalized_data['docdt']).dt.year\n",
    "\n",
    "#         existing_cols = normalized_data.columns.intersection(columns)\n",
    "#         new_cols = pd.Index(set(columns).difference(normalized_data.columns))\n",
    "\n",
    "#         normalized_data = normalized_data[existing_cols]\n",
    "\n",
    "#         for col in new_cols:\n",
    "#             normalized_data[col] = None\n",
    "\n",
    "        if normalized_df.empty:\n",
    "            normalized_df = normalized_data\n",
    "        else:\n",
    "            normalized_df = pd.concat([normalized_df, normalized_data], axis=0)\n",
    "\n",
    "#     if save_data:\n",
    "#         if fname is None:\n",
    "#             fname = f\"WBCorpus_metadata_{pd.datetime.now().strftime('%m-%d-%Y')}.csv\"\n",
    "#         if data_dir:\n",
    "#             fname = os.path.join(data_dir, fname)\n",
    "            \n",
    "#         normalized_df.to_csv(fname)\n",
    "\n",
    "    return normalized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "normalized_df = normalize_document_data(data_dir=SCRAPER_DIR)\n",
    "\n",
    "print('\\nMissing fields:')\n",
    "for c in 'uid,guid,docyear,majdoctype,doctype,authors,colti,display_title,docdt,docm_id,historic_topic,pdfurl,seccl,txturl,language,admreg,country,txtfilename,txtfileid,txturl2,doctypeid,lang_detected,lang_score,tokens'.split(','):\n",
    "    if c == 'uid':\n",
    "        continue\n",
    "    try:\n",
    "        normalized_df[c]\n",
    "    except:\n",
    "        print(f'\\t{c}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(296994, 67)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(297347, 67)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(297347, 67)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improvements\n",
    "\n",
    "By using dataframes, we can easily exploit its slice and filter methods to get certain partitions of the dataset given an arbitrary filter set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering by majdoctype\n",
    "\n",
    "We list all the available **major document types**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for majdoctype in normalized_df.majdoctype.unique():\n",
    "    print(majdoctype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for majdoctype in normalized_df.majdoctype.unique():\n",
    "    print(majdoctype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can sample from all the dataset all the documents corresponding to the `Project Documents` document type as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_documents = normalized_df[normalized_df.majdoctype == 'Project Documents']\n",
    "project_documents.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering by year\n",
    "\n",
    "Again, we can perform a filtered view of the metadata based on the document year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_df[normalized_df.docyear == 2018].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering the dataset by combination of conditions\n",
    "\n",
    "Shown below is the method of extracting snapshots of the entire metadata based on specific filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = (\n",
    "    (normalized_df.docyear == 2018) &\n",
    "    (normalized_df.majdoctype == 'Project Documents') &\n",
    "    (normalized_df.country == 'Philippines')\n",
    ")\n",
    "\n",
    "normalized_df[filters].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METADATA_COLS = [\n",
    "    'corpus', 'id', 'path_original', 'path_clean', 'filename_original', 'year',\n",
    "    'major_doc_type', 'doc_type', 'author', 'collection', 'title', 'journal', 'volume',\n",
    "    'date_published', 'digital_identifier', 'topics_src', 'url_pdf', 'url_txt', 'language_src',\n",
    "    'adm_region', 'geo_region', 'country',\n",
    "\n",
    "    # Not yet available at this stage...,\n",
    "    # 'language_detected', 'language_score', 'tokens'  \n",
    "\n",
    "    # WB specific fields\n",
    "    'wb_lending_instrument', 'wb_product_line', 'wb_major_theme', 'wb_theme', 'wb_sector',\n",
    "    'wb_subtopic_src', 'wb_project_id',\n",
    "    # 'wb_environmental_category', \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_wb_id(uid, max_len=9):\n",
    "    # return f'wb_{\"0\"*(max_len - len(str(uid)))}{uid}'\n",
    "    return f'wb_{uid}'\n",
    "\n",
    "\n",
    "def standardize_metadata_fields(metadata_df):\n",
    "    '''\n",
    "    This method must be applied to the original metadata processed dataframe.\n",
    "    This will assign the final field names.\n",
    "    '''\n",
    "    metadata_df = metadata_df.reset_index()\n",
    "    metadata_df['uid'] = metadata_df.uid.map(build_wb_id)\n",
    "\n",
    "    wb_core_field_map = {\n",
    "        'uid': 'id',\n",
    "        'docyear': 'year',\n",
    "        'majdoctype': 'major_doc_type',\n",
    "        'doctype': 'doc_type',\n",
    "        'authors': 'author',\n",
    "        'colti': 'collection',\n",
    "        'display_title': 'title',\n",
    "        'docdt': 'date_published',\n",
    "        'docm_id': 'digital_identifier',\n",
    "        'historic_topic': 'topics_src',\n",
    "        'pdfurl': 'url_pdf',\n",
    "        'txturl': 'url_txt',\n",
    "        'language': 'language_src',\n",
    "        'admreg': 'adm_region',\n",
    "        'country': 'country',\n",
    "        'geo_regions': 'geo_region',\n",
    "    }\n",
    "\n",
    "    wb_specific_field_map = {\n",
    "        'lndinstr': 'wb_lending_instrument',\n",
    "        'prdln': 'wb_product_line',\n",
    "        'majtheme': 'wb_major_theme',\n",
    "        'theme': 'wb_theme',\n",
    "        'sectr': 'wb_sector',\n",
    "        # 'envcat': 'wb_environmental_category',\n",
    "        'projectid': 'wb_project_id',\n",
    "        'subtopic': 'wb_subtopic_src',\n",
    "    }\n",
    "\n",
    "    wb_new_fields = ['corpus', 'path_original', 'path_clean', 'filename_original', 'journal', 'volume']\n",
    "\n",
    "    # path_original_dir = '/NLP/CORPUS/WB/TXT_ORIG'\n",
    "    # path_clean_dir = '/NLP/CORPUS/WB/TXT_CLEAN'\n",
    "\n",
    "    path_original_dir = 'data/corpus/WB'\n",
    "    path_clean_dir = ''\n",
    "\n",
    "    # Perform post normalization preprocessing\n",
    "    metadata_df['docdt'] = pd.to_datetime(metadata_df['docdt']).dt.date.map(str)\n",
    "\n",
    "    # Apply final field names\n",
    "    metadata_df = metadata_df.rename(columns=wb_core_field_map)\n",
    "    metadata_df = metadata_df.rename(columns=wb_specific_field_map)\n",
    "\n",
    "    for nf in wb_new_fields:\n",
    "        if nf == 'corpus':\n",
    "            metadata_df[nf] = 'WB'\n",
    "        elif nf == 'filename_original':\n",
    "            metadata_df[nf] = metadata_df.url_txt.map(lambda x: os.path.basename(x) if isinstance(x, str) else x)\n",
    "        elif nf == 'path_original':\n",
    "            metadata_df[nf] = metadata_df['id'].map(lambda x: f\"{path_original_dir}/{x}.txt\")\n",
    "        elif nf == 'path_clean':\n",
    "            metadata_df[nf] = metadata_df['id'].map(lambda x: f\"{path_clean_dir}/{x}.txt\" if path_clean_dir else None)\n",
    "        elif nf in ['journal', 'volume']:\n",
    "            metadata_df[nf] = None\n",
    "\n",
    "    metadata_df = metadata_df[METADATA_COLS]\n",
    "    return metadata_df.set_index('id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "normalized_final_df = standardize_metadata_fields(normalized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = f\"wb_metadata-{datetime.now()}.csv\"\n",
    "\n",
    "normalized_final_df.reset_index()[METADATA_COLS].to_csv(\n",
    "    os.path.join(SCRAPER_DIR, fname),\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_final_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(296994, 28)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(297347, 28)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(297490, 28)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_final_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading actual files\n",
    "\n",
    "So far, we already have a collection of metadata for the documents in the database. The following scripts will enable us to download the actual text documents associate with each entry in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_document_and_reports_file(data=None):\n",
    "    download_links = {}\n",
    "\n",
    "    if data is None:\n",
    "        for json_file in glob.iglob(os.path.join(API_JSON_DIR, '*.json')):\n",
    "            print(json_file)\n",
    "\n",
    "            with open(json_file) as fl:\n",
    "                data = json.load(fl)\n",
    "                data = pd.DataFrame(data).T\n",
    "                data.index = data.index.map(build_wb_id)\n",
    "                data.index.name = 'id'\n",
    "                download_data = data['txturl']  # txturl since this directly uses data from api\n",
    "\n",
    "                download_links.update(download_data)\n",
    "    else:\n",
    "        download_links = data['url_txt'].to_dict()\n",
    "\n",
    "    return download_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WB_TXT_DIR = dir_manager.get_data_dir(\"corpus\", \"WB\", \"TXT_ORIG\")\n",
    "\n",
    "if not os.path.isdir(WB_TXT_DIR):\n",
    "    os.makedirs(WB_TXT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# links = download_document_and_reports_file(data=normalized_final_df)\n",
    "links = download_document_and_reports_file(data=None)\n",
    "error_downloads = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def download_and_store_file(ix, fname, link):\n",
    "    if ix and ix%10000 == 0:\n",
    "        print(ix, id, link)\n",
    "\n",
    "    id = os.path.splitext(os.path.basename(fname))[0] # os.path.basename(fname).replace('.txt', '')\n",
    "    ret_val = 0\n",
    "\n",
    "    if not isinstance(link, str):\n",
    "        ret_val = (id, link, 'NaN link')\n",
    "    else:    \n",
    "        try:\n",
    "            if not os.path.isfile(fname):\n",
    "                # Download only files that are not yet available locally.\n",
    "                response = download_with_retry(link)\n",
    "\n",
    "                if response:\n",
    "                    with open(fname, 'wb') as fl:\n",
    "                        fl.write(response.content)\n",
    "                else:\n",
    "                    ret_val = (id, link, 'Empty response.')\n",
    "        except Exception as e:\n",
    "            ret_val = (id, link, e)\n",
    "\n",
    "    return ret_val\n",
    "\n",
    "def transform_link_to_pdf(link):\n",
    "    if not isinstance(link, str):\n",
    "        return None\n",
    "\n",
    "    link = link.replace(\"/text/\", \"/pdf/\")\n",
    "    link = os.path.splitext(link)[0] + \".pdf\"\n",
    "\n",
    "    return link\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download TXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(links))\n",
    "WB_TXT_DIR = dir_manager.get_data_dir(\"corpus\", \"WB\", \"TXT_ORIG\")\n",
    "\n",
    "if not os.path.isdir(WB_TXT_DIR):\n",
    "    os.makedirs(WB_TXT_DIR)\n",
    "\n",
    "ret_codes = Parallel(n_jobs=20)(\n",
    "    delayed(download_and_store_file)(ix, os.path.join(WB_TXT_DIR, f'{id}.txt'), link) for ix, (id, link) in enumerate(links.items())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_links = {id: transform_link_to_pdf(link) for id, link in links.items()}\n",
    "print(len(pdf_links))\n",
    "\n",
    "WB_PDF_DIR = dir_manager.get_data_dir(\"corpus\", \"WB\", \"PDF_ORIG\")\n",
    "\n",
    "if not os.path.isdir(WB_PDF_DIR):\n",
    "    os.makedirs(WB_PDF_DIR)\n",
    "\n",
    "pdf_ret_codes = Parallel(n_jobs=5)(\n",
    "    delayed(download_and_store_file)(ix, os.path.join(WB_PDF_DIR, f'{id}.pdf'), link) for ix, (id, link) in enumerate(pdf_links.items()) if link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 wb_D32412661 http://documents.worldbank.org/curated/en/296571600333530125/text/Audited-FS-2018-USD-account-pdf.txt\n",
      "20000 wb_D32044719 http://documents.worldbank.org/curated/en/333461589809644880/text/West-Bank-and-Gaza-MIDDLE-EAST-AND-NORTH-AFRICA-P150481-Health-System-Resiliency-Strengthening-Procurement-Plan.txt\n",
      "30000 wb_D31754788 http://documents.worldbank.org/curated/en/589771580823230896/text/Romania-EUROPE-AND-CENTRAL-ASIA-P148585-Romania-Secondary-Education-Project-Procurement-Plan.txt\n",
      "40000 wb_D31405141 http://documents.worldbank.org/curated/en/891021568371703236/text/Nigeria-AFRICA-P124905-Nigeria-Erosion-and-Watershed-Management-Project-Procurement-Plan.txt\n",
      "50000 wb_D31189642 http://documents.worldbank.org/curated/en/649271563588060501/text/Angola-Growth-and-Inclusion-Development-Policy-Financing-Project.txt\n",
      "60000 wb_D30758447 http://documents.worldbank.org/curated/en/570071547484250986/text/Rwanda-AFRICA-P131464-Landscape-Approach-to-Forest-Restoration-and-Conservation-LAFREC-Procurement-Plan.txt\n",
      "70000 wb_D30349702 http://documents.worldbank.org/curated/en/827661534340565421/text/Plan-Archive-19.txt\n",
      "80000 wb_D29939517 http://documents.worldbank.org/curated/en/150921527708768316/text/ITK171540-201804301529.txt\n",
      "90000 wb_D28139785 http://documents.worldbank.org/curated/en/576801510076208252/text/WPS8235.txt\n",
      "100000 wb_D31246970 http://documents.worldbank.org/curated/en/622481563185787551/text/Agriculture-for-Jobs-and-Growth-in-the-Western-Balkans-A-Regional-Report.txt\n",
      "110000 wb_D26856427 http://documents.worldbank.org/curated/en/565271476334707085/text/SFG2550-V1-RP-P112334-Box396315B-PUBLIC-Disclosed-10-12-2016.txt\n",
      "120000 wb_D25655317 http://documents.worldbank.org/curated/en/311171468277750533/text/Official-Documents-Supplemental-Letter-Ref-Performance-Monitoring-Indicators-for-Credit-5726-XK-Closing-Package.txt\n",
      "130000 wb_D24447367 http://documents.worldbank.org/curated/en/825191468171860291/text/95721-REVISED-WP-PUBLIC-Box391443B-Research-TrningInsightintoImpact-med.txt\n",
      "140000 wb_D19197777 http://documents.worldbank.org/curated/en/406101468202451878/text/857140BRI0WB0H00Box382147B00PUBLIC0.txt\n",
      "150000 wb_D18041755 http://documents.worldbank.org/curated/en/553701468006613407/text/797580PROP0P130Box0379789B00PUBLIC0.txt\n",
      "160000 wb_D16482657 http://documents.worldbank.org/curated/en/887181468777937745/text/ISR0Disclosabl009201201341866813562.txt\n",
      "170000 wb_D15419931 http://documents.worldbank.org/curated/en/542101468044651337/text/Integrated0Saf00Sheet0Concept0Stage.txt\n",
      "180000 wb_D13687012 http://documents.worldbank.org/curated/en/361431468117558890/text/RP10580P1241141LIC10AFR1RAP1P124114.txt\n",
      "190000 wb_D13306522 http://documents.worldbank.org/curated/en/389421468029648198/text/584770AR0Doing101public10BOX353803B.txt\n",
      "200000 wb_D16866866 http://documents.worldbank.org/curated/en/843631468270575627/text/71138020080Mac0Box0371948B00PUBLIC0.txt\n",
      "210000 wb_D8373143 http://documents.worldbank.org/curated/en/114341468014453955/text/C43311BOS0Avian0Flu0RS1Conformed.txt\n",
      "220000 wb_D6687124 http://documents.worldbank.org/curated/en/563181468038108758/text/PA0conformed73590.txt\n",
      "230000 wb_D2863431 http://documents.worldbank.org/curated/en/825761468762012568/text/276470Precis231.txt\n",
      "240000 wb_D437556 http://documents.worldbank.org/curated/en/744261468249279245/text/multi-page.txt\n",
      "250000 wb_D19523368 http://documents.worldbank.org/curated/en/733671468327329178/text/854200IBRD50000PUBLIC00Box0382159B.txt\n",
      "260000 wb_D737143 http://documents.worldbank.org/curated/en/593291468169169969/text/multi-page.txt\n",
      "270000 wb_D26853100 http://documents.worldbank.org/curated/en/806851476167441050/text/108901-BR-PUBLIC-IDA133-MembershipofthePeoplesRepublicofMozambique.txt\n",
      "280000 wb_D32326401 http://documents.worldbank.org/curated/en/886441597407065324/text/Announcement-of-First-IFC-Investment-in-Upper-Volta-Five-Million-Thirty-Five-Thousand-US-Dollars-for-Plastics-Manufacturer-on-October-26-1978.txt\n",
      "290000 wb_D19340289 http://documents.worldbank.org/curated/en/710851468339652294/text/850850IBRD23700PUBLIC00Box0382158B.txt\n"
     ]
    }
   ],
   "source": [
    "nan_ids = []\n",
    "for ix, (id, link) in enumerate(links.items()):\n",
    "    if not isinstance(link, str):\n",
    "        nan_ids.append(id)\n",
    "        continue\n",
    "\n",
    "    fname = os.path.join(WB_TXT_DIR, f'{id}.txt')\n",
    "    download_and_store_file(fname, link)\n",
    "\n",
    "    if ix and ix % 10000 == 0:\n",
    "        print(ix, id, link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_store_pdf(fname, link):\n",
    "    id = os.path.splitext(os.path.basename(fname))[0]  #.replace('.pdf', '')\n",
    "    ret_val = 0\n",
    "\n",
    "    try:\n",
    "        if not os.path.isfile(fname):\n",
    "            # Download only files that are not yet available locally.\n",
    "            response = download_with_retry(link)\n",
    "\n",
    "            if response:\n",
    "                with open(fname, 'wb') as fl:\n",
    "                    fl.write(response.content)\n",
    "            else:\n",
    "                ret_val = (id, link, 'Empty response.')\n",
    "    except Exception as e:\n",
    "        ret_val = (id, link, e)\n",
    "\n",
    "    return ret_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "296724"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nan_ids) + 295436"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 584 ms, sys: 16.8 ms, total: 600 ms\n",
      "Wall time: 599 ms\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "296724"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "WB_PDF_DIR = dir_manager.get_data_dir(\"corpus\", \"WB\", \"PDF_ORIG\")\n",
    "\n",
    "pdf_ret_codes = Parallel(n_jobs=5)(\n",
    "    delayed(download_and_store_file)(os.path.join(WB_PDF_DIR, f'{id}.pdf'), link) for id, link in pdf_links.items() if link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "295436"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(filter(lambda x: x, pdf_links.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "295436"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pdf_ret_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-fc7dcf83e11d>\u001b[0m in \u001b[0;36mdownload_with_retry\u001b[0;34m(url, params, max_retries)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/wb-nlp/lib/python3.7/site-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/wb-nlp/lib/python3.7/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/wb-nlp/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/wb-nlp/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[0;31m# Resolve redirects if allowed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 665\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mresp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mresp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_redirects\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/wb-nlp/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[0;31m# Resolve redirects if allowed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 665\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mresp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mresp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_redirects\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/wb-nlp/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mresolve_redirects\u001b[0;34m(self, resp, req, stream, timeout, verify, cert, proxies, yield_requests, **adapter_kwargs)\u001b[0m\n\u001b[1;32m    244\u001b[0m                     \u001b[0mallow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m                     \u001b[0;34m**\u001b[0m\u001b[0madapter_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m                 )\n",
      "\u001b[0;32m~/anaconda3/envs/wb-nlp/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    682\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 683\u001b[0;31m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/wb-nlp/lib/python3.7/site-packages/requests/models.py\u001b[0m in \u001b[0;36mcontent\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    828\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONTENT_CHUNK_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34mb''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/wb-nlp/lib/python3.7/site-packages/requests/models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    750\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/wb-nlp/lib/python3.7/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_fp_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/wb-nlp/lib/python3.7/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    506\u001b[0m                 \u001b[0mcache_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfp_closed\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m                 if (\n",
      "\u001b[0;32m~/anaconda3/envs/wb-nlp/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/wb-nlp/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/wb-nlp/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-60427b749c18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWB_PDF_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'{id}.pdf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mdownload_and_store_pdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlink\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mix\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mix\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-67-449df647a3e9>\u001b[0m in \u001b[0;36mdownload_and_store_pdf\u001b[0;34m(fname, link)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0;31m# Download only files that are not yet available locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_with_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-fc7dcf83e11d>\u001b[0m in \u001b[0;36mdownload_with_retry\u001b[0;34m(url, params, max_retries)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mretry_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretry_count\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mmax_retries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pdf_nan_ids = []\n",
    "for ix, (id, link) in enumerate(links.items()):\n",
    "    if not isinstance(link, str):\n",
    "        pdf_nan_ids.append(id)\n",
    "        continue\n",
    "\n",
    "    link = link.replace(\"/text/\", \"/pdf/\")\n",
    "    link = os.path.splitext(link)[0] + \".pdf\"\n",
    "\n",
    "    fname = os.path.join(WB_PDF_DIR, f'{id}.pdf')\n",
    "    download_and_store_pdf(fname, link)\n",
    "\n",
    "    if ix and ix % 10000 == 0:\n",
    "        print(ix, id, link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wb_D32374016',\n",
       " 'wb_D32214123',\n",
       " 'wb_D31352427',\n",
       " 'wb_D31303351',\n",
       " 'wb_D31298833',\n",
       " 'wb_D31298222',\n",
       " 'wb_D31293382',\n",
       " 'wb_D31270193',\n",
       " 'wb_D31254585',\n",
       " 'wb_D31231960']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nan_ids[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Madagascar-AFRICA-EAST-P149323-Social-Safety-Net-Project-Procurement-Plan.pdf'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.basename(link).replace('.txt', '.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "corpus                                                                  WB\n",
       "path_original                              data/corpus/WB/wb_D32374016.txt\n",
       "path_clean                                               /wb_D32374016.txt\n",
       "filename_original                                                      NaN\n",
       "year                                                                  2020\n",
       "major_doc_type                                           Project Documents\n",
       "doc_type                                                  Procurement Plan\n",
       "author                                                                 NaN\n",
       "collection                                                             NaN\n",
       "title                    Kenya - AFRICA EAST - P153349 - National Agric...\n",
       "journal                                                               None\n",
       "volume                                                                None\n",
       "date_published                                                  2020-09-02\n",
       "digital_identifier                                        090224b087d582e1\n",
       "topics_src                                                             NaN\n",
       "url_pdf                                                                NaN\n",
       "url_txt                                                                NaN\n",
       "language_src                                                       English\n",
       "adm_region                                                          Africa\n",
       "geo_region                                                             NaN\n",
       "country                                                              Kenya\n",
       "wb_lending_instrument                                                  NaN\n",
       "wb_product_line                                                        NaN\n",
       "wb_major_theme                                                         NaN\n",
       "wb_theme                                                               NaN\n",
       "wb_sector                                                              NaN\n",
       "wb_subtopic_src                                                        NaN\n",
       "wb_project_id                                                      P153349\n",
       "Name: wb_D32374016, dtype: object"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_final_df.loc[\"wb_D32374016\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "wb_D32658475   NaN\n",
       "wb_D32659506   NaN\n",
       "wb_D32659154   NaN\n",
       "wb_D32659153   NaN\n",
       "wb_D32659160   NaN\n",
       "                ..\n",
       "wb_D29851457   NaN\n",
       "wb_D31825733   NaN\n",
       "wb_D16919195   NaN\n",
       "wb_D32092090   NaN\n",
       "wb_D14551809   NaN\n",
       "Name: author, Length: 296994, dtype: float64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_final_df[\"author\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wb_D32658475',\n",
       " 'wb_D32659506',\n",
       " 'wb_D32659154',\n",
       " 'wb_D32659153',\n",
       " 'wb_D32659160',\n",
       " 'wb_D32658894',\n",
       " 'wb_D32658893',\n",
       " 'wb_D32659159',\n",
       " 'wb_D32654909',\n",
       " 'wb_D32654907']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(links)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://documents.worldbank.org/curated/en/326541607635914974/text/Disclosable-Version-of-the-ISR-Roads-and-Employment-Project-P160223-Sequence-No-08.txt'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links['wb_D32659160']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min 42s, sys: 27 s, total: 10min 9s\n",
      "Wall time: 1h 10min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ret_codes = Parallel(n_jobs=20)(\n",
    "    delayed(download_and_store_file)(os.path.join(WB_TXT_DIR, f'{id}.txt'), link) for id, link in links.items()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus</th>\n",
       "      <th>path_original</th>\n",
       "      <th>path_clean</th>\n",
       "      <th>filename_original</th>\n",
       "      <th>year</th>\n",
       "      <th>major_doc_type</th>\n",
       "      <th>doc_type</th>\n",
       "      <th>author</th>\n",
       "      <th>collection</th>\n",
       "      <th>title</th>\n",
       "      <th>...</th>\n",
       "      <th>adm_region</th>\n",
       "      <th>geo_region</th>\n",
       "      <th>country</th>\n",
       "      <th>wb_lending_instrument</th>\n",
       "      <th>wb_product_line</th>\n",
       "      <th>wb_major_theme</th>\n",
       "      <th>wb_theme</th>\n",
       "      <th>wb_sector</th>\n",
       "      <th>wb_subtopic_src</th>\n",
       "      <th>wb_project_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>wb_19431275</td>\n",
       "      <td>wb</td>\n",
       "      <td>/NLP/CORPUS/WB/TXT_ORIG/wb_19431275.txt</td>\n",
       "      <td>/NLP/CORPUS/WB/TXT_CLEAN/wb_19431275.txt</td>\n",
       "      <td>RAD2066245936.txt</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>Project Documents</td>\n",
       "      <td>Loan Agreement</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Official Documents - Loan Agreement for Loan 8...</td>\n",
       "      <td>...</td>\n",
       "      <td>Europe and Central Asia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Moldova</td>\n",
       "      <td>Development Policy Lending</td>\n",
       "      <td>IBRD/IDA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Public sector governance,Rural development,Fin...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Climate Change and Agriculture,Consumption,Fis...</td>\n",
       "      <td>P143283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>wb_19431317</td>\n",
       "      <td>wb</td>\n",
       "      <td>/NLP/CORPUS/WB/TXT_ORIG/wb_19431317.txt</td>\n",
       "      <td>/NLP/CORPUS/WB/TXT_CLEAN/wb_19431317.txt</td>\n",
       "      <td>RAD1222573634.txt</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>Project Documents</td>\n",
       "      <td>Financing Agreement</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Official Documents - Financing Agreement for C...</td>\n",
       "      <td>...</td>\n",
       "      <td>Europe and Central Asia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Moldova</td>\n",
       "      <td>Development Policy Lending</td>\n",
       "      <td>IBRD/IDA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Public sector governance,Rural development,Fin...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Climate Change and Agriculture,Consumption,Fis...</td>\n",
       "      <td>P143283</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            corpus                            path_original  \\\n",
       "id                                                            \n",
       "wb_19431275     wb  /NLP/CORPUS/WB/TXT_ORIG/wb_19431275.txt   \n",
       "wb_19431317     wb  /NLP/CORPUS/WB/TXT_ORIG/wb_19431317.txt   \n",
       "\n",
       "                                           path_clean  filename_original  \\\n",
       "id                                                                         \n",
       "wb_19431275  /NLP/CORPUS/WB/TXT_CLEAN/wb_19431275.txt  RAD2066245936.txt   \n",
       "wb_19431317  /NLP/CORPUS/WB/TXT_CLEAN/wb_19431317.txt  RAD1222573634.txt   \n",
       "\n",
       "               year     major_doc_type             doc_type  author  \\\n",
       "id                                                                    \n",
       "wb_19431275  2014.0  Project Documents       Loan Agreement     NaN   \n",
       "wb_19431317  2014.0  Project Documents  Financing Agreement     NaN   \n",
       "\n",
       "            collection                                              title  \\\n",
       "id                                                                          \n",
       "wb_19431275        NaN  Official Documents - Loan Agreement for Loan 8...   \n",
       "wb_19431317        NaN  Official Documents - Financing Agreement for C...   \n",
       "\n",
       "             ...               adm_region geo_region  country  \\\n",
       "id           ...                                                \n",
       "wb_19431275  ...  Europe and Central Asia        NaN  Moldova   \n",
       "wb_19431317  ...  Europe and Central Asia        NaN  Moldova   \n",
       "\n",
       "                  wb_lending_instrument wb_product_line wb_major_theme  \\\n",
       "id                                                                       \n",
       "wb_19431275  Development Policy Lending        IBRD/IDA            NaN   \n",
       "wb_19431317  Development Policy Lending        IBRD/IDA            NaN   \n",
       "\n",
       "                                                      wb_theme wb_sector  \\\n",
       "id                                                                         \n",
       "wb_19431275  Public sector governance,Rural development,Fin...       NaN   \n",
       "wb_19431317  Public sector governance,Rural development,Fin...       NaN   \n",
       "\n",
       "                                               wb_subtopic_src wb_project_id  \n",
       "id                                                                            \n",
       "wb_19431275  Climate Change and Agriculture,Consumption,Fis...       P143283  \n",
       "wb_19431317  Climate Change and Agriculture,Consumption,Fis...       P143283  \n",
       "\n",
       "[2 rows x 28 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_final_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/wb536061/wbes2474/NLP/CORPUS/WB'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SCRAPER_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = f\"wb_metadata.csv\"\n",
    "\n",
    "normalized_final_df.reset_index()[METADATA_COLS].to_csv(\n",
    "    os.path.join(SCRAPER_DIR, fname),\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(249805, 28)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(249539, 28)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # Cleanup temporary json files from API\n",
    "# import shutil\n",
    "\n",
    "# shutil.rmtree(API_JSON_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# meta = pd.read_csv('/home/wb536061/wbes2474/NLP/CORPUS/WB/wb_metadata.csv')\n",
    "# meta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(249805, 29)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ssh -A wb536061@w1lxbdatad07 rsync -vuarP \"/home/wb536061/wbes2474/NLP/CORPUS/WB/wb_metadata-2021-02-17\\ 18:06:18.367302.csv\" wb536061@w0lxsnlp01:/decfile2/Modeling/NLP/CORPUS/WB/\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wb_nlp",
   "language": "python",
   "name": "wb_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
