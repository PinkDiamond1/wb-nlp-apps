{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"text-align:center\">World Bank Documents and Reports Cleaner</div>\n",
    "\n",
    "This notebook implements the cleaner classes for the data from the **Documents and Reports API**. This cleaner module provides respelling functionality as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Requirements:\n",
    "# # Please install spacy library and the `en` model\n",
    "# !~/anaconda3/bin/pip install spacy\n",
    "# !~/anaconda3/bin/python -m spacy download en\n",
    "# !~/anaconda3/bin/pip install contexttimer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing pattern as alternative to pyenchant\n",
    "#### Note, the pattern module's spell checking function is quite slow!\n",
    "\n",
    "\n",
    "Clone first the development repo ([pypi version is outdated](https://github.com/clips/pattern/issues/217\n",
    "))\n",
    "- `git clone -b development https://github.com/clips/pattern`\n",
    "- `cd pattern/`\n",
    "- Commenting out `\"mysqlclient\"` inside the `setup.py` file may be necessary if errors are encountered in the next step.\n",
    "- `pip install .`\n",
    "\n",
    "Make sure that the `pip` that you use corresponds to the python installation that you will use to run the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from /home/avsolatorio/WBG/wb-nlp/SCRIPTS/acronyms/AcronymModule.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from acronyms.AcronymModule import AcronymMapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "nltk.data.path.append(\"/R/nltk_data\")\n",
    "\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import PlaintextCorpusReader, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "from langdetect import detect, detect_langs\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing as mp\n",
    "import multiprocessing\n",
    "\n",
    "from contexttimer import Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from phoenix.cleaner.cleaner import Cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spelling correction module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorpusCleaner(Cleaner):\n",
    "\n",
    "    # Clean documents using spell checker\n",
    "    def batch_clean_docs(self, doclist, batch_size=None, save_docs=False, collect_text_log=False, collect_spell_errors=False, skip_existing=True, default_docs_per_worker=20):\n",
    "        if batch_size is None:\n",
    "            # Use a multiplier for efficient usage of workers\n",
    "            batch_size = default_docs_per_worker * self.num_workers\n",
    "\n",
    "        file_counter_x = 0\n",
    "        input_folder  = self.input_folder\n",
    "        output_folder = self.output_folder\n",
    "        \n",
    "        #log statistics\n",
    "        lang_log = {} # Lang info per document - uses the format - lang_log[fileid]=('lang', 'score')\n",
    "        text_log = {} # Errors count per document\n",
    "        token_log = {} # Tokens count per document\n",
    "        skipped_log = {} # Documents not processed\n",
    "        spell_errors = {}\n",
    "        exception_log = {}\n",
    "        write_status_log = {}\n",
    "        \n",
    "        log_interval = batch_size\n",
    "\n",
    "        with Parallel(n_jobs=self.num_workers, backend='multiprocessing') as pool:\n",
    "            # Cleaning all text files found in input in folder\n",
    "            batch = []\n",
    "            for ix, fileid in enumerate(doclist):\n",
    "                if ix % log_interval == 0:\n",
    "                    self.logger(f'Docset {ix}')\n",
    "\n",
    "                file_counter_x += 1\n",
    "                if fileid.endswith('.txt'):    # text files only \n",
    "\n",
    "                    filen = os.path.join(input_folder, fileid)     # input file \n",
    "                    newfile = os.path.join(output_folder, fileid)   # output file\n",
    "                    \n",
    "                    if not os.path.isfile(filen):\n",
    "                        self.logger(f\"No input file: {fileid}\")\n",
    "                        continue\n",
    "\n",
    "                    # Skip if output file already exists\n",
    "                    if os.path.isfile(newfile) and skip_existing:\n",
    "                        # self.logger(f\"Output file exists: {fileid}. Skipping...\")\n",
    "                        continue\n",
    "\n",
    "                    if len(batch) != batch_size:\n",
    "                        batch.append(filen)\n",
    "\n",
    "                    else:\n",
    "                        with Timer() as timer:\n",
    "                            doc_outputs = pool((delayed(self.clean_doc)(fln, save_doc=save_docs) for fln in batch))\n",
    "                            # doc_outputs = Parallel(n_jobs=self.num_workers, backend='multiprocessing')(delayed(self.clean_doc)(fln, save_doc=save_docs) for fln in batch)\n",
    "                            # doc_outputs = pool.map(self.clean_doc, [(fln, save_docs) for fln in batch], chunksize=batch_size)\n",
    "\n",
    "                            for doc_output in doc_outputs:\n",
    "\n",
    "                                lang_log.update(doc_output['lang'])\n",
    "                                token_log.update(doc_output['tokens'])\n",
    "                                skipped_log.update(doc_output['skipped'])  \n",
    "                                exception_log.update(doc_output['exception'])\n",
    "                                write_status_log.update(doc_output['write_status'])\n",
    "\n",
    "                                if collect_text_log:\n",
    "                                    # Don't do this if you're processing a lot of docs\n",
    "                                    text_log.update(doc_output['text'])\n",
    "                                if collect_spell_errors:\n",
    "                                    spell_errors.update(doc_output['spell_errors'])\n",
    "\n",
    "                            batch = []\n",
    "                            \n",
    "                        self.logger(f'Set {ix}: {log_interval} items for {timer.elapsed:.2f} seconds.')\n",
    "\n",
    "            if batch:\n",
    "                doc_outputs = pool((delayed(self.clean_doc)(fln, save_doc=save_docs) for fln in batch))\n",
    "                # doc_outputs = Parallel(n_jobs=self.num_workers, backend='multiprocessing')(delayed(self.clean_doc)(fln, save_doc=save_docs) for fln in batch)\n",
    "                # doc_outputs = pool.map(self.clean_doc, [(fln, save_docs) for fln in batch], chunksize=batch_size)\n",
    "\n",
    "                for doc_output in doc_outputs:\n",
    "\n",
    "                    lang_log.update(doc_output['lang'])\n",
    "                    token_log.update(doc_output['tokens'])\n",
    "                    skipped_log.update(doc_output['skipped']) \n",
    "                    exception_log.update(doc_output['exception'])\n",
    "                    write_status_log.update(doc_output['write_status'])\n",
    "\n",
    "                    if collect_text_log:\n",
    "                        # Don't do this if you're processing a lot of docs\n",
    "                        text_log.update(doc_output['text'])\n",
    "                    if collect_spell_errors:\n",
    "                        spell_errors.update(doc_output['spell_errors'])\n",
    "\n",
    "        output_log = {}\n",
    "        output_log['lang'] = lang_log\n",
    "        output_log['tokens'] = token_log\n",
    "        output_log['text'] = text_log\n",
    "        output_log['spell_errors'] = spell_errors\n",
    "        output_log['skipped'] = skipped_log\n",
    "        output_log['exception'] = exception_log\n",
    "        output_log['write_status'] = write_status_log\n",
    "\n",
    "        return output_log\n",
    "\n",
    "    # Clean a single document using spell checker    \n",
    "    def clean_doc(self, filepath, save_doc=False):  # args):\n",
    "        # filepath, *save_doc = args\n",
    "\n",
    "        if save_doc is None:\n",
    "            save_doc = False\n",
    "\n",
    "        # log statistics\n",
    "        lang_log = {}  # lang info per document - uses the format - lang_log[fileid]={'score','lang'}\n",
    "        spell_errors = {}\n",
    "        token_log = {}  # Tokens count per document\n",
    "        text_errors = {}\n",
    "        text_log = {}\n",
    "        skipped_log = {}\n",
    "        exception_log = {}\n",
    "        write_status_log = {}\n",
    "\n",
    "        filename = filepath.split('/')[-1]\n",
    "    \n",
    "        fileid = filename.strip('.txt')\n",
    "        \n",
    "        with open(filepath, 'rb') as fl:\n",
    "            # Use context so that the file will be closed automatically upon exit from the context.\n",
    "            text = fl.read()\n",
    "            text = text.decode('utf-8', errors='ignore')\n",
    "        \n",
    "        cleaning_output = self.clean_text(text, filen=fileid)\n",
    "        text = cleaning_output['text']\n",
    "        \n",
    "        lang_log[fileid] = cleaning_output['lang']\n",
    "        token_log[fileid] = cleaning_output['token']\n",
    "        skipped_log[fileid] = cleaning_output['skipped']\n",
    "        text_log[fileid] = text\n",
    "        spell_errors[fileid] = cleaning_output['spell_errors']\n",
    "        exception_log[fileid] = cleaning_output['exception']\n",
    "        write_status_log[fileid] = cleaning_output['write_status']\n",
    "        \n",
    "        if save_doc and cleaning_output['write_status']:\n",
    "            with open(os.path.join(self.output_folder, filename), 'w') as fl:\n",
    "                fl.write(text)\n",
    "\n",
    "        # return logs\n",
    "        output_log = {}\n",
    "        output_log['lang'] = lang_log\n",
    "        output_log['tokens'] = token_log\n",
    "        output_log['text'] = text_log\n",
    "        output_log['spell_errors'] = spell_errors\n",
    "        output_log['skipped'] = skipped_log\n",
    "        output_log['exception'] = exception_log\n",
    "        output_log['write_status'] = write_status_log\n",
    "\n",
    "        return output_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ParallelCorpusCleaner(Cleaner):\n",
    "\n",
    "    # Clean documents using spell checker    \n",
    "    def batch_clean_docs(self, doclist, batch_size=None, save_docs=False, collect_text_log=False, collect_spell_errors=False, skip_existing=True):\n",
    "        if batch_size is None:\n",
    "            # Use a multiplier for efficient usage of workers\n",
    "            batch_size = 4 * self.num_workers\n",
    "\n",
    "        file_counter_x = 0\n",
    "        input_folder  = self.input_folder\n",
    "        output_folder = self.output_folder\n",
    "        \n",
    "        #log statistics\n",
    "        lang_log = {} # Lang info per document - uses the format - lang_log[fileid]=('lang', 'score')\n",
    "        text_log = {} # Errors count per document\n",
    "        token_log = {} # Tokens count per document\n",
    "        skipped_log = {} # Documents not processed\n",
    "        spell_errors = {}\n",
    "        exception_log = {}\n",
    "        write_status_log = {}\n",
    "        \n",
    "        log_interval = batch_size\n",
    "        \n",
    "        process_output_manager = multiprocessing.Manager()\n",
    "        process_output_dict = process_output_manager.dict()\n",
    "\n",
    "        batch = {}\n",
    "        ix = 0\n",
    "        while True:\n",
    "            try:\n",
    "                if len(batch) < batch_size:\n",
    "                    fileid = doclist.pop(0)\n",
    "                    # print(f'Processing {ix}: {fileid}')\n",
    "                    ix += 1\n",
    "\n",
    "                    if ix % log_interval == 0:\n",
    "                        self.logger(f'Docset {ix}')\n",
    "\n",
    "                    if fileid.endswith('.txt'):    # text files only \n",
    "                        filen = os.path.join(input_folder, fileid)     # input file \n",
    "                        newfile = os.path.join(output_folder, fileid)   # output file\n",
    "\n",
    "                        if not os.path.isfile(filen):\n",
    "                            self.logger(f\"No input file: {fileid}\")\n",
    "                            continue\n",
    "\n",
    "                        # Skip if output file already exists\n",
    "                        if os.path.isfile(newfile) and skip_existing:\n",
    "                            p = multiprocessing.Process(target=self.load_existing_and_extract_metadata, args=(fileid, newfile, save_docs, process_output_dict))  # , kwargs=kwargs)\n",
    "                            batch[fileid] = p\n",
    "                            p.start()\n",
    "                            # self.logger(f\"Output file exists: {fileid}. Skipping...\")\n",
    "                            continue\n",
    "\n",
    "                        # kwargs = {'process_output_dict': process_output_dict, 'save_doc': save_docs}                    \n",
    "                        p = multiprocessing.Process(target=self.clean_doc, args=(fileid, filen, save_docs, process_output_dict))  # , kwargs=kwargs)\n",
    "                        batch[fileid] = p\n",
    "                        p.start()\n",
    "                else:\n",
    "                    completed_data = set(process_output_dict.keys())\n",
    "\n",
    "                    for fld in completed_data:\n",
    "                        # print(f'Completed {fld}')\n",
    "                        if fld not in batch:\n",
    "                            continue\n",
    "                        pk = batch.pop(fld)\n",
    "                        pk.join()\n",
    "\n",
    "                        # This shouldn't be necessary but still doing this just to be safe... :)\n",
    "                        if pk.is_alive():\n",
    "                            pk.terminate()\n",
    "                            pk.join()\n",
    "\n",
    "                        fileid = doclist.pop(0)\n",
    "                        ix += 1\n",
    "                        \n",
    "                        # print(f'Starting {fileid}')\n",
    "\n",
    "                        if ix % log_interval == 0:\n",
    "                            self.logger(f'Docset {ix}')\n",
    "\n",
    "                        if fileid.endswith('.txt'):    # text files only \n",
    "                            filen = os.path.join(input_folder, fileid)     # input file \n",
    "                            newfile = os.path.join(output_folder, fileid)   # output file\n",
    "\n",
    "                            if not os.path.isfile(filen):\n",
    "                                self.logger(f\"No input file: {fileid}\")\n",
    "                                continue\n",
    "\n",
    "                            # Skip if output file already exists\n",
    "                            if os.path.isfile(newfile) and skip_existing:\n",
    "                                p = multiprocessing.Process(target=self.load_existing_and_extract_metadata, args=(fileid, newfile, save_docs, process_output_dict))  # , kwargs=kwargs)\n",
    "                                batch[fileid] = p\n",
    "                                p.start()\n",
    "                                # self.logger(f\"Output file exists: {fileid}. Skipping...\")\n",
    "                                continue\n",
    "\n",
    "                            # kwargs = {'process_output_dict': process_output_dict, 'save_doc': save_docs}                    \n",
    "                            p = multiprocessing.Process(target=self.clean_doc, args=(fileid, filen, save_docs, process_output_dict))  # , kwargs=kwargs)\n",
    "                            batch[fileid] = p\n",
    "                            p.start()\n",
    "                            \n",
    "            except Exception as e:\n",
    "                print(f'Exception received: {e.args[0]}')\n",
    "                bfileids = set(batch.keys())\n",
    "                for fileid in bfileids:\n",
    "                    p = batch.pop(fileid)\n",
    "                    p.join()\n",
    "\n",
    "                    if p.is_alive():\n",
    "                        p.terminate()\n",
    "                        p.join()\n",
    "                break\n",
    "\n",
    "        # Cleanup just in case... \n",
    "        bfileids = set(batch.keys())\n",
    "        for fileid in bfileids:\n",
    "            p = batch.pop(fileid)\n",
    "            p.join()\n",
    "\n",
    "            if p.is_alive():\n",
    "                p.terminate()\n",
    "                p.join()\n",
    "                \n",
    "        for fileid in process_output_dict.keys():\n",
    "            doc_output = process_output_dict[fileid]\n",
    "\n",
    "            lang_log.update(doc_output['lang'])\n",
    "            token_log.update(doc_output['tokens'])\n",
    "            skipped_log.update(doc_output['skipped'])  \n",
    "            exception_log.update(doc_output['exception'])\n",
    "            write_status_log.update(doc_output['write_status'])\n",
    "\n",
    "            if collect_text_log:\n",
    "                # Don't do this if you're processing a lot of docs\n",
    "                text_log.update(doc_output['text'])\n",
    "            if collect_spell_errors:\n",
    "                spell_errors.update(doc_output['spell_errors'])\n",
    "\n",
    "        output_log = {}\n",
    "        output_log['lang'] = lang_log\n",
    "        output_log['tokens'] = token_log\n",
    "        output_log['text'] = text_log\n",
    "        output_log['spell_errors'] = spell_errors\n",
    "        output_log['skipped'] = skipped_log\n",
    "        output_log['exception'] = exception_log\n",
    "        output_log['write_status'] = write_status_log\n",
    "\n",
    "        return output_log\n",
    "\n",
    "    # Clean a single document using spell checker    \n",
    "    def clean_doc(self, fileid, filepath, save_doc=False, process_output_dict=None):  # args):\n",
    "        proc_fileid = fileid\n",
    "        # filepath, *save_doc = args\n",
    "\n",
    "        if save_doc is None:\n",
    "            save_doc = False\n",
    "\n",
    "        # log statistics\n",
    "        lang_log = {}  # lang info per document - uses the format - lang_log[fileid]={'score','lang'}\n",
    "        spell_errors = {}\n",
    "        token_log = {}  # Tokens count per document\n",
    "        text_errors = {}\n",
    "        text_log = {}\n",
    "        skipped_log = {}\n",
    "        exception_log = {}\n",
    "        write_status_log = {}\n",
    "\n",
    "        filename = filepath.split('/')[-1]\n",
    "    \n",
    "        fileid = filename.strip('.txt')\n",
    "        \n",
    "        with open(filepath, 'rb') as fl:\n",
    "            # Use context so that the file will be closed automatically upon exit from the context.\n",
    "            text = fl.read()\n",
    "            text = text.decode('utf-8', errors='ignore')\n",
    "            text = text.lower()\n",
    "        \n",
    "        cleaning_output = self.clean_text(text, filen=fileid)\n",
    "        text = cleaning_output['text']\n",
    "        \n",
    "        lang_log[fileid] = cleaning_output['lang']\n",
    "        token_log[fileid] = cleaning_output['token']\n",
    "        skipped_log[fileid] = cleaning_output['skipped']\n",
    "        text_log[fileid] = text\n",
    "        spell_errors[fileid] = cleaning_output['spell_errors']\n",
    "        exception_log[fileid] = cleaning_output['exception']\n",
    "        write_status_log[fileid] = cleaning_output['write_status']\n",
    "        \n",
    "        if save_doc and cleaning_output['write_status']:\n",
    "            with open(os.path.join(self.output_folder, filename), 'w') as fl:\n",
    "                fl.write(text)\n",
    "\n",
    "        # return logs\n",
    "        output_log = {}\n",
    "        output_log['lang'] = lang_log\n",
    "        output_log['tokens'] = token_log\n",
    "        output_log['text'] = text_log\n",
    "        output_log['spell_errors'] = spell_errors\n",
    "        output_log['skipped'] = skipped_log\n",
    "        output_log['exception'] = exception_log\n",
    "        output_log['write_status'] = write_status_log\n",
    "        \n",
    "        if process_output_dict is not None:\n",
    "            process_output_dict[proc_fileid] = output_log\n",
    "        else: \n",
    "            return output_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# payload = dict(\n",
    "#     lang=lang_log,\n",
    "#     token=token_log,\n",
    "#     text=text_log,\n",
    "#     skipped=skipped_log,\n",
    "#     spell_errors=spell_errors,\n",
    "#     exception=exp,\n",
    "#     write_status=write_status,\n",
    "# )\n",
    "\n",
    "# payload = {k: v if not isinstance(v, set) else list(v) for k, v in payload.items()}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Input document text and process documents with at least `ignore_length` characters.\n",
    "\n",
    "Step 1: Detect countries from the document if a country map file is provided.\n",
    "Step 2: Apply lemmatization if specified (lemmatizer options: spacy or nltk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}