{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document deduplication\n",
    "\n",
    "Perform a basic deduplication of documents using cosine similarity.\n",
    "\n",
    "\n",
    "Use **scipy=1.3.1** to avoid library load error `(Library not loaded: @rpath/libopenblas.dylib)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import pickle\n",
    "import warnings\n",
    "import importlib\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim import corpora\n",
    "from gensim.similarities import Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<module 'wb_nlp.cleaning.cleaner' from '/Users/avsolatorio/WBG/wb_nlp/src/wb_nlp/cleaning/cleaner.py'>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "from wb_nlp.cleaning import cleaner\n",
    "from wb_nlp import dir_manager\n",
    "importlib.reload(cleaner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configs and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '../../src/wb_nlp'\n",
    "EXTENSION = 'py'\n",
    "\n",
    "SIMILARITY_THRESHOLD = 0.9\n",
    "\n",
    "DUPLICATES_DIR = dir_manager.get_data_dir('preprocessed', 'duplicates')\n",
    "\n",
    "if not os.path.isdir(DUPLICATES_DIR):\n",
    "    os.makedirs(DUPLICATES_DIR)\n",
    "\n",
    "DUPLICATES_CORPUS_FILE = os.path.join(DUPLICATES_DIR, 'corpus_generator.pickle')\n",
    "SIMILARITY_OUTPUT_PREFIX = os.path.join(DUPLICATES_DIR, 'corpus_similarity.gensim')\n",
    "DUPLICATE_DOC_IDS_FILE = os.path.join(DUPLICATES_DIR, 'duplicate_doc_ids.dict.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_cleaner = cleaner.SimpleCleaner()\n",
    "simple_corpus_generator = cleaner.CorpusCleaner(DATA_DIR, simple_cleaner.clean_text, extension=EXTENSION)\n",
    "\n",
    "# lda_corpus_generator = cleaner.CorpusCleaner(DATA_DIR, lda_cleaner.clean_text, extension=EXTENSION)\n",
    "# lda_cleaner = cleaner.LDACleaner()\n",
    "\n",
    "# word2vec_corpus_generator = cleaner.CorpusCleaner(DATA_DIR, word2vec_cleaner.clean_text, extension=EXTENSION)\n",
    "# word2vec_cleaner = cleaner.Word2VecCleaner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_generator = simple_corpus_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load documents and compute phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    corpus_generator.reset()\n",
    "except ValueError:\n",
    "    pass\n",
    "\n",
    "bigram = Phrases(corpus_generator, min_count=1)\n",
    "bigram_phraser = Phraser(bigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the bigram docs, dictionary, and corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_bigrams = list(corpus_generator.stream_gensim_transformer(bigram_phraser))\n",
    "dictionary = corpora.Dictionary(doc_bigrams)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in doc_bigrams]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the similarity index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = Similarity(\n",
    "    corpus=corpus,\n",
    "    num_features=len(dictionary),\n",
    "    output_prefix=SIMILARITY_OUTPUT_PREFIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0.17149858 0.17149858 0.17149858 0.17149858 0.34299716 0.17149858\n 0.17149858 0.17149858 0.17149858 0.17149858 0.17149858 0.17149858\n 0.34299716 0.17149858 0.17149858 0.17149858 0.17149858 0.17149858\n 0.17149858 0.17149858 0.17149858 0.17149858 0.17149858 0.17149858\n 0.17149858 0.17149858 0.17149858 0.17149858 0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.        ]\n[0.99999994 0.01642659]\n"
    }
   ],
   "source": [
    "print(index.vector_by_id(0))\n",
    "print(index.similarity_by_id(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect duplicate documents based on similarity threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_doc_ids = set()\n",
    "duplicated_docs = []\n",
    "\n",
    "for doc_id, doc in enumerate(corpus):\n",
    "    indices = np.where(index.similarity_by_id(0) > SIMILARITY_THRESHOLD)[0]\n",
    "\n",
    "    if len(indices) == 1:\n",
    "        unique_doc_ids.add(doc_id)\n",
    "    else:\n",
    "        duplicated_docs.append(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = dict(\n",
    "    unique_doc_ids=unique_doc_ids,\n",
    "    duplicated_docs=duplicated_docs)\n",
    "\n",
    "with open(DUPLICATE_DOC_IDS_FILE, 'wb') as fl:\n",
    "    pickle.dump(payload, fl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the corpus generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_generator.save(DUPLICATES_CORPUS_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load saved corpus generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_corpus_generator = cleaner.CorpusCleaner(DATA_DIR, simple_cleaner.clean_text, extension=EXTENSION)\n",
    "load_corpus_generator.load(DUPLICATES_CORPUS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load_corpus_generator.reset()\n",
    "# for i in load_corpus_generator:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wb_nlp",
   "language": "python",
   "name": "wb_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}