{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "corpus_dir = Path(dir_manager.get_data_dir(\"corpus\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_use = Counter()\n",
    "word_doc = Counter()\n",
    "\n",
    "newline_dash = re.compile(r\"(\\S+)-\\s+(\\S+)\")\n",
    "for ix, fpath in enumerate(corpus_dir.glob(\"*/EN_TXT_ORIG/*.txt\")):\n",
    "\n",
    "    with open(fpath, \"rb\") as open_file:\n",
    "        text = open_file.read().decode(\"utf-8\", errors=\"ignore\")\n",
    "        text = newline_dash.sub(r\"\\1\\2\", text).lower()\n",
    "        doc_use = Counter(re.findall(r\"[a-z]+\", text))\n",
    "        for word in doc_use:\n",
    "            word_doc[word] += 1\n",
    "\n",
    "    word_use.update(doc_use)\n",
    "\n",
    "with open(corpus_dir / \"en_corpus_word_use.dict.pickle\", \"wb\") as open_file:\n",
    "    pickle.dump(word_use, open_file)\n",
    "\n",
    "with open(corpus_dir / \"en_corpus_word_doc.dict.pickle\", \"wb\") as open_file:\n",
    "    pickle.dump(word_doc, open_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enchant\n",
    "import re\n",
    "VALID_TOKEN_PAT = re.compile(\"^[a-z]+$\")\n",
    "en_dict = enchant.Dict(\"en_US\")\n",
    "\n",
    "def check(word):\n",
    "    is_en = en_dict.check(word)\n",
    "    if not is_en:\n",
    "        s = en_dict.suggest(word)\n",
    "        is_en = word.lower() in {i.lower() for i in s}\n",
    "    return is_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_doc_df = pd.Series(word_doc).sort_values(ascending=False)\n",
    "word_doc_df.name = \"freq\"\n",
    "\n",
    "word_use_df = pd.Series(word_use).sort_values(ascending=False)\n",
    "word_use_df.name = \"freq\"\n",
    "\n",
    "word_usage_rate = word_use_df / word_doc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisite for next step: Wikipedia word list\n",
    "\n",
    "1. `cd /data/wb536061 && git clone https://github.com/IlyaSemenov/wikipedia-word-frequency.git && cd wikipedia-word-frequency`\n",
    "2. Download latest wiki data from [https://dumps.wikimedia.org/enwiki/latest/](https://dumps.wikimedia.org/enwiki/latest/) :: [https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2](https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2) to /data/wb536061/wb_nlp/data/external/wikipedia/\n",
    "    1. Download in VM then use WinSCP to transfer the data to the server\n",
    "3. Run `./gather_wordfreq.py ../wb_nlp/data/external/wikipedia/enwiki-latest-pages-articles.xml.bz2 > wordfreq-enwiki-latest-pages-articles.xml.bz2.txt`\n",
    "4. Copy data to repo `cp wordfreq-enwiki-latest-pages-articles.xml.bz2.txt ~/wb_nlp/data/whitelists/whitelists/`\n",
    "5. Add and commit to repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_count = pd.read_csv(\n",
    "    dir_manager.get_data_dir(\"whitelists\", \"whitelists\", \"wordfreq-enwiki-latest-pages-articles.xml.bz2.txt\"),\n",
    "    sep=\" \", names=[\"word\", \"freq\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_doc_frame = word_doc_df.to_frame().head(100000)\n",
    "word_doc_frame[\"en\"] = word_doc_frame.index.map(check)\n",
    "word_doc_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = word_doc_frame[word_doc_frame[\"en\"] == False].index.intersection(wiki_count.head(500000)[\"word\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dir_manager.get_data_dir(\"whitelists\", \"whitelists\", \"doc-freq-wiki-wordlist.txt\"), \"w\") as open_file:\n",
    "    for i in w:\n",
    "        if VALID_TOKEN_PAT.match(i):\n",
    "            open_file.write(i.strip() + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oov_words = word_doc_df.index.difference(wiki_count.word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wb_nlp",
   "language": "python",
   "name": "wb_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
