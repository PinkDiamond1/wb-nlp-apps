{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia-based vocabulary\n",
    "\n",
    "Part of the text cleaning is to recover misspelled tokens in documents. The base toolkit to implement the spell checking component is the [pyenchant](https://github.com/pyenchant/pyenchant) library.\n",
    "\n",
    "While the existing solution works, there are some issues that this implementation face. The most important of which is the detection of emerging or novel words. Recently, Covid has become a common term but the vocabulary of the dictionary that we're using doesn't contain it. This means that when a document containing this term is processed, it will be classified as misspelled and the pipeline will be try to \"fix\" it.\n",
    "\n",
    "To remedy this, we modify the solution by updating the standard vocabulary with the vocabulary from a dynamically updating corpus. In this case, we choose the [Wikipedia corpus](https://dumps.wikimedia.org/enwiki/latest/) as the source of our updated vocabulary.\n",
    "\n",
    "\n",
    "\n",
    "The solution for this is summarized as follows:\n",
    "\n",
    "1. Download the latest wikipedia corpus from https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2.\n",
    "2. Use gensim to process and collect the tokens in the corpus.\n",
    "\n",
    "        ```python\n",
    "        from gensim.corpora import WikiCorpus\n",
    "        from wb_nlp.dir_manager import get_data_dir\n",
    "        import os\n",
    "\n",
    "        wiki_dump = get_data_dir('raw', 'wiki', 'enwiki-latest-pages-articles.xml.bz2')\n",
    "        wiki_dict = get_data_dir('processed', 'wiki')\n",
    "        if not os.path.isdir(wiki_dict):\n",
    "            os.makedirs(wiki_dict)\n",
    "\n",
    "        wiki = WikiCorpus(\n",
    "                wiki_dump, processes=max(1, os.cpu_count() - 4),\n",
    "                lemmatize=False,\n",
    "                article_min_tokens=50, token_min_len=2,\n",
    "                token_max_len=50, lower=True)\n",
    "\n",
    "        wiki.dictionary.save(os.path.join(wiki_dict, 'wiki_en.gensim.dict.pickle'))\n",
    "        ```\n",
    "\n",
    "3. Filter the tokens using the `.cfs` and `.idf` attributes of the `wiki.dictionary`.\n",
    "4. Update use an updated dictionary.\n",
    "\n",
    "        ```\n",
    "        import enchant\n",
    "        en_dict = enchant.DictWithPWL(\"en_US\", \"wiki_en.txt\")\n",
    "        ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "''"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from wb_nlp.dir_manager import get_data_dir\n",
    "import urllib.request\n",
    "\n",
    "wiki_meta_url = 'https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2-rss.xml'\n",
    "\n",
    "soup = BeautifulSoup(requests.get(wiki_meta_url).content, 'html.parser')\n",
    "wiki_latest_url = BeautifulSoup(soup.find('item').find('description').text).find('a', href=True)['href']\n",
    "\n",
    "wiki_latest = wiki_latest_url.split('/')[-1]\n",
    "wiki_data_path = get_data_dir('raw', 'wiki')\n",
    "wiki_data_file = os.path.join(wiki_data_path, wiki_latest)\n",
    "\n",
    "if not os.path.isdir(wiki_data_path):\n",
    "    os.makedirs(wiki_data_path)\n",
    "\n",
    "if not os.path.isfile(wiki_data_file):\n",
    "    local_filename, headers = urllib.request.urlretrieve(wiki_latest_url, wiki_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from gensim.corpora import WikiCorpus\n",
    "\n",
    "wiki_dict = get_data_dir('processed', 'wiki')\n",
    "\n",
    "if not os.path.isdir(wiki_dict):\n",
    "    os.makedirs(wiki_dict)\n",
    "\n",
    "wiki = WikiCorpus(\n",
    "        wiki_data_file, processes=max(1, os.cpu_count() - 4),\n",
    "        lemmatize=False,\n",
    "        article_min_tokens=50, token_min_len=2,\n",
    "        token_max_len=50, lower=True)\n",
    "\n",
    "wiki.dictionary.save(os.path.join(wiki_dict, 'wiki_en.gensim.dict.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wb_nlp",
   "language": "python",
   "name": "wb_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}