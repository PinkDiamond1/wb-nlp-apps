{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "%run ../path_manager.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def notify(fname, message):\n",
    "    message = f'{datetime.now()}: {message}'\n",
    "    with open(fname, 'a+') as fl:\n",
    "        fl.write(message + '\\n')\n",
    "        \n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_map = {}\n",
    "\n",
    "\n",
    "def file_generator(cleaned_files):\n",
    "    for ix, f in enumerate(cleaned_files):\n",
    "        if ix and (ix % 1000) == 0:\n",
    "            notify('run_log.txt', f'Processed {ix} documents...')\n",
    "            \n",
    "        if f not in file_map:\n",
    "            with open(f) as fl:\n",
    "                file_map[f] = fl.read().split()\n",
    "        yield file_map[f]\n",
    "\n",
    "\n",
    "def phraser_generator(cleaned_files, phrasers=None):\n",
    "    \n",
    "    if phrasers is None:\n",
    "        phrasers = []\n",
    "        \n",
    "    corpora = file_generator(cleaned_files=cleaned_files)\n",
    "    for corpus in corpora:\n",
    "        for phraser in phrasers:\n",
    "            corpus = phraser[corpus]\n",
    "        yield corpus\n",
    "\n",
    "\n",
    "def train_phrasers(cleaned_files, threshold=0.5, scoring='npmi'):\n",
    "    '''\n",
    "    Empirical test shows that npmi with threshold of 0.5 works better for batches of 10k-15k.\n",
    "    '''\n",
    "    phrasers = []\n",
    "\n",
    "    notify('run_log.txt', 'Start generating bigrams...')\n",
    "\n",
    "    sentences = phraser_generator(cleaned_files=cleaned_files, phrasers=phrasers)\n",
    "    bigram_phrases = Phrases(sentences, threshold=threshold, scoring=scoring)  # train model\n",
    "    bigram_model = Phraser(bigram_phrases)\n",
    "    phrasers.append(bigram_model)\n",
    "\n",
    "    notify('run_log.txt', 'Start generating trigrams...')\n",
    "    sentences = phraser_generator(cleaned_files=cleaned_files, phrasers=phrasers)\n",
    "    trigram_phrases = Phrases(sentences, threshold=threshold, scoring=scoring)  # train model\n",
    "    trigram_model = Phraser(trigram_phrases)\n",
    "    phrasers.append(trigram_model)\n",
    "\n",
    "    # print('Start generating 4-grams...')\n",
    "    # sentences = phraser_generator(clean_file_dir=clean_file_dir, phrasers=phrasers)\n",
    "    # four_gram_phrases = Phrases(sentences, min_count=1, threshold=10)  # threshold=0.5, scoring='npmi')  # train model\n",
    "    # four_gram_model = Phraser(four_gram_phrases)\n",
    "    # phrasers.append(four_gram_model)\n",
    "\n",
    "    # print('Start generating 5-grams...')\n",
    "    # sentences = phraser_generator(clean_file_dir=clean_file_dir, phrasers=phrasers)\n",
    "    # five_gram_phrases = Phrases(sentences, min_count=1, threshold=10)  # threshold=0.5, scoring='npmi')  # train model\n",
    "    # five_gram_model = Phraser(five_gram_phrases)\n",
    "    # phrasers.append(five_gram_model)\n",
    "    \n",
    "    return phrasers\n",
    "\n",
    "\n",
    "def collect_ngrams(cleaned_files, phrasers, filename=None):\n",
    "    ngrams = Counter()\n",
    "\n",
    "    for ngram_corpus in phraser_generator(cleaned_files=cleaned_files, phrasers=phrasers):    \n",
    "        ngrams.update(set([i for i in ngram_corpus if '_' in i]))\n",
    "\n",
    "    ngram_freq = pd.DataFrame(ngrams.most_common(), columns=['ngram', 'occ'])\n",
    "\n",
    "    if filename is not None:\n",
    "        ngram_freq.to_csv(filename, index=False)\n",
    "    \n",
    "    return ngram_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORPUS_ID = 'WB'\n",
    "# NUM_SAMPLES = 10000\n",
    "# all_cleaned_files = glob.glob(os.path.join(get_txt_clean_path(CORPUS_ID), '*.txt'))\n",
    "# phrasers_map = {}\n",
    "\n",
    "# all_ngram_freqs = []\n",
    "\n",
    "# for ix in range(2):  # 30):\n",
    "#     print(f\"Starting with sample {ix + 1}...\")\n",
    "    \n",
    "#     np.random.seed(ix + 1)\n",
    "#     cleaned_files = np.random.choice(all_cleaned_files, NUM_SAMPLES, replace=False)\n",
    "#     _phrasers = train_phrasers(cleaned_files)\n",
    "#     phrasers_map[ix] = _phrasers\n",
    "    \n",
    "#     ngram_freq = collect_ngrams(\n",
    "#         cleaned_files, _phrasers,\n",
    "#         filename=os.path.join(get_corpus_path(CORPUS_ID), f'{CORPUS_ID.lower()}_ngrams-random_{ix}.csv')\n",
    "#     )\n",
    "    \n",
    "#     all_ngram_freqs.append(ngram_freq.set_index('ngram'))\n",
    "    \n",
    "# all_ngram_freqs_df = pd.concat(all_ngram_freqs, axis=1).sum(axis=1)\n",
    "\n",
    "# all_ngram_freqs_df.index.name = 'ngram'\n",
    "# all_ngram_freqs_df.name = 'occ'\n",
    "# all_ngram_freqs_df = all_ngram_freqs_df.reset_index()\n",
    "\n",
    "# all_ngram_freqs_df = all_ngram_freqs_df.sort_values('occ', ascending=False).reset_index(drop='index')\n",
    "# # all_ngram_freqs_df.to_csv(os.path.join(get_corpus_path(CORPUS_ID), f'{CORPUS_ID.lower()}_ngrams-random_all.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-01 23:15:04.879418: Generating ngrams for IMF...\n",
      "freq_cutoff: 131.68\n",
      "2019-11-01 23:15:04.913399: Starting with sample 1...\n",
      "2019-11-01 23:15:04.913669: Start generating bigrams...\n",
      "2019-11-01 23:15:17.988119: Processed 1000 documents...\n",
      "2019-11-01 23:15:32.398013: Processed 2000 documents...\n",
      "2019-11-01 23:15:46.023481: Processed 3000 documents...\n",
      "2019-11-01 23:15:59.407703: Processed 4000 documents...\n",
      "2019-11-01 23:16:12.655955: Processed 5000 documents...\n",
      "2019-11-01 23:16:26.805160: Processed 6000 documents...\n",
      "2019-11-01 23:16:40.996419: Processed 7000 documents...\n",
      "2019-11-01 23:16:56.845262: Processed 8000 documents...\n",
      "2019-11-01 23:17:10.408503: Processed 9000 documents...\n",
      "2019-11-01 23:17:24.174271: Processed 10000 documents...\n",
      "2019-11-01 23:17:38.544196: Processed 11000 documents...\n",
      "2019-11-01 23:17:52.147334: Processed 12000 documents...\n",
      "2019-11-01 23:18:05.758124: Processed 13000 documents...\n",
      "2019-11-01 23:19:07.530390: Start generating trigrams...\n",
      "2019-11-01 23:19:28.851502: Processed 1000 documents...\n",
      "2019-11-01 23:19:52.363437: Processed 2000 documents...\n",
      "2019-11-01 23:20:15.163788: Processed 3000 documents...\n",
      "2019-11-01 23:20:36.976158: Processed 4000 documents...\n",
      "2019-11-01 23:20:58.694049: Processed 5000 documents...\n",
      "2019-11-01 23:21:22.511285: Processed 6000 documents...\n",
      "2019-11-01 23:21:45.902872: Processed 7000 documents...\n",
      "2019-11-01 23:22:08.817082: Processed 8000 documents...\n",
      "2019-11-01 23:22:31.148127: Processed 9000 documents...\n",
      "2019-11-01 23:22:53.755920: Processed 10000 documents...\n",
      "2019-11-01 23:23:17.344447: Processed 11000 documents...\n",
      "2019-11-01 23:23:42.507422: Processed 12000 documents...\n",
      "2019-11-01 23:24:06.295582: Processed 13000 documents...\n",
      "2019-11-01 23:28:54.559744: Processed 1000 documents...\n",
      "2019-11-01 23:29:21.019885: Processed 2000 documents...\n",
      "2019-11-01 23:29:46.469968: Processed 3000 documents...\n",
      "2019-11-01 23:30:11.092603: Processed 4000 documents...\n",
      "2019-11-01 23:30:35.430495: Processed 5000 documents...\n",
      "2019-11-01 23:31:02.125972: Processed 6000 documents...\n",
      "2019-11-01 23:31:27.751893: Processed 7000 documents...\n",
      "2019-11-01 23:31:53.495494: Processed 8000 documents...\n",
      "2019-11-01 23:32:18.701978: Processed 9000 documents...\n",
      "2019-11-01 23:32:43.891322: Processed 10000 documents...\n",
      "2019-11-01 23:33:10.475282: Processed 11000 documents...\n",
      "2019-11-01 23:33:35.341631: Processed 12000 documents...\n",
      "2019-11-01 23:34:00.336591: Processed 13000 documents...\n",
      "2019-11-01 23:34:06.099213: Concatenating 1 partitions...\n",
      "2019-11-01 23:34:06.181376: Saving all 557373 ngrams...\n",
      "2019-11-01 23:34:07.247124: Truncating ngrams to minimum of 131.68 occurrences...\n",
      "2019-11-01 23:34:07.250707: Saving all 18350 truncated ngrams...\n",
      "2019-11-01 23:34:07.284360: Generating ngrams for WB...\n",
      "freq_cutoff: 500\n",
      "2019-11-01 23:34:07.995370: Starting with sample 1...\n",
      "2019-11-01 23:34:07.995996: Start generating bigrams...\n",
      "2019-11-01 23:34:18.505981: Processed 1000 documents...\n",
      "2019-11-01 23:34:31.200709: Processed 2000 documents...\n",
      "2019-11-01 23:34:42.670920: Processed 3000 documents...\n",
      "2019-11-01 23:34:54.532165: Processed 4000 documents...\n",
      "2019-11-01 23:35:06.533211: Processed 5000 documents...\n",
      "2019-11-01 23:35:18.459101: Processed 6000 documents...\n",
      "2019-11-01 23:35:30.169820: Processed 7000 documents...\n",
      "2019-11-01 23:35:41.516536: Processed 8000 documents...\n",
      "2019-11-01 23:35:52.060025: Processed 9000 documents...\n",
      "2019-11-01 23:36:03.967364: Processed 10000 documents...\n",
      "2019-11-01 23:36:15.757374: Processed 11000 documents...\n",
      "2019-11-01 23:36:27.928158: Processed 12000 documents...\n",
      "2019-11-01 23:36:39.842481: Processed 13000 documents...\n",
      "2019-11-01 23:36:51.049905: Processed 14000 documents...\n",
      "2019-11-01 23:37:03.502273: Processed 15000 documents...\n",
      "2019-11-01 23:37:15.922567: Processed 16000 documents...\n",
      "2019-11-01 23:37:27.803516: Processed 17000 documents...\n",
      "2019-11-01 23:37:39.941036: Processed 18000 documents...\n",
      "2019-11-01 23:37:51.593391: Processed 19000 documents...\n",
      "2019-11-01 23:38:04.536167: Processed 20000 documents...\n",
      "2019-11-01 23:38:16.444471: Processed 21000 documents...\n",
      "2019-11-01 23:39:50.688566: Start generating trigrams...\n",
      "2019-11-01 23:40:04.593567: Processed 1000 documents...\n",
      "2019-11-01 23:40:22.444229: Processed 2000 documents...\n",
      "2019-11-01 23:40:36.602144: Processed 3000 documents...\n",
      "2019-11-01 23:40:50.977148: Processed 4000 documents...\n",
      "2019-11-01 23:41:06.356158: Processed 5000 documents...\n",
      "2019-11-01 23:41:21.365953: Processed 6000 documents...\n",
      "2019-11-01 23:41:36.305970: Processed 7000 documents...\n",
      "2019-11-01 23:41:50.447729: Processed 8000 documents...\n",
      "2019-11-01 23:42:04.812955: Processed 9000 documents...\n",
      "2019-11-01 23:42:21.478839: Processed 10000 documents...\n",
      "2019-11-01 23:42:36.131569: Processed 11000 documents...\n",
      "2019-11-01 23:42:50.218000: Processed 12000 documents...\n",
      "2019-11-01 23:43:05.001122: Processed 13000 documents...\n",
      "2019-11-01 23:43:19.052213: Processed 14000 documents...\n",
      "2019-11-01 23:43:35.185392: Processed 15000 documents...\n",
      "2019-11-01 23:43:52.366205: Processed 16000 documents...\n",
      "2019-11-01 23:44:08.280283: Processed 17000 documents...\n",
      "2019-11-01 23:44:24.134745: Processed 18000 documents...\n",
      "2019-11-01 23:44:39.596834: Processed 19000 documents...\n",
      "2019-11-01 23:44:55.653271: Processed 20000 documents...\n",
      "2019-11-01 23:45:11.912246: Processed 21000 documents...\n"
     ]
    }
   ],
   "source": [
    "THRESHOLD = 0.2\n",
    "SCORING = 'npmi'\n",
    "\n",
    "PHRASER_ID = f'{SCORING}{THRESHOLD}'\n",
    "try:\n",
    "    CORPUS_ID_SAMPLES = {\n",
    "        'IMF': 20000,\n",
    "        'WB': 20000,\n",
    "    }\n",
    "\n",
    "    all_truncated_ngrams = []\n",
    "\n",
    "    for CORPUS_ID, NUM_SAMPLES in CORPUS_ID_SAMPLES.items():\n",
    "        notify('run_log.txt', f'Generating ngrams for {CORPUS_ID}...')\n",
    "        all_cleaned_files = glob.glob(os.path.join(get_txt_clean_path(CORPUS_ID), '*.txt'))\n",
    "        freq_cutoff = min(500, 0.01 * len(all_cleaned_files))  # Cutoff set to minimize noise from ngrams of rare occurrence.\n",
    "        print(f'freq_cutoff: {freq_cutoff}')\n",
    "\n",
    "        np.random.shuffle(all_cleaned_files)\n",
    "        cleaned_files_partition = np.array_split(all_cleaned_files, max(len(all_cleaned_files) // NUM_SAMPLES, 1)) # if len(all_cleaned_files) is less than twice NUM_SAMPLES, it will return a single list containing all the files.\n",
    "\n",
    "        phrasers_map = {}\n",
    "        all_ngram_freqs = []\n",
    "\n",
    "        for ix in range(len(cleaned_files_partition)):\n",
    "            notify('run_log.txt', f\"Starting with sample {ix + 1}...\")\n",
    "\n",
    "            # np.random.seed(ix + 1)\n",
    "            # cleaned_files = np.random.choice(all_cleaned_files, NUM_SAMPLES, replace=False)\n",
    "\n",
    "            cleaned_files = cleaned_files_partition[ix]\n",
    "            _phrasers = train_phrasers(cleaned_files, threshold=THRESHOLD, scoring=SCORING)\n",
    "            phrasers_map[ix] = _phrasers\n",
    "\n",
    "            ngram_freq = collect_ngrams(\n",
    "                cleaned_files, _phrasers,\n",
    "                filename=os.path.join(get_corpus_path(CORPUS_ID), f'{CORPUS_ID.lower()}_ngrams-random_{ix}.csv')\n",
    "            )\n",
    "\n",
    "            all_ngram_freqs.append(ngram_freq.set_index('ngram'))\n",
    "\n",
    "        notify('run_log.txt', f'Concatenating {len(all_ngram_freqs)} partitions...')\n",
    "        all_ngram_freqs_df = pd.concat(all_ngram_freqs, axis=1).sum(axis=1)\n",
    "\n",
    "        all_ngram_freqs_df.index.name = 'ngram'\n",
    "        all_ngram_freqs_df.name = 'occ'\n",
    "        all_ngram_freqs_df = all_ngram_freqs_df.reset_index()\n",
    "\n",
    "        all_ngram_freqs_df = all_ngram_freqs_df.sort_values('occ', ascending=False).reset_index(drop='index')\n",
    "        \n",
    "        notify('run_log.txt', f'Saving all {all_ngram_freqs_df.shape[0]} ngrams...')\n",
    "        all_ngram_freqs_df.to_csv(os.path.join(get_corpus_path(CORPUS_ID), f'{CORPUS_ID.lower()}_ngrams-random_all-{PHRASER_ID}-{NUM_SAMPLES}.csv'), index=False)\n",
    "\n",
    "        notify('run_log.txt', f'Truncating ngrams to minimum of {freq_cutoff} occurrences...')\n",
    "        truncated_ngram_freqs_df = all_ngram_freqs_df[all_ngram_freqs_df['occ'] >= freq_cutoff].copy()\n",
    "        \n",
    "        notify('run_log.txt', f'Saving all {truncated_ngram_freqs_df.shape[0]} truncated ngrams...')\n",
    "        truncated_ngram_freqs_df.to_csv(os.path.join(get_corpus_path(CORPUS_ID), f'{CORPUS_ID.lower()}_ngrams-random_truncated-{PHRASER_ID}-{NUM_SAMPLES}.csv'), index=False)\n",
    "\n",
    "        all_truncated_ngrams.append(truncated_ngram_freqs_df)\n",
    "\n",
    "    notify('run_log.txt', f'Combining all {len(all_truncated_ngrams)} corpus ngrams...')\n",
    "    all_truncated_ngrams_df = pd.concat(all_truncated_ngrams, axis=0)\n",
    "    all_truncated_ngrams_df = all_truncated_ngrams_df.groupby('ngram')['occ'].sum().reset_index().sort_values('occ', ascending=False)\n",
    "    \n",
    "    notify('run_log.txt', f'Saving combined truncated ngrams...')\n",
    "    all_truncated_ngrams_df.to_csv('../whitelists/truncated_ngrams.csv', index=False)\n",
    "    notify('run_success.txt', 'completed successfully!')\n",
    "    \n",
    "except Exception as e:\n",
    "    notify('run_failed.txt', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_truncated_ngrams_df[all_truncated_ngrams_df.ngram == 'consumer_price_index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_truncated_ngrams_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     CORPUS_ID_SAMPLES = {\n",
    "#         'WB': 10000,\n",
    "#         'IMF': 10000,\n",
    "#     }\n",
    "\n",
    "#     all_truncated_ngrams = []\n",
    "\n",
    "#     for CORPUS_ID, NUM_SAMPLES in CORPUS_ID_SAMPLES.items():\n",
    "#         notify('run_log.txt', f'Generating ngrams for {CORPUS_ID}...')\n",
    "#         all_cleaned_files = glob.glob(os.path.join(get_txt_clean_path(CORPUS_ID), '*.txt'))\n",
    "#         freq_cutoff = min(500, 0.01 * len(all_cleaned_files))  # Cutoff set to minimize noise from ngrams of rare occurrence.\n",
    "\n",
    "#         np.random.shuffle(all_cleaned_files)\n",
    "#         cleaned_files_partition = np.array_split(all_cleaned_files, len(all_cleaned_files) // NUM_SAMPLES) # if len(all_cleaned_files) is less than twice NUM_SAMPLES, it will return a single list containing all the files.\n",
    "\n",
    "#         phrasers_map = {}\n",
    "#         all_ngram_freqs = []\n",
    "\n",
    "#         for ix in range(len(cleaned_files_partition)):\n",
    "#             notify('run_log.txt', f\"Starting with sample {ix + 1}...\")\n",
    "\n",
    "#             # np.random.seed(ix + 1)\n",
    "#             # cleaned_files = np.random.choice(all_cleaned_files, NUM_SAMPLES, replace=False)\n",
    "\n",
    "#             cleaned_files = cleaned_files_partition[ix]\n",
    "#             _phrasers = train_phrasers(cleaned_files)\n",
    "#             phrasers_map[ix] = _phrasers\n",
    "\n",
    "#             ngram_freq = collect_ngrams(\n",
    "#                 cleaned_files, _phrasers,\n",
    "#                 filename=os.path.join(get_corpus_path(CORPUS_ID), f'{CORPUS_ID.lower()}_ngrams-random_{ix}.csv')\n",
    "#             )\n",
    "\n",
    "#             all_ngram_freqs.append(ngram_freq.set_index('ngram'))\n",
    "\n",
    "#         notify('run_log.txt', f'Concatenating {len(all_ngram_freqs)} partitions...')\n",
    "#         all_ngram_freqs_df = pd.concat(all_ngram_freqs, axis=1).sum(axis=1)\n",
    "\n",
    "#         all_ngram_freqs_df.index.name = 'ngram'\n",
    "#         all_ngram_freqs_df.name = 'occ'\n",
    "#         all_ngram_freqs_df = all_ngram_freqs_df.reset_index()\n",
    "\n",
    "#         all_ngram_freqs_df = all_ngram_freqs_df.sort_values('occ', ascending=False).reset_index(drop='index')\n",
    "        \n",
    "#         notify('run_log.txt', f'Saving all {all_ngram_freqs_df.shape[0]} ngrams...')\n",
    "#         all_ngram_freqs_df.to_csv(os.path.join(get_corpus_path(CORPUS_ID), f'{CORPUS_ID.lower()}_ngrams-random_all.csv'), index=False)\n",
    "\n",
    "#         notify('run_log.txt', f'Truncating ngrams to minimum of {freq_cutoff} occurrences...')\n",
    "#         truncated_ngram_freqs_df = all_ngram_freqs_df[all_ngram_freqs_df['occ'] >= freq_cutoff].copy()\n",
    "        \n",
    "#         notify('run_log.txt', f'Saving all {truncated_ngram_freqs_df.shape[0]} truncated ngrams...')\n",
    "#         truncated_ngram_freqs_df.to_csv(os.path.join(get_corpus_path(CORPUS_ID), f'{CORPUS_ID.lower()}_ngrams-random_truncated.csv'), index=False)\n",
    "\n",
    "#         all_truncated_ngrams.append(truncated_ngram_freqs_df)\n",
    "\n",
    "#     notify('run_log.txt', f'Combining all {len(all_truncated_ngrams)} corpus ngrams...')\n",
    "#     all_truncated_ngrams_df = pd.concat(all_truncated_ngrams, axis=0)\n",
    "#     all_truncated_ngrams_df = all_truncated_ngrams_df.groupby('ngram')['occ'].sum().reset_index().sort_values('occ', ascending=False)\n",
    "    \n",
    "#     notify('run_log.txt', f'Saving combined truncated ngrams...')\n",
    "#     all_truncated_ngrams_df.to_csv('../whitelists/truncated_ngrams.csv', index=False)\n",
    "#     notify('run_success.txt', 'completed successfully!')\n",
    "    \n",
    "# except Exception as e:\n",
    "#     notify('run_failed.txt', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process `ngrams` to create a cleaned version of each "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Cleaner\n",
    "%run ../DataCleanerModule.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notify('run_log.txt', f'Setting up cleaner...')\n",
    "\n",
    "cleaner = Cleaner(\n",
    "    use_spellchecker=True, use_respeller=True, use_lemmatizer=True, use_spacy=True,\n",
    "    replacements_plurals_to_singular_file='../whitelists/whitelist_replacements_plurals_to_singular.csv',\n",
    "    acronyms_file='../whitelists/whitelist_acronyms.csv',\n",
    "    num_workers=22,\n",
    "    ignore_length=0,\n",
    "#     supported_lang=('en', 'it', 'ro', 'fr'),\n",
    "    check_language=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notify('run_log.txt', f'Loading truncated_ngrams.csv ...')\n",
    "# ngrams_df = pd.read_csv('../whitelists/truncated_ngrams.csv', header=None, index_col=False)\n",
    "ngrams_df = pd.read_csv('../whitelists/truncated_ngrams.csv', index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cleaner.clean_text('world_bank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_ngram(ngram):\n",
    "    return {ngram: cleaner.clean_text(ngram)['text']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKERS = max(1, os.cpu_count() - 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notify('run_log.txt', f'Running cleaner on ngrams...')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    p = Pool(WORKERS)\n",
    "    res = p.map(clean_ngram, ngrams_df['ngram'])\n",
    "    ngram_cleaned_map = {k:v for i in res for k, v in i.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notify('run_log.txt', f'Processing dataframe...')\n",
    "ngrams_df['cleaned'] = ngrams_df['ngram'].map(ngram_cleaned_map)\n",
    "ngrams_df = ngrams_df.drop('occ', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notify('run_log.txt', f'Saving cleaned ngrams to whitelist...')\n",
    "ngrams_df.to_csv('../whitelists/whitelist_ngrams_truncated_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_freq[ngram_freq.ngram=='']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_freq[ngram_freq.ngram=='world_bank']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "ngram_freq.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ngram_freq.to_csv(os.path.join(get_corpus_path(CORPUS_ID), f'{CORPUS_ID.lower()}_ngrams_sample_wb_44.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "ngram_freq['num_gram'] = ngram_freq.ngram.str.split('_').map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ngram_freq[(ngram_freq.num_gram <= 5) & (ngram_freq.occ > 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ngram</th>\n",
       "      <th>occ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>7062</td>\n",
       "      <td>climate_change</td>\n",
       "      <td>21087.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ngram      occ\n",
       "7062  climate_change  21087.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_truncated_ngrams_df[all_truncated_ngrams_df.ngram == 'climate_change']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ngram</th>\n",
       "      <th>occ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>674</td>\n",
       "      <td>climate_change</td>\n",
       "      <td>25961.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ngram      occ\n",
       "674  climate_change  25961.0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_truncated_ngrams_df[all_truncated_ngrams_df.ngram == 'climate_change']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ngram</th>\n",
       "      <th>occ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2974</td>\n",
       "      <td>policy_maker</td>\n",
       "      <td>22090.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2273</td>\n",
       "      <td>living_standard</td>\n",
       "      <td>21921.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3093</td>\n",
       "      <td>primary_secondary</td>\n",
       "      <td>21897.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>431</td>\n",
       "      <td>board_director</td>\n",
       "      <td>21732.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4413</td>\n",
       "      <td>vulnerable_group</td>\n",
       "      <td>21686.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3811</td>\n",
       "      <td>solid_waste</td>\n",
       "      <td>21309.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3294</td>\n",
       "      <td>real_estate</td>\n",
       "      <td>21260.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4174</td>\n",
       "      <td>timely_manner</td>\n",
       "      <td>20946.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1738</td>\n",
       "      <td>general_condition</td>\n",
       "      <td>20828.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>878</td>\n",
       "      <td>contract_award</td>\n",
       "      <td>20816.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1148</td>\n",
       "      <td>direct_indirect</td>\n",
       "      <td>20320.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>299</td>\n",
       "      <td>authorize_representative</td>\n",
       "      <td>20048.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3923</td>\n",
       "      <td>steering_committee</td>\n",
       "      <td>19942.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2995</td>\n",
       "      <td>positive_negative</td>\n",
       "      <td>19701.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2329</td>\n",
       "      <td>male_female</td>\n",
       "      <td>19518.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3286</td>\n",
       "      <td>raw_material</td>\n",
       "      <td>19471.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2366</td>\n",
       "      <td>master_plan</td>\n",
       "      <td>19321.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>654</td>\n",
       "      <td>civil_servant</td>\n",
       "      <td>19105.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2455</td>\n",
       "      <td>middle_income</td>\n",
       "      <td>19041.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3599</td>\n",
       "      <td>secondary_school</td>\n",
       "      <td>18715.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2760</td>\n",
       "      <td>organizational_structure</td>\n",
       "      <td>18685.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>841</td>\n",
       "      <td>consult_service</td>\n",
       "      <td>18582.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2632</td>\n",
       "      <td>non_governmental_organization</td>\n",
       "      <td>18576.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2112</td>\n",
       "      <td>job_creation</td>\n",
       "      <td>18490.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1643</td>\n",
       "      <td>food_security</td>\n",
       "      <td>18420.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3602</td>\n",
       "      <td>section_general_condition</td>\n",
       "      <td>18041.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>429</td>\n",
       "      <td>board_approval</td>\n",
       "      <td>18018.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2452</td>\n",
       "      <td>middle_east_north</td>\n",
       "      <td>17856.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3098</td>\n",
       "      <td>prime_minister</td>\n",
       "      <td>17822.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3842</td>\n",
       "      <td>special_account</td>\n",
       "      <td>17715.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>755</td>\n",
       "      <td>comparative_advantage</td>\n",
       "      <td>17606.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3972</td>\n",
       "      <td>subject_prior_review</td>\n",
       "      <td>17586.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3435</td>\n",
       "      <td>resettlement_action_plan</td>\n",
       "      <td>17531.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3773</td>\n",
       "      <td>small_medium</td>\n",
       "      <td>17453.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3021</td>\n",
       "      <td>poverty_alleviation</td>\n",
       "      <td>17368.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2586</td>\n",
       "      <td>natural_disaster</td>\n",
       "      <td>17364.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2272</td>\n",
       "      <td>living_condition</td>\n",
       "      <td>17294.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3292</td>\n",
       "      <td>readily_available</td>\n",
       "      <td>17269.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2589</td>\n",
       "      <td>natural_habitat</td>\n",
       "      <td>17255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3348</td>\n",
       "      <td>regional_vice_president</td>\n",
       "      <td>17167.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2925</td>\n",
       "      <td>physical_cultural_resource</td>\n",
       "      <td>17109.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>adverse_effect</td>\n",
       "      <td>17033.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2954</td>\n",
       "      <td>plan_actual_plan_actual</td>\n",
       "      <td>16983.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>669</td>\n",
       "      <td>clearly_define</td>\n",
       "      <td>16940.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2335</td>\n",
       "      <td>man_woman</td>\n",
       "      <td>16765.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4067</td>\n",
       "      <td>team_leader</td>\n",
       "      <td>16713.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1327</td>\n",
       "      <td>eligibility_criterion</td>\n",
       "      <td>16500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>accordance_provision_paragraph</td>\n",
       "      <td>16467.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2176</td>\n",
       "      <td>later_month_end</td>\n",
       "      <td>16439.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3726</td>\n",
       "      <td>short_run</td>\n",
       "      <td>16304.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               ngram      occ\n",
       "2974                    policy_maker  22090.0\n",
       "2273                 living_standard  21921.0\n",
       "3093               primary_secondary  21897.0\n",
       "431                   board_director  21732.0\n",
       "4413                vulnerable_group  21686.0\n",
       "3811                     solid_waste  21309.0\n",
       "3294                     real_estate  21260.0\n",
       "4174                   timely_manner  20946.0\n",
       "1738               general_condition  20828.0\n",
       "878                   contract_award  20816.0\n",
       "1148                 direct_indirect  20320.0\n",
       "299         authorize_representative  20048.0\n",
       "3923              steering_committee  19942.0\n",
       "2995               positive_negative  19701.0\n",
       "2329                     male_female  19518.0\n",
       "3286                    raw_material  19471.0\n",
       "2366                     master_plan  19321.0\n",
       "654                    civil_servant  19105.0\n",
       "2455                   middle_income  19041.0\n",
       "3599                secondary_school  18715.0\n",
       "2760        organizational_structure  18685.0\n",
       "841                  consult_service  18582.0\n",
       "2632   non_governmental_organization  18576.0\n",
       "2112                    job_creation  18490.0\n",
       "1643                   food_security  18420.0\n",
       "3602       section_general_condition  18041.0\n",
       "429                   board_approval  18018.0\n",
       "2452               middle_east_north  17856.0\n",
       "3098                  prime_minister  17822.0\n",
       "3842                 special_account  17715.0\n",
       "755            comparative_advantage  17606.0\n",
       "3972            subject_prior_review  17586.0\n",
       "3435        resettlement_action_plan  17531.0\n",
       "3773                    small_medium  17453.0\n",
       "3021             poverty_alleviation  17368.0\n",
       "2586                natural_disaster  17364.0\n",
       "2272                living_condition  17294.0\n",
       "3292               readily_available  17269.0\n",
       "2589                 natural_habitat  17255.0\n",
       "3348         regional_vice_president  17167.0\n",
       "2925      physical_cultural_resource  17109.0\n",
       "89                    adverse_effect  17033.0\n",
       "2954         plan_actual_plan_actual  16983.0\n",
       "669                   clearly_define  16940.0\n",
       "2335                       man_woman  16765.0\n",
       "4067                     team_leader  16713.0\n",
       "1327           eligibility_criterion  16500.0\n",
       "29    accordance_provision_paragraph  16467.0\n",
       "2176                 later_month_end  16439.0\n",
       "3726                       short_run  16304.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_truncated_ngrams_df.iloc[100:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ngram</th>\n",
       "      <th>occ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2974</td>\n",
       "      <td>policy_maker</td>\n",
       "      <td>22090.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2273</td>\n",
       "      <td>living_standard</td>\n",
       "      <td>21921.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3093</td>\n",
       "      <td>primary_secondary</td>\n",
       "      <td>21897.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>431</td>\n",
       "      <td>board_director</td>\n",
       "      <td>21732.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4413</td>\n",
       "      <td>vulnerable_group</td>\n",
       "      <td>21686.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3811</td>\n",
       "      <td>solid_waste</td>\n",
       "      <td>21309.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3294</td>\n",
       "      <td>real_estate</td>\n",
       "      <td>21260.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4174</td>\n",
       "      <td>timely_manner</td>\n",
       "      <td>20946.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1738</td>\n",
       "      <td>general_condition</td>\n",
       "      <td>20828.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>878</td>\n",
       "      <td>contract_award</td>\n",
       "      <td>20816.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1148</td>\n",
       "      <td>direct_indirect</td>\n",
       "      <td>20320.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>299</td>\n",
       "      <td>authorize_representative</td>\n",
       "      <td>20048.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3923</td>\n",
       "      <td>steering_committee</td>\n",
       "      <td>19942.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2995</td>\n",
       "      <td>positive_negative</td>\n",
       "      <td>19701.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2329</td>\n",
       "      <td>male_female</td>\n",
       "      <td>19518.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3286</td>\n",
       "      <td>raw_material</td>\n",
       "      <td>19471.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2366</td>\n",
       "      <td>master_plan</td>\n",
       "      <td>19321.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>654</td>\n",
       "      <td>civil_servant</td>\n",
       "      <td>19105.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2455</td>\n",
       "      <td>middle_income</td>\n",
       "      <td>19041.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3599</td>\n",
       "      <td>secondary_school</td>\n",
       "      <td>18715.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2760</td>\n",
       "      <td>organizational_structure</td>\n",
       "      <td>18685.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>841</td>\n",
       "      <td>consult_service</td>\n",
       "      <td>18582.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2632</td>\n",
       "      <td>non_governmental_organization</td>\n",
       "      <td>18576.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2112</td>\n",
       "      <td>job_creation</td>\n",
       "      <td>18490.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1643</td>\n",
       "      <td>food_security</td>\n",
       "      <td>18420.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3602</td>\n",
       "      <td>section_general_condition</td>\n",
       "      <td>18041.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>429</td>\n",
       "      <td>board_approval</td>\n",
       "      <td>18018.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2452</td>\n",
       "      <td>middle_east_north</td>\n",
       "      <td>17856.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3098</td>\n",
       "      <td>prime_minister</td>\n",
       "      <td>17822.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3842</td>\n",
       "      <td>special_account</td>\n",
       "      <td>17715.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>755</td>\n",
       "      <td>comparative_advantage</td>\n",
       "      <td>17606.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3972</td>\n",
       "      <td>subject_prior_review</td>\n",
       "      <td>17586.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3435</td>\n",
       "      <td>resettlement_action_plan</td>\n",
       "      <td>17531.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3773</td>\n",
       "      <td>small_medium</td>\n",
       "      <td>17453.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3021</td>\n",
       "      <td>poverty_alleviation</td>\n",
       "      <td>17368.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2586</td>\n",
       "      <td>natural_disaster</td>\n",
       "      <td>17364.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2272</td>\n",
       "      <td>living_condition</td>\n",
       "      <td>17294.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3292</td>\n",
       "      <td>readily_available</td>\n",
       "      <td>17269.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2589</td>\n",
       "      <td>natural_habitat</td>\n",
       "      <td>17255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3348</td>\n",
       "      <td>regional_vice_president</td>\n",
       "      <td>17167.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2925</td>\n",
       "      <td>physical_cultural_resource</td>\n",
       "      <td>17109.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>adverse_effect</td>\n",
       "      <td>17033.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2954</td>\n",
       "      <td>plan_actual_plan_actual</td>\n",
       "      <td>16983.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>669</td>\n",
       "      <td>clearly_define</td>\n",
       "      <td>16940.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2335</td>\n",
       "      <td>man_woman</td>\n",
       "      <td>16765.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4067</td>\n",
       "      <td>team_leader</td>\n",
       "      <td>16713.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1327</td>\n",
       "      <td>eligibility_criterion</td>\n",
       "      <td>16500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>accordance_provision_paragraph</td>\n",
       "      <td>16467.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2176</td>\n",
       "      <td>later_month_end</td>\n",
       "      <td>16439.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3726</td>\n",
       "      <td>short_run</td>\n",
       "      <td>16304.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               ngram      occ\n",
       "2974                    policy_maker  22090.0\n",
       "2273                 living_standard  21921.0\n",
       "3093               primary_secondary  21897.0\n",
       "431                   board_director  21732.0\n",
       "4413                vulnerable_group  21686.0\n",
       "3811                     solid_waste  21309.0\n",
       "3294                     real_estate  21260.0\n",
       "4174                   timely_manner  20946.0\n",
       "1738               general_condition  20828.0\n",
       "878                   contract_award  20816.0\n",
       "1148                 direct_indirect  20320.0\n",
       "299         authorize_representative  20048.0\n",
       "3923              steering_committee  19942.0\n",
       "2995               positive_negative  19701.0\n",
       "2329                     male_female  19518.0\n",
       "3286                    raw_material  19471.0\n",
       "2366                     master_plan  19321.0\n",
       "654                    civil_servant  19105.0\n",
       "2455                   middle_income  19041.0\n",
       "3599                secondary_school  18715.0\n",
       "2760        organizational_structure  18685.0\n",
       "841                  consult_service  18582.0\n",
       "2632   non_governmental_organization  18576.0\n",
       "2112                    job_creation  18490.0\n",
       "1643                   food_security  18420.0\n",
       "3602       section_general_condition  18041.0\n",
       "429                   board_approval  18018.0\n",
       "2452               middle_east_north  17856.0\n",
       "3098                  prime_minister  17822.0\n",
       "3842                 special_account  17715.0\n",
       "755            comparative_advantage  17606.0\n",
       "3972            subject_prior_review  17586.0\n",
       "3435        resettlement_action_plan  17531.0\n",
       "3773                    small_medium  17453.0\n",
       "3021             poverty_alleviation  17368.0\n",
       "2586                natural_disaster  17364.0\n",
       "2272                living_condition  17294.0\n",
       "3292               readily_available  17269.0\n",
       "2589                 natural_habitat  17255.0\n",
       "3348         regional_vice_president  17167.0\n",
       "2925      physical_cultural_resource  17109.0\n",
       "89                    adverse_effect  17033.0\n",
       "2954         plan_actual_plan_actual  16983.0\n",
       "669                   clearly_define  16940.0\n",
       "2335                       man_woman  16765.0\n",
       "4067                     team_leader  16713.0\n",
       "1327           eligibility_criterion  16500.0\n",
       "29    accordance_provision_paragraph  16467.0\n",
       "2176                 later_month_end  16439.0\n",
       "3726                       short_run  16304.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_truncated_ngrams_df.iloc[100:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}