{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"text-align:center\">World Bank Documents and Reports Cleaner</div>\n",
    "\n",
    "This notebook implements the cleaner classes for the data from the **Documents and Reports API**. This cleaner module provides respelling functionality as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Requirements:\n",
    "# # Please install spacy library and the `en` model\n",
    "# !~/anaconda3/bin/pip install spacy\n",
    "# !~/anaconda3/bin/python -m spacy download en\n",
    "# !~/anaconda3/bin/pip install contexttimer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing pattern as alternative to pyenchant\n",
    "#### Note, the pattern module's spell checking function is quite slow!\n",
    "\n",
    "\n",
    "Clone first the development repo ([pypi version is outdated](https://github.com/clips/pattern/issues/217\n",
    "))\n",
    "- `git clone -b development https://github.com/clips/pattern`\n",
    "- `cd pattern/`\n",
    "- Commenting out `\"mysqlclient\"` inside the `setup.py` file may be necessary if errors are encountered in the next step.\n",
    "- `pip install .`\n",
    "\n",
    "Make sure that the `pip` that you use corresponds to the python installation that you will use to run the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from /home/avsolatorio/WBG/wb-nlp/SCRIPTS/acronyms/AcronymModule.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from acronyms.AcronymModule import AcronymMapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "nltk.data.path.append(\"/R/nltk_data\")\n",
    "\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from nltk.corpus import PlaintextCorpusReader, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "from langdetect import detect, detect_langs\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing as mp\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "# Spelling correction\n",
    "ENCHANT_INSTALLED = True\n",
    "try:\n",
    "    from enchant.checker import SpellChecker\n",
    "    from enchant import Dict\n",
    "except:\n",
    "    # Make sure that these are installed for the pattern module to work\n",
    "    for token in (\"stopwords\", \"wordnet\", \"wordnet_ic\", \"sentiwordnet\"):\n",
    "        nltk.download(token)\n",
    "    print('Using pattern module...')\n",
    "    import pattern.en\n",
    "    ENCHANT_INSTALLED = False\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.metrics.distance import edit_distance\n",
    "\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "import spacy\n",
    "\n",
    "from contexttimer import Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from sklearn.feature_extraction import stop_words\n",
    "\n",
    "roman_nums = set([\n",
    "    'i', 'ii', 'iii', 'iv', 'v', 'vi', 'vii', 'viii', 'ix', 'x', 'xi',\n",
    "    'xii', 'xiii', 'xiv', 'xv', 'xvi', 'xvii', 'xviii', 'xix', 'xx',\n",
    "    'xxi', 'xxii', 'xxiii', 'xxiv', 'xxv', 'xxvi', 'xxvii', 'xxviii',\n",
    "    'xxix', 'xxx', 'xxxi', 'xxxii', 'xxxiii', 'xxxiv', 'xxxv', 'xxxvi',\n",
    "    'xxxvii', 'xxxviii', 'xxxix', 'xl', 'xli', 'xlii', 'xliii', 'xliv',\n",
    "    'xlv', 'xlvi', 'xlvii', 'xlviii', 'xlix', 'l', 'li', 'lii', 'liii',\n",
    "    'liv', 'lv', 'lvi', 'lvii', 'lviii', 'lix', 'lx', 'lxi', 'lxii',\n",
    "    'lxiii', 'lxiv', 'lxv', 'lxvi', 'lxvii', 'lxviii', 'lxix', 'lxx',\n",
    "    'lxxi', 'lxxii', 'lxxiii', 'lxxiv', 'lxxv', 'lxxvi', 'lxxvii',\n",
    "    'lxxviii', 'lxxix', 'lxstopwordsxx', 'lxxxi', 'lxxxii', 'lxxxiii', 'lxxxiv',\n",
    "    'lxxxv', 'lxxxvi', 'lxxxvii', 'lxxxviii', 'lxxxix', 'xc', 'xci',\n",
    "    'xcii', 'xciii', 'xciv', 'xcv', 'xcvi', 'xcvii', 'xcviii', 'xcix', 'c'\n",
    "])\n",
    "try:\n",
    "    stopwords = set(nltk_stopwords.words('english'))\n",
    "except:\n",
    "    stopwords = set()\n",
    "    print('Warning: NLTK stopwords not used! Please check if the nltk stopwords corpus is avaialable in your system.')\n",
    "stopwords.update(stop_words.ENGLISH_STOP_WORDS)\n",
    "stopwords.update(roman_nums)\n",
    "\n",
    "stopwords = list(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spelling correction module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement redis cacher\n",
    "import joblib\n",
    "import os\n",
    "import json\n",
    "import redis\n",
    "\n",
    "# docker run --name=respeller-redis --publish=6379:6379 --hostname=redis --restart=on-failure --detach redis:latest\n",
    "redis_cache = redis.Redis(host='localhost', port=6379, db=0)\n",
    "\n",
    "\n",
    "def get_func_fullname(func):\n",
    "    # derived from joblib: https://github.com/joblib/joblib/blob/master/joblib/memory.py\n",
    "    \"\"\"Compute the part of part associated with a function.\"\"\"\n",
    "    modules, funcname = joblib.func_inspect.get_func_name(func)\n",
    "    modules.append(funcname)\n",
    "    \n",
    "    return os.path.join(*modules)\n",
    "\n",
    "\n",
    "def get_argument_hash(func, args, kwargs, ignore_list=None):\n",
    "    if ignore_list is None:\n",
    "        ignore_list = []\n",
    "        \n",
    "    argument_hash = joblib.hashing.hash(\n",
    "        joblib.func_inspect.filter_args(\n",
    "            func, ignore_list, args, kwargs),\n",
    "        coerce_mmap=False # (self.mmap_mode is not None) # mmap_mode is None by default\n",
    "    )\n",
    "    \n",
    "    return argument_hash\n",
    "\n",
    "\n",
    "def redis_cacher(func):\n",
    "    # TODO: add a namespace\n",
    "    '''\n",
    "    Must be used only to cache string in the meantime.\n",
    "    '''\n",
    "    def wrapper(*args, **kwargs):\n",
    "\n",
    "        argument_hash = get_argument_hash(func, args, kwargs)\n",
    "        func_id = get_func_fullname(func)\n",
    "        \n",
    "        fromcache = redis_cache.hget(func_id, argument_hash)\n",
    "        \n",
    "        if fromcache is None:\n",
    "            value = func(*args, **kwargs)\n",
    "            tocache = json.dumps(value)\n",
    "            \n",
    "            # print(func_id, argument_hash)\n",
    "            redis_cache.hset(func_id, argument_hash, tocache)\n",
    "        else:\n",
    "            # Decode since redis returns a byte encoded string\n",
    "            fromcache = fromcache.decode('utf-8')\n",
    "            value = json.loads(fromcache)\n",
    "\n",
    "        return value\n",
    "    \n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @redis_cacher\n",
    "# def multiplier(x, y, z=10):\n",
    "#     return x * y + z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup caching mechanism for speedup. Take note that `get_suggestions` using enchant is quite slow (~75% of the `cached_infer_correct_word` function).\n",
    "from joblib import Memory\n",
    "\n",
    "USE_JOBLIB_MEMORY = False\n",
    "\n",
    "if USE_JOBLIB_MEMORY:\n",
    "    respeller_cache_location = '/dev/shm/respeller-cachedir'\n",
    "    respeller_cache = Memory(respeller_cache_location, verbose=0)\n",
    "\n",
    "    cache_decorator = respeller_cache.cache\n",
    "else:\n",
    "    cache_decorator = redis_cacher\n",
    "\n",
    "# cache_decorator = lambda f: f\n",
    "\n",
    "en_dict = Dict('en_US') if ENCHANT_INSTALLED else pattern.en\n",
    "\n",
    "\n",
    "@cache_decorator\n",
    "def get_suggestions(word):\n",
    "    return en_dict.suggest(word)\n",
    "\n",
    "\n",
    "# @cache_decorator\n",
    "# def en_dict_check(word):\n",
    "#     # High overhead. Uncached speed ~100us vs cached speed ~500us\n",
    "#     return en_dict.check(word)\n",
    "\n",
    "\n",
    "def morph_word(word):\n",
    "    # word = word.replace(' ', '')  # Check if compound word suggestion matches the misspelled word\n",
    "    m_word = word + ''.join(sorted(word)) # Perform this opperation to add more robustness to the matching\n",
    "\n",
    "    return m_word\n",
    "\n",
    "\n",
    "@cache_decorator\n",
    "def cached_infer_correct_word(word, sim_thresh=0.0, print_log=False, min_len=3, use_suggest_score=True):\n",
    "    correct_word = None\n",
    "    score = -1\n",
    "\n",
    "    payload = dict(word=word,correct_word=correct_word, score=score)\n",
    "\n",
    "    if len(word) <= min_len:\n",
    "        return payload\n",
    "\n",
    "    candidates = get_suggestions(word)\n",
    "    if not ENCHANT_INSTALLED:\n",
    "        # Do this since pattern returns a tuple of (word, score)\n",
    "        candidates = [w for w, sim in candidates if sim > 0.1]\n",
    "\n",
    "    if use_suggest_score:\n",
    "        suggest_score = 1 / rankdata(range(len(candidates)))**0.5\n",
    "    else:\n",
    "        suggest_score = np.ones(len(candidates))\n",
    "\n",
    "    if candidates:\n",
    "        try:\n",
    "            m_word = morph_word(word)\n",
    "            m_candidates = [morph_word(c.lower()) for c in candidates]\n",
    "\n",
    "            tfidf = TfidfVectorizer(analyzer='char', ngram_range=(2, 4))\n",
    "            candX = tfidf.fit_transform(m_candidates)\n",
    "            wordX = tfidf.transform([m_word])\n",
    "\n",
    "            r = 1.0 / rankdata([edit_distance(m_word, x) for x in m_candidates])\n",
    "\n",
    "            sim = cosine_similarity(candX, wordX)\n",
    "            sim_r = sim * r.reshape(-1, 1) * suggest_score.reshape(-1, 1)\n",
    "\n",
    "            sim_ind = sim_r.argmax()\n",
    "            score = sim_r[sim_ind]\n",
    "            if score > sim_thresh:\n",
    "                correct_word = candidates[sim_ind]\n",
    "        except Exception as e:\n",
    "            # raise ValueError(word)\n",
    "            print(f\"Error word: {word}\")\n",
    "\n",
    "    if print_log:\n",
    "        print(sim_r)\n",
    "        print(r)\n",
    "        print(word)\n",
    "        print(candidates)\n",
    "        print(candidates[sim_ind])\n",
    "\n",
    "    payload['correct_word'] = correct_word\n",
    "    payload['score'] = float(score)\n",
    "\n",
    "    return payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "respell_error = []\n",
    "\n",
    "class Respeller:\n",
    "    '''\n",
    "    Use https://joblib.readthedocs.io/en/latest/auto_examples/memory_basic_usage.html#sphx-glr-auto-examples-memory-basic-usage-py\n",
    "    to efficiently cache data for parallel computing.\n",
    "    '''\n",
    "\n",
    "    # en_dict = Dict('en_US') if ENCHANT_INSTALLED else pattern.en\n",
    "    WORKERS = os.cpu_count() - 1\n",
    "    spell_cache = {}\n",
    "\n",
    "    def __init__(self, dictionary_file=None, spell_threshold=0.3, spell_cache=None):\n",
    "#         if spell_cache:\n",
    "#             self.spell_cache = spell_cache\n",
    "#         self.spell_cache = {}\n",
    "        self.spell_cache = spell_cache if spell_cache is not None else {}  # pd.Series()\n",
    "        self.dictionary_file = dictionary_file\n",
    "        self.spell_threshold = spell_threshold\n",
    "        self.stopwords = stopwords\n",
    "        \n",
    "        if (self.dictionary_file is not None) and os.path.isfile(self.dictionary_file):\n",
    "                self.spell_cache = pd.read_csv(self.dictionary_file)\n",
    "\n",
    "    def save_spell_cache(self):\n",
    "        pd.Series(self.spell_cache).to_csv(self.dictionary_file)\n",
    "\n",
    "    def infer_correct_word(self, word, sim_thresh=0.0, print_log=False, min_len=3, use_suggest_score=True):\n",
    "        if word not in self.spell_cache:\n",
    "            # Implement internal caching as well since Memory is still slow due to its utilization of disk.\n",
    "            payload = cached_infer_correct_word(word, sim_thresh=0.0, print_log=False, min_len=3, use_suggest_score=True)\n",
    "            self.spell_cache[word] = payload\n",
    "\n",
    "        return self.spell_cache[word]\n",
    "\n",
    "    def qualified_word(self, word):\n",
    "        stopwords = set(self.stopwords)\n",
    "        is_valid = (\n",
    "            (word not in stopwords) and\n",
    "            (not word[0].isupper()) and\n",
    "            len(word) > 2\n",
    "        )\n",
    "        \n",
    "        return is_valid\n",
    "    \n",
    "    def parallel_infer_correct_word(self, words, num_workers):\n",
    "        respelled_set = {}\n",
    "        \n",
    "        respell_results = [self.infer_correct_word(ew) for ew in words]\n",
    "\n",
    "        words = set([])\n",
    "\n",
    "        for res in respell_results:\n",
    "            word = res['word']\n",
    "            correct_word = res['correct_word']\n",
    "            score = res['score']\n",
    "\n",
    "            if correct_word and score > self.spell_threshold:\n",
    "                if correct_word.istitle():\n",
    "                    # If the respelling results to a `Title` word\n",
    "                    # it implies that the word is a proper noun, therefore, omit.\n",
    "                    words.add(word)\n",
    "                else:\n",
    "                    # Split and filter since some words are compound terms.\n",
    "                    respelled_set[word] = [i for i in correct_word.split() if self.qualified_word(i)]\n",
    "            else:\n",
    "                words.add(word)\n",
    "\n",
    "        return words, respelled_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimized cleaner with internal parallelization support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedSpellChecker(SpellChecker):\n",
    "    '''\n",
    "    Reduces the tokens only to unique words in the text. Output is not in the same order relative\n",
    "    to the original text.\n",
    "    '''\n",
    "    dict_words = set()\n",
    "    \n",
    "    def __init__(self, lang=None, text=None, tokenize=None, chunkers=None, filters=None):\n",
    "        super().__init__(\n",
    "            lang=lang, text=text, tokenize=tokenize, chunkers=chunkers, filters=filters\n",
    "        )\n",
    "        \n",
    "    def set_tokens(self, tokens):\n",
    "        \"\"\"Set the text to be spell-checked.\n",
    "        This method must be called, or the 'text' argument supplied\n",
    "        to the constructor, before calling the 'next()' method.\n",
    "        \"\"\"\n",
    "        self._tokens = enumerate(tokens)\n",
    "        \n",
    "    def next(self):\n",
    "        \"\"\"Process text up to the next spelling error.\n",
    "        \n",
    "        This method is designed to support the iterator protocol.\n",
    "        Each time it is called, it will advance the 'word' attribute\n",
    "        to the next spelling error in the text.  When no more errors\n",
    "        are found, it will raise StopIteration.\n",
    "        \n",
    "        The method will always return self, so that it can be used\n",
    "        sensibly in common idioms such as:\n",
    "            for err in checker:\n",
    "                err.do_something()\n",
    "        \n",
    "        \"\"\"\n",
    "        # Find the next spelling error.\n",
    "        # The uncaught StopIteration from next(self._tokens)\n",
    "        # will provide the StopIteration for this method\n",
    "        while True:\n",
    "            pos, word = next(self._tokens)\n",
    "            if word in self.dict_words:\n",
    "                continue\n",
    "            if self.dict.check(word):\n",
    "                self.dict_words.add(word)\n",
    "                continue\n",
    "            if word in self._ignore_words:\n",
    "                continue\n",
    "            self.word = word\n",
    "            self.wordpos = pos\n",
    "            if word in self._replace_words:\n",
    "                self.replace(self._replace_words[word])\n",
    "                continue\n",
    "            break\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    _lmtzr_spacy = spacy.load('en', disable=['parser', 'ner', 'textcat'])\n",
    "except OSError:\n",
    "    _lmtzr_spacy = spacy.load('/R/spacy_data/en_core_web_sm/en_core_web_sm-2.0.0', disable=['parser', 'ner', 'textcat'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cleaner:\n",
    "    \n",
    "    from enchant.checker import SpellChecker\n",
    "    \n",
    "    input_folder = ''\n",
    "    output_folder = ''\n",
    "    custom_stopwords = []\n",
    "    spellchecker = None\n",
    "    respeller = None\n",
    "    acronym_mapper = None\n",
    "    lemma = None\n",
    "    space_normalize_text_pattern = re.compile('[.\\s]{2,}')\n",
    "    noise_normalize_text_pattern = re.compile('[^a-z\\'\\.\\?\\!\\s]+')\n",
    "    short_valid_tokens_pattern = re.compile('[a-z]{3,}')\n",
    "    long_invalid_tokens_pattern = re.compile('\\w{25,}')\n",
    "\n",
    "    spell_cache_manager = multiprocessing.Manager()\n",
    "    spell_cache_dict = spell_cache_manager.dict()\n",
    "    \n",
    "    def __init__(\n",
    "        self, use_spellchecker=False, use_respeller=False,\n",
    "        use_lemmatizer=False, num_workers=None,\n",
    "        ignore_length=50, use_spacy=True,\n",
    "        replacements_plurals_to_singular_file=None,\n",
    "        acronyms_file=None,\n",
    "        min_en_lang_prob=0.98,\n",
    "        supported_lang=('en',),\n",
    "        logger=None,\n",
    "        check_language=True\n",
    "    ):\n",
    "        self.data=[]\n",
    "        self.check_language = check_language\n",
    "        self.use_spellchecker = use_spellchecker\n",
    "        self.use_lemmatizer = use_lemmatizer\n",
    "        self.use_respeller = use_respeller\n",
    "        self.num_workers = (os.cpu_count() - 1) if num_workers is None else num_workers\n",
    "        self.patterns = []\n",
    "        self.lemma_cache = {}\n",
    "        self.respelled_set = {}\n",
    "        self.use_spacy_lemmatizer = use_spacy\n",
    "        self.ignore_length = ignore_length\n",
    "        self.ENCHANT_INSTALLED = ENCHANT_INSTALLED\n",
    "        self.replacements_plurals_to_singular_file = replacements_plurals_to_singular_file\n",
    "        self.acronyms_file = acronyms_file\n",
    "        self.min_en_lang_prob = min_en_lang_prob\n",
    "        self.supported_lang = supported_lang\n",
    "        \n",
    "        if logger:\n",
    "            self.logger = logger.error\n",
    "        else:\n",
    "            self.logger = print\n",
    "        \n",
    "        self.plural_singular_map = {}\n",
    "        \n",
    "        if self.replacements_plurals_to_singular_file is not None:\n",
    "            self.build_plurals_to_singular_map()\n",
    "\n",
    "        if self.use_spellchecker:\n",
    "            # self.spellchecker = SpellChecker(\"en_US\") if ENCHANT_INSTALLED else pattern.en\n",
    "            self.spellchecker = OptimizedSpellChecker(\"en_US\") if ENCHANT_INSTALLED else pattern.en\n",
    "        \n",
    "        if self.use_lemmatizer:\n",
    "            self.lmtzr_spacy = None  # spacy.load('en')\n",
    "            self.lmtzr_wordnet = None if self.use_spacy_lemmatizer else WordNetLemmatizer()\n",
    "\n",
    "        if self.use_respeller:\n",
    "            self.respeller = Respeller(spell_threshold=0.7, spell_cache=self.spell_cache_dict)\n",
    "            \n",
    "        if self.acronyms_file is not None:\n",
    "            self.acronym_mapper = AcronymMapper(whitelist_file=self.acronyms_file, sim_thresh=0.8)\n",
    "            \n",
    "        self.stopwords = stopwords\n",
    "        \n",
    "        # initialize clean_text\n",
    "        self.clean_text('initialize cleaner')\n",
    "\n",
    "    def build_plurals_to_singular_map(self):\n",
    "        '''\n",
    "        Assume that the whitelist is a two column excel file without a header: first col - plural, second col - singular.\n",
    "        Don't catch exception such that any errors will be apparent.\n",
    "        '''\n",
    "        plural_singular_map = pd.read_csv(self.replacements_plurals_to_singular_file, header=None, index_col=0).dropna()[1]\n",
    "        self.plural_singular_map = dict(plural_singular_map)\n",
    "\n",
    "    def set_input_folder(self,input_folder):\n",
    "        self.input_folder = input_folder\n",
    "\n",
    "    def set_output_folder(self,output_folder):\n",
    "        self.output_folder = output_folder\n",
    "        if not os.path.isdir(self.output_folder):\n",
    "            os.makedirs(self.output_folder)\n",
    "\n",
    "    def set_custom_stopwords(self, stopwords):\n",
    "        self.custom_stopwords = stopwords\n",
    "\n",
    "    # remove noise words\n",
    "    def remove_noise(self, text):\n",
    "        text = text.lower()\n",
    "        text = self.long_invalid_tokens_pattern.sub('', text)\n",
    "        text = ' '.join(self.short_valid_tokens_pattern.findall(text))\n",
    "\n",
    "        return text\n",
    "    \n",
    "    def get_lemma(self, word, word_pos):\n",
    "        stopwords = set(self.stopwords)\n",
    "        if word in stopwords:\n",
    "            return None\n",
    "        \n",
    "        key = (word, word_pos)\n",
    "        \n",
    "        if key not in self.lemma_cache:\n",
    "            lemma = self.lmtzr_wordnet.lemmatize(word, word_pos)\n",
    "            self.lemma_cache[key] = lemma\n",
    "            \n",
    "        return self.lemma_cache[key]\n",
    "\n",
    "    def lemmatize_text_wordnet(self, text):\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "\n",
    "        txt_out = ''\n",
    "\n",
    "        # Before lemmatizing, we tag words (part-of-speech tagging)\n",
    "        tagged_tokens = pos_tag(tokens)    \n",
    "\n",
    "        # We now lemmatize based on a simplified list of POS tags\n",
    "        for tagged_token in tagged_tokens:\n",
    "            word = tagged_token[0]\n",
    "            word_pos = tagged_token[1]\n",
    "\n",
    "            # We recode NLTK tagging for consistency with wordnet \n",
    "            if tagged_token[1].startswith('J'):\n",
    "                word_pos = wordnet.ADJ\n",
    "            elif tagged_token[1].startswith('V'):\n",
    "                word_pos = wordnet.VERB\n",
    "            elif tagged_token[1].startswith('N'):\n",
    "                word_pos = wordnet.NOUN\n",
    "            elif tagged_token[1].startswith('R'):\n",
    "                word_pos = wordnet.ADV\n",
    "            else:\n",
    "                word_pos = wordnet.NOUN # Assume noun if other  \n",
    "            \n",
    "            # We now lemmatize, taking the POS tag into account\n",
    "            lemma = self.get_lemma(word, word_pos)\n",
    "            \n",
    "            if lemma is not None:\n",
    "                txt_out = txt_out + lemma + ' '\n",
    "\n",
    "        return txt_out  \n",
    "\n",
    "    def lemmatize_text_spacy(self, text):\n",
    "        stopwords = set(self.stopwords)\n",
    "#         try:\n",
    "#             lmtzr_spacy = spacy.load('en', disable=['parser', 'ner', 'textcat'])\n",
    "#         except OSError:\n",
    "#             lmtzr_spacy = spacy.load('/R/spacy_data/en_core_web_sm/en_core_web_sm-2.0.0', disable=['parser', 'ner', 'textcat'])\n",
    "\n",
    "        doc = _lmtzr_spacy(text.lower())\n",
    "            # ' '.join(re.findall('[a-z0-9]+', text.lower())))\n",
    "        \n",
    "        txt_out = [token.lemma_ for token in doc if token.lemma_ not in stopwords]\n",
    "        txt_out = ' '.join(txt_out)\n",
    "        txt_out = txt_out.replace('-PRON-', '')\n",
    "\n",
    "        return txt_out\n",
    "    \n",
    "    def space_normalize_text(self, text):\n",
    "        text = self.space_normalize_text_pattern.sub(' . ', text.lower())\n",
    "        text = self.noise_normalize_text_pattern.sub(' ', text)\n",
    "        return text\n",
    "\n",
    "    def lemmatize_text(self, text):\n",
    "        # Perform preliminary removal of noise\n",
    "        text = self.space_normalize_text(text)\n",
    "\n",
    "        txt_out = ''\n",
    "        if self.use_spacy_lemmatizer:\n",
    "            txt_out = self.lemmatize_text_spacy(text)\n",
    "        else:\n",
    "            txt_out = self.lemmatize_text_wordnet(text)\n",
    "            \n",
    "        return txt_out\n",
    "    \n",
    "    def get_misspelled_tokens(self, text):\n",
    "        if self.spellchecker is None:\n",
    "            raise ValueError('Spellchecker is not enabled')\n",
    "        \n",
    "        errors = set([])\n",
    "\n",
    "#         if ENCHANT_INSTALLED:\n",
    "#             # Input is a text\n",
    "#             self.spellchecker.set_text(text)\n",
    "\n",
    "#             for err in self.spellchecker:\n",
    "#                 #print (err.word)\n",
    "#                 if err.word not in self.respelled_set:\n",
    "#                     errors.add(err.word)\n",
    "\n",
    "        if ENCHANT_INSTALLED:\n",
    "            # Input is a list of tokens\n",
    "            text_tokens = set(text)\n",
    "            self.spellchecker.set_tokens(text)\n",
    "\n",
    "            for err in self.spellchecker:\n",
    "                # print (err.word)\n",
    "                if err.word not in self.respelled_set:\n",
    "                    errors.add(err.word)\n",
    "        else:\n",
    "            # Input is a list of tokens\n",
    "            text_tokens = set(text)\n",
    "            for token in text_tokens:\n",
    "                suggestions = self.spellchecker.suggest(token)\n",
    "                \n",
    "                # If suggestions are available, make sure that the first\n",
    "                # suggestion is similar to the token to make sure\n",
    "                # that the token being testing is a legit word.\n",
    "                if suggestions and (suggestions[0][0] == token and suggestions[0][1] > 0.9):\n",
    "                    continue\n",
    "                else:\n",
    "                    errors.add(token)\n",
    "        return errors\n",
    "\n",
    "    # Run spell checker on text to keep words found in dictionary only\n",
    "    def spellcheck_text(self, text):\n",
    "        text_tokens=word_tokenize(text)\n",
    "        errors = self.get_misspelled_tokens(text_tokens)  # text if ENCHANT_INSTALLED else text_tokens)\n",
    "\n",
    "        if errors and self.respeller:\n",
    "            errors, respelled_set = self.respeller.parallel_infer_correct_word(errors, self.num_workers * 2)  # max((self.num_workers // 2), 1))\n",
    "            # print(respelled_set)\n",
    "            self.respelled_set.update(respelled_set)\n",
    "\n",
    "        errors_set=set(errors)\n",
    "        cleaned_text = []\n",
    "        \n",
    "        for x in text_tokens:\n",
    "            if (x in errors_set):\n",
    "                continue\n",
    "            \n",
    "            elif x in self.respelled_set:\n",
    "                for x in self.respelled_set[x]:\n",
    "                    x = self.plural_singular_map.get(x, x)\n",
    "                    cleaned_text.append(x)\n",
    "\n",
    "            elif (x in self.stopwords):\n",
    "                continue\n",
    "\n",
    "            else:\n",
    "                x = self.plural_singular_map.get(x, x)\n",
    "                cleaned_text.append(x)\n",
    "\n",
    "        output={}\n",
    "        output['text']=\" \".join(cleaned_text)\n",
    "        output['errors']=errors\n",
    "   \n",
    "        return output\n",
    "\n",
    "    def load_existing_and_extract_metadata(self, fileid, filepath, save_docs, process_output_dict=None):\n",
    "        proc_fileid = fileid\n",
    "        \n",
    "        filename = filepath.split('/')[-1]\n",
    "        fileid = filename.strip('.txt')\n",
    "        \n",
    "        with open(filepath) as fl:\n",
    "            text = fl.read()\n",
    "        \n",
    "        lang_log = ('ERROR', 0)\n",
    "        token_log = 0\n",
    "        skipped_log = ''\n",
    "        text_log = ''\n",
    "        spell_errors = []\n",
    "        exp = None\n",
    "        write_status = True\n",
    "\n",
    "        predict_lang = detect_langs(text)[0]\n",
    "        lang_log = (predict_lang.lang, predict_lang.prob)\n",
    "        # Log tokens count\n",
    "        token_log = len(word_tokenize(text))\n",
    "        text_log = text\n",
    "\n",
    "        cleaning_output = dict(\n",
    "            lang=lang_log,\n",
    "            token=token_log,\n",
    "            text=text_log,\n",
    "            skipped=skipped_log,\n",
    "            spell_errors=spell_errors,\n",
    "            exception=exp,\n",
    "            write_status=write_status,\n",
    "        )\n",
    "\n",
    "        # log statistics\n",
    "        lang_log = {}  # lang info per document - uses the format - lang_log[fileid]={'score','lang'}\n",
    "        spell_errors = {}\n",
    "        token_log = {}  # Tokens count per document\n",
    "        text_errors = {}\n",
    "        text_log = {}\n",
    "        skipped_log = {}\n",
    "        exception_log = {}\n",
    "        write_status_log = {}\n",
    "\n",
    "        lang_log[fileid] = cleaning_output['lang']\n",
    "        token_log[fileid] = cleaning_output['token']\n",
    "        skipped_log[fileid] = cleaning_output['skipped']\n",
    "        text_log[fileid] = cleaning_output['text']\n",
    "        spell_errors[fileid] = cleaning_output['spell_errors']\n",
    "        exception_log[fileid] = cleaning_output['exception']\n",
    "        write_status_log[fileid] = cleaning_output['write_status']\n",
    "        \n",
    "        # return logs\n",
    "        output_log = {}\n",
    "        output_log['lang'] = lang_log\n",
    "        output_log['tokens'] = token_log\n",
    "        output_log['text'] = text_log\n",
    "        output_log['spell_errors'] = spell_errors\n",
    "        output_log['skipped'] = skipped_log\n",
    "        output_log['exception'] = exception_log\n",
    "        output_log['write_status'] = write_status_log\n",
    "        \n",
    "        if process_output_dict is not None:\n",
    "            process_output_dict[proc_fileid] = output_log\n",
    "        else: \n",
    "            return output_log\n",
    "\n",
    "    def clean_text(self, text, filen=None):\n",
    "        lang_log = ('ERROR', 0)\n",
    "        token_log = 0\n",
    "        skipped_log = ''\n",
    "        text_log = ''\n",
    "        spell_errors = []\n",
    "        exp = None\n",
    "        write_status = False\n",
    "        \n",
    "        if self.acronym_mapper is not None:\n",
    "            text = self.acronym_mapper.expand_doc_acronyms(text)\n",
    "\n",
    "        text = text.lower()\n",
    "        len_text = len(text)\n",
    "        \n",
    "        if len_text > self.ignore_length:\n",
    "            \n",
    "            if self.use_lemmatizer:\n",
    "                # Apply lemmatizer\n",
    "                try:\n",
    "                    text = self.lemmatize_text(text)\n",
    "                except Exception as excp:\n",
    "                    self.logger(f'Failed lemmatization for {filen}')\n",
    "                    exp = excp.args[0]\n",
    "\n",
    "            if exp is None:\n",
    "                # Remove noise words e.g. punctuation, numbers, non-utf characters etc\n",
    "                text = self.remove_noise(text)\n",
    "\n",
    "                # Skip documents with no content\n",
    "                if len(text) > 0:      \n",
    "                    # Detect majority language of the document \n",
    "                    try:\n",
    "                        predict_lang = detect_langs(text)[0]\n",
    "\n",
    "                        lang_log = (predict_lang.lang, predict_lang.prob)\n",
    "\n",
    "                        if self.check_language:\n",
    "                            if (any([predict_lang.lang == lg for lg in self.supported_lang])) and (predict_lang.prob >= self.min_en_lang_prob):   # Only process documents in English                            \n",
    "                                if self.use_spellchecker:\n",
    "                                    # Run spell check and keep only the words found in dictionary\n",
    "                                    spell_data = self.spellcheck_text(text)\n",
    "                                    spell_errors = spell_data['errors']\n",
    "                                    text = spell_data['text']\n",
    "\n",
    "                                # Log tokens count\n",
    "                                token_log = len(word_tokenize(text))\n",
    "                                write_status = True\n",
    "                            else:\n",
    "                                #not in english\n",
    "                                skipped_log = f'Not in english | {predict_lang}'\n",
    "                        else:\n",
    "                            if self.use_spellchecker:\n",
    "                                # Run spell check and keep only the words found in dictionary\n",
    "                                spell_data = self.spellcheck_text(text)\n",
    "                                spell_errors = spell_data['errors']\n",
    "                                text = spell_data['text']\n",
    "                                \n",
    "                            # Log tokens count\n",
    "                            token_log = len(word_tokenize(text))\n",
    "                            write_status = True\n",
    "\n",
    "                    except Exception as excp:\n",
    "                        skipped_log = f\"Error detecting language for = {filen}. {excp.args[0]}\"\n",
    "                        self.logger(skipped_log)\n",
    "                        exp = excp.args[0]\n",
    "                else:\n",
    "                    skipped_log = f\"Empty doc post lemmatizer = {filen}\"\n",
    "                    self.logger(skipped_log)\n",
    "                    # Log tokens count\n",
    "        else:\n",
    "            skipped_log = f\"Doclen {len_text} < {self.ignore_length} = {filen}\"\n",
    "            self.logger(skipped_log)\n",
    "            # Log tokens count\n",
    "            token_log = 0\n",
    "\n",
    "        text_log = text\n",
    "\n",
    "        payload = dict(\n",
    "            lang=lang_log,\n",
    "            token=token_log,\n",
    "            text=text_log,\n",
    "            skipped=skipped_log,\n",
    "            spell_errors=spell_errors,\n",
    "            exception=exp,\n",
    "            write_status=write_status,\n",
    "        )\n",
    "\n",
    "        payload = {k: v if not isinstance(v, set) else list(v) for k, v in payload.items()}\n",
    "        return payload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorpusCleaner(Cleaner):\n",
    "\n",
    "    # Clean documents using spell checker\n",
    "    def batch_clean_docs(self, doclist, batch_size=None, save_docs=False, collect_text_log=False, collect_spell_errors=False, skip_existing=True, default_docs_per_worker=20):\n",
    "        if batch_size is None:\n",
    "            # Use a multiplier for efficient usage of workers\n",
    "            batch_size = default_docs_per_worker * self.num_workers\n",
    "\n",
    "        file_counter_x = 0\n",
    "        input_folder  = self.input_folder\n",
    "        output_folder = self.output_folder\n",
    "        \n",
    "        #log statistics\n",
    "        lang_log = {} # Lang info per document - uses the format - lang_log[fileid]=('lang', 'score')\n",
    "        text_log = {} # Errors count per document\n",
    "        token_log = {} # Tokens count per document\n",
    "        skipped_log = {} # Documents not processed\n",
    "        spell_errors = {}\n",
    "        exception_log = {}\n",
    "        write_status_log = {}\n",
    "        \n",
    "        log_interval = batch_size\n",
    "\n",
    "        with Parallel(n_jobs=self.num_workers, backend='multiprocessing') as pool:\n",
    "            # Cleaning all text files found in input in folder\n",
    "            batch = []\n",
    "            for ix, fileid in enumerate(doclist):\n",
    "                if ix % log_interval == 0:\n",
    "                    self.logger(f'Docset {ix}')\n",
    "\n",
    "                file_counter_x += 1\n",
    "                if fileid.endswith('.txt'):    # text files only \n",
    "\n",
    "                    filen = os.path.join(input_folder, fileid)     # input file \n",
    "                    newfile = os.path.join(output_folder, fileid)   # output file\n",
    "                    \n",
    "                    if not os.path.isfile(filen):\n",
    "                        self.logger(f\"No input file: {fileid}\")\n",
    "                        continue\n",
    "\n",
    "                    # Skip if output file already exists\n",
    "                    if os.path.isfile(newfile) and skip_existing:\n",
    "                        # self.logger(f\"Output file exists: {fileid}. Skipping...\")\n",
    "                        continue\n",
    "\n",
    "                    if len(batch) != batch_size:\n",
    "                        batch.append(filen)\n",
    "\n",
    "                    else:\n",
    "                        with Timer() as timer:\n",
    "                            doc_outputs = pool((delayed(self.clean_doc)(fln, save_doc=save_docs) for fln in batch))\n",
    "                            # doc_outputs = Parallel(n_jobs=self.num_workers, backend='multiprocessing')(delayed(self.clean_doc)(fln, save_doc=save_docs) for fln in batch)\n",
    "                            # doc_outputs = pool.map(self.clean_doc, [(fln, save_docs) for fln in batch], chunksize=batch_size)\n",
    "\n",
    "                            for doc_output in doc_outputs:\n",
    "\n",
    "                                lang_log.update(doc_output['lang'])\n",
    "                                token_log.update(doc_output['tokens'])\n",
    "                                skipped_log.update(doc_output['skipped'])  \n",
    "                                exception_log.update(doc_output['exception'])\n",
    "                                write_status_log.update(doc_output['write_status'])\n",
    "\n",
    "                                if collect_text_log:\n",
    "                                    # Don't do this if you're processing a lot of docs\n",
    "                                    text_log.update(doc_output['text'])\n",
    "                                if collect_spell_errors:\n",
    "                                    spell_errors.update(doc_output['spell_errors'])\n",
    "\n",
    "                            batch = []\n",
    "                            \n",
    "                        self.logger(f'Set {ix}: {log_interval} items for {timer.elapsed:.2f} seconds.')\n",
    "\n",
    "            if batch:\n",
    "                doc_outputs = pool((delayed(self.clean_doc)(fln, save_doc=save_docs) for fln in batch))\n",
    "                # doc_outputs = Parallel(n_jobs=self.num_workers, backend='multiprocessing')(delayed(self.clean_doc)(fln, save_doc=save_docs) for fln in batch)\n",
    "                # doc_outputs = pool.map(self.clean_doc, [(fln, save_docs) for fln in batch], chunksize=batch_size)\n",
    "\n",
    "                for doc_output in doc_outputs:\n",
    "\n",
    "                    lang_log.update(doc_output['lang'])\n",
    "                    token_log.update(doc_output['tokens'])\n",
    "                    skipped_log.update(doc_output['skipped']) \n",
    "                    exception_log.update(doc_output['exception'])\n",
    "                    write_status_log.update(doc_output['write_status'])\n",
    "\n",
    "                    if collect_text_log:\n",
    "                        # Don't do this if you're processing a lot of docs\n",
    "                        text_log.update(doc_output['text'])\n",
    "                    if collect_spell_errors:\n",
    "                        spell_errors.update(doc_output['spell_errors'])\n",
    "\n",
    "        output_log = {}\n",
    "        output_log['lang'] = lang_log\n",
    "        output_log['tokens'] = token_log\n",
    "        output_log['text'] = text_log\n",
    "        output_log['spell_errors'] = spell_errors\n",
    "        output_log['skipped'] = skipped_log\n",
    "        output_log['exception'] = exception_log\n",
    "        output_log['write_status'] = write_status_log\n",
    "\n",
    "        return output_log\n",
    "\n",
    "    # Clean a single document using spell checker    \n",
    "    def clean_doc(self, filepath, save_doc=False):  # args):\n",
    "        # filepath, *save_doc = args\n",
    "\n",
    "        if save_doc is None:\n",
    "            save_doc = False\n",
    "\n",
    "        # log statistics\n",
    "        lang_log = {}  # lang info per document - uses the format - lang_log[fileid]={'score','lang'}\n",
    "        spell_errors = {}\n",
    "        token_log = {}  # Tokens count per document\n",
    "        text_errors = {}\n",
    "        text_log = {}\n",
    "        skipped_log = {}\n",
    "        exception_log = {}\n",
    "        write_status_log = {}\n",
    "\n",
    "        filename = filepath.split('/')[-1]\n",
    "    \n",
    "        fileid = filename.strip('.txt')\n",
    "        \n",
    "        with open(filepath, 'rb') as fl:\n",
    "            # Use context so that the file will be closed automatically upon exit from the context.\n",
    "            text = fl.read()\n",
    "            text = text.decode('utf-8', errors='ignore')\n",
    "        \n",
    "        cleaning_output = self.clean_text(text, filen=fileid)\n",
    "        text = cleaning_output['text']\n",
    "        \n",
    "        lang_log[fileid] = cleaning_output['lang']\n",
    "        token_log[fileid] = cleaning_output['token']\n",
    "        skipped_log[fileid] = cleaning_output['skipped']\n",
    "        text_log[fileid] = text\n",
    "        spell_errors[fileid] = cleaning_output['spell_errors']\n",
    "        exception_log[fileid] = cleaning_output['exception']\n",
    "        write_status_log[fileid] = cleaning_output['write_status']\n",
    "        \n",
    "        if save_doc and cleaning_output['write_status']:\n",
    "            with open(os.path.join(self.output_folder, filename), 'w') as fl:\n",
    "                fl.write(text)\n",
    "\n",
    "        # return logs\n",
    "        output_log = {}\n",
    "        output_log['lang'] = lang_log\n",
    "        output_log['tokens'] = token_log\n",
    "        output_log['text'] = text_log\n",
    "        output_log['spell_errors'] = spell_errors\n",
    "        output_log['skipped'] = skipped_log\n",
    "        output_log['exception'] = exception_log\n",
    "        output_log['write_status'] = write_status_log\n",
    "\n",
    "        return output_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ParallelCorpusCleaner(Cleaner):\n",
    "\n",
    "    # Clean documents using spell checker    \n",
    "    def batch_clean_docs(self, doclist, batch_size=None, save_docs=False, collect_text_log=False, collect_spell_errors=False, skip_existing=True):\n",
    "        if batch_size is None:\n",
    "            # Use a multiplier for efficient usage of workers\n",
    "            batch_size = 4 * self.num_workers\n",
    "\n",
    "        file_counter_x = 0\n",
    "        input_folder  = self.input_folder\n",
    "        output_folder = self.output_folder\n",
    "        \n",
    "        #log statistics\n",
    "        lang_log = {} # Lang info per document - uses the format - lang_log[fileid]=('lang', 'score')\n",
    "        text_log = {} # Errors count per document\n",
    "        token_log = {} # Tokens count per document\n",
    "        skipped_log = {} # Documents not processed\n",
    "        spell_errors = {}\n",
    "        exception_log = {}\n",
    "        write_status_log = {}\n",
    "        \n",
    "        log_interval = batch_size\n",
    "        \n",
    "        process_output_manager = multiprocessing.Manager()\n",
    "        process_output_dict = process_output_manager.dict()\n",
    "\n",
    "        batch = {}\n",
    "        ix = 0\n",
    "        while True:\n",
    "            try:\n",
    "                if len(batch) < batch_size:\n",
    "                    fileid = doclist.pop(0)\n",
    "                    # print(f'Processing {ix}: {fileid}')\n",
    "                    ix += 1\n",
    "\n",
    "                    if ix % log_interval == 0:\n",
    "                        self.logger(f'Docset {ix}')\n",
    "\n",
    "                    if fileid.endswith('.txt'):    # text files only \n",
    "                        filen = os.path.join(input_folder, fileid)     # input file \n",
    "                        newfile = os.path.join(output_folder, fileid)   # output file\n",
    "\n",
    "                        if not os.path.isfile(filen):\n",
    "                            self.logger(f\"No input file: {fileid}\")\n",
    "                            continue\n",
    "\n",
    "                        # Skip if output file already exists\n",
    "                        if os.path.isfile(newfile) and skip_existing:\n",
    "                            p = multiprocessing.Process(target=self.load_existing_and_extract_metadata, args=(fileid, newfile, save_docs, process_output_dict))  # , kwargs=kwargs)\n",
    "                            batch[fileid] = p\n",
    "                            p.start()\n",
    "                            # self.logger(f\"Output file exists: {fileid}. Skipping...\")\n",
    "                            continue\n",
    "\n",
    "                        # kwargs = {'process_output_dict': process_output_dict, 'save_doc': save_docs}                    \n",
    "                        p = multiprocessing.Process(target=self.clean_doc, args=(fileid, filen, save_docs, process_output_dict))  # , kwargs=kwargs)\n",
    "                        batch[fileid] = p\n",
    "                        p.start()\n",
    "                else:\n",
    "                    completed_data = set(process_output_dict.keys())\n",
    "\n",
    "                    for fld in completed_data:\n",
    "                        # print(f'Completed {fld}')\n",
    "                        if fld not in batch:\n",
    "                            continue\n",
    "                        pk = batch.pop(fld)\n",
    "                        pk.join()\n",
    "\n",
    "                        # This shouldn't be necessary but still doing this just to be safe... :)\n",
    "                        if pk.is_alive():\n",
    "                            pk.terminate()\n",
    "                            pk.join()\n",
    "\n",
    "                        fileid = doclist.pop(0)\n",
    "                        ix += 1\n",
    "                        \n",
    "                        # print(f'Starting {fileid}')\n",
    "\n",
    "                        if ix % log_interval == 0:\n",
    "                            self.logger(f'Docset {ix}')\n",
    "\n",
    "                        if fileid.endswith('.txt'):    # text files only \n",
    "                            filen = os.path.join(input_folder, fileid)     # input file \n",
    "                            newfile = os.path.join(output_folder, fileid)   # output file\n",
    "\n",
    "                            if not os.path.isfile(filen):\n",
    "                                self.logger(f\"No input file: {fileid}\")\n",
    "                                continue\n",
    "\n",
    "                            # Skip if output file already exists\n",
    "                            if os.path.isfile(newfile) and skip_existing:\n",
    "                                p = multiprocessing.Process(target=self.load_existing_and_extract_metadata, args=(fileid, newfile, save_docs, process_output_dict))  # , kwargs=kwargs)\n",
    "                                batch[fileid] = p\n",
    "                                p.start()\n",
    "                                # self.logger(f\"Output file exists: {fileid}. Skipping...\")\n",
    "                                continue\n",
    "\n",
    "                            # kwargs = {'process_output_dict': process_output_dict, 'save_doc': save_docs}                    \n",
    "                            p = multiprocessing.Process(target=self.clean_doc, args=(fileid, filen, save_docs, process_output_dict))  # , kwargs=kwargs)\n",
    "                            batch[fileid] = p\n",
    "                            p.start()\n",
    "                            \n",
    "            except Exception as e:\n",
    "                print(f'Exception received: {e.args[0]}')\n",
    "                bfileids = set(batch.keys())\n",
    "                for fileid in bfileids:\n",
    "                    p = batch.pop(fileid)\n",
    "                    p.join()\n",
    "\n",
    "                    if p.is_alive():\n",
    "                        p.terminate()\n",
    "                        p.join()\n",
    "                break\n",
    "\n",
    "        # Cleanup just in case... \n",
    "        bfileids = set(batch.keys())\n",
    "        for fileid in bfileids:\n",
    "            p = batch.pop(fileid)\n",
    "            p.join()\n",
    "\n",
    "            if p.is_alive():\n",
    "                p.terminate()\n",
    "                p.join()\n",
    "                \n",
    "        for fileid in process_output_dict.keys():\n",
    "            doc_output = process_output_dict[fileid]\n",
    "\n",
    "            lang_log.update(doc_output['lang'])\n",
    "            token_log.update(doc_output['tokens'])\n",
    "            skipped_log.update(doc_output['skipped'])  \n",
    "            exception_log.update(doc_output['exception'])\n",
    "            write_status_log.update(doc_output['write_status'])\n",
    "\n",
    "            if collect_text_log:\n",
    "                # Don't do this if you're processing a lot of docs\n",
    "                text_log.update(doc_output['text'])\n",
    "            if collect_spell_errors:\n",
    "                spell_errors.update(doc_output['spell_errors'])\n",
    "\n",
    "        output_log = {}\n",
    "        output_log['lang'] = lang_log\n",
    "        output_log['tokens'] = token_log\n",
    "        output_log['text'] = text_log\n",
    "        output_log['spell_errors'] = spell_errors\n",
    "        output_log['skipped'] = skipped_log\n",
    "        output_log['exception'] = exception_log\n",
    "        output_log['write_status'] = write_status_log\n",
    "\n",
    "        return output_log\n",
    "\n",
    "    # Clean a single document using spell checker    \n",
    "    def clean_doc(self, fileid, filepath, save_doc=False, process_output_dict=None):  # args):\n",
    "        proc_fileid = fileid\n",
    "        # filepath, *save_doc = args\n",
    "\n",
    "        if save_doc is None:\n",
    "            save_doc = False\n",
    "\n",
    "        # log statistics\n",
    "        lang_log = {}  # lang info per document - uses the format - lang_log[fileid]={'score','lang'}\n",
    "        spell_errors = {}\n",
    "        token_log = {}  # Tokens count per document\n",
    "        text_errors = {}\n",
    "        text_log = {}\n",
    "        skipped_log = {}\n",
    "        exception_log = {}\n",
    "        write_status_log = {}\n",
    "\n",
    "        filename = filepath.split('/')[-1]\n",
    "    \n",
    "        fileid = filename.strip('.txt')\n",
    "        \n",
    "        with open(filepath, 'rb') as fl:\n",
    "            # Use context so that the file will be closed automatically upon exit from the context.\n",
    "            text = fl.read()\n",
    "            text = text.decode('utf-8', errors='ignore')\n",
    "            text = text.lower()\n",
    "        \n",
    "        cleaning_output = self.clean_text(text, filen=fileid)\n",
    "        text = cleaning_output['text']\n",
    "        \n",
    "        lang_log[fileid] = cleaning_output['lang']\n",
    "        token_log[fileid] = cleaning_output['token']\n",
    "        skipped_log[fileid] = cleaning_output['skipped']\n",
    "        text_log[fileid] = text\n",
    "        spell_errors[fileid] = cleaning_output['spell_errors']\n",
    "        exception_log[fileid] = cleaning_output['exception']\n",
    "        write_status_log[fileid] = cleaning_output['write_status']\n",
    "        \n",
    "        if save_doc and cleaning_output['write_status']:\n",
    "            with open(os.path.join(self.output_folder, filename), 'w') as fl:\n",
    "                fl.write(text)\n",
    "\n",
    "        # return logs\n",
    "        output_log = {}\n",
    "        output_log['lang'] = lang_log\n",
    "        output_log['tokens'] = token_log\n",
    "        output_log['text'] = text_log\n",
    "        output_log['spell_errors'] = spell_errors\n",
    "        output_log['skipped'] = skipped_log\n",
    "        output_log['exception'] = exception_log\n",
    "        output_log['write_status'] = write_status_log\n",
    "        \n",
    "        if process_output_dict is not None:\n",
    "            process_output_dict[proc_fileid] = output_log\n",
    "        else: \n",
    "            return output_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# payload = dict(\n",
    "#     lang=lang_log,\n",
    "#     token=token_log,\n",
    "#     text=text_log,\n",
    "#     skipped=skipped_log,\n",
    "#     spell_errors=spell_errors,\n",
    "#     exception=exp,\n",
    "#     write_status=write_status,\n",
    "# )\n",
    "\n",
    "# payload = {k: v if not isinstance(v, set) else list(v) for k, v in payload.items()}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Input document text and process documents with at least `ignore_length` characters.\n",
    "\n",
    "Step 1: Detect countries from the document if a country map file is provided.\n",
    "Step 2: Apply lemmatization if specified (lemmatizer options: spacy or nltk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}