{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize spell checking engine\n",
    "\n",
    "### Description\n",
    "Implementation of a fast spell checking and suggestion engine using character vectors.\n",
    "\n",
    "### Methodology:\n",
    "- Acquire a words list that will serve as the absolute truth of spellings.\n",
    "- Use tfidf or phoenetic vectorization method to encode words.\n",
    "- Use similarity metrics to identify possible respelling of out of vocabulary terms in a document.\n",
    "\n",
    "### Word list resources\n",
    "\n",
    "- http://wordlist.aspell.net/\n",
    "- https://github.com/dwyl/english-words :: https://github.com/dwyl/english-words/blob/master/words_dictionary.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import json\n",
    "\n",
    "from nltk.corpus import words\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing as mp\n",
    "import multiprocessing\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.metrics.distance import edit_distance\n",
    "\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "import spacy\n",
    "\n",
    "from contexttimer import Timer\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.43 ms\n"
     ]
    }
   ],
   "source": [
    "from lsh import minhash, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 7.3 ms\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.73 ms\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.87 ms\n"
     ]
    }
   ],
   "source": [
    "from enchant.checker import SpellChecker\n",
    "from enchant import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 77.6 ms\n"
     ]
    }
   ],
   "source": [
    "class OptimizedSpellChecker(SpellChecker):\n",
    "    '''\n",
    "    Reduces the tokens only to unique words in the text. Output is not in the same order relative\n",
    "    to the original text.\n",
    "    '''\n",
    "    dict_words = set()\n",
    "    \n",
    "    def __init__(self, lang=None, text=None, tokenize=None, chunkers=None, filters=None):\n",
    "        super().__init__(\n",
    "            lang=lang, text=text, tokenize=tokenize, chunkers=chunkers, filters=filters\n",
    "        )\n",
    "        \n",
    "    def set_tokens(self, tokens):\n",
    "        \"\"\"Set the text to be spell-checked.\n",
    "        This method must be called, or the 'text' argument supplied\n",
    "        to the constructor, before calling the 'next()' method.\n",
    "        \"\"\"\n",
    "        self._tokens = enumerate(tokens)\n",
    "        \n",
    "    def next(self):\n",
    "        \"\"\"Process text up to the next spelling error.\n",
    "        \n",
    "        This method is designed to support the iterator protocol.\n",
    "        Each time it is called, it will advance the 'word' attribute\n",
    "        to the next spelling error in the text.  When no more errors\n",
    "        are found, it will raise StopIteration.\n",
    "        \n",
    "        The method will always return self, so that it can be used\n",
    "        sensibly in common idioms such as:\n",
    "            for err in checker:\n",
    "                err.do_something()\n",
    "        \n",
    "        \"\"\"\n",
    "        # Find the next spelling error.\n",
    "        # The uncaught StopIteration from next(self._tokens)\n",
    "        # will provide the StopIteration for this method\n",
    "        while True:\n",
    "            pos, word = next(self._tokens)\n",
    "            if word in self.dict_words:\n",
    "                continue\n",
    "            if self.dict.check(word):\n",
    "                self.dict_words.add(word)\n",
    "                continue\n",
    "            if word in self._ignore_words:\n",
    "                continue\n",
    "            self.word = word\n",
    "            self.wordpos = pos\n",
    "            if word in self._replace_words:\n",
    "                self.replace(self._replace_words[word])\n",
    "                continue\n",
    "            break\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 14.4 ms\n"
     ]
    }
   ],
   "source": [
    "en_dict = Dict(u'en_US')\n",
    "spellchecker = OptimizedSpellChecker(u\"en_US\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32.7 ms, sys: 319 Âµs, total: 33 ms\n",
      "Wall time: 32.2 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['nighttime',\n",
       " 'nighttimes',\n",
       " 'night time',\n",
       " 'night-time',\n",
       " 'nightmare',\n",
       " 'nightmarish']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 40.2 ms\n"
     ]
    }
   ],
   "source": [
    "%time en_dict.suggest(u'nighttime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![im](https://cdn-images-1.medium.com/max/1600/1*5bvgtaEv3UcowA3vw8SWpQ.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.71 ms\n"
     ]
    }
   ],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "416297\n",
      "time: 142 ms\n"
     ]
    }
   ],
   "source": [
    "with open('./data/words.txt') as fl:\n",
    "    words = dict(enumerate([i.strip() for i in fl.readlines() if i.strip().isalpha()]))\n",
    "\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 24.8 s\n"
     ]
    }
   ],
   "source": [
    "hasher = minhash.MinHasher(seeds=100, char_ngram=3, hashbytes=4)\n",
    "lsh_cache = cache.Cache(bands=10, hasher=hasher)\n",
    "\n",
    "for word_id, word in words.items():\n",
    "    lsh_cache.add_fingerprint(hasher.fingerprint(word), word_id)\n",
    "\n",
    "duplicate_pairs = set()\n",
    "\n",
    "total_bins = 0\n",
    "\n",
    "for cache_bin in lsh_cache.bins:\n",
    "    for bucket_id in cache_bin:\n",
    "        if len(cache_bin[bucket_id]) > 1:\n",
    "            pairs = set(itertools.combinations(cache_bin[bucket_id], r=2))\n",
    "            duplicate_pairs.update(pairs)\n",
    "        total_bins += 1\n",
    "\n",
    "duplicate_pairs = list(duplicate_pairs)\n",
    "total_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3903171"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.13 ms\n"
     ]
    }
   ],
   "source": [
    "total_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('unassumable', 'assumable'),\n",
       " ('ji', 'RL'),\n",
       " ('geosphere', 'cosphered'),\n",
       " ('Jr', 'FB'),\n",
       " ('reenlistness', 'reenlistnesses'),\n",
       " ('undispose', 'undisposed'),\n",
       " ('anticlassicism', 'anticlassicist'),\n",
       " ('Verrucaria', 'Verrucariaceae'),\n",
       " ('le', 'WH'),\n",
       " ('mischaracterize', 'mischaracterized')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.53 ms\n"
     ]
    }
   ],
   "source": [
    "[(words[i], words[j]) for i, j in duplicate_pairs[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 23.4 ms\n"
     ]
    }
   ],
   "source": [
    "swords = set(words.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.01 ms\n"
     ]
    }
   ],
   "source": [
    "'Python' in swords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.37 s\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(analyzer='char', ngram_range=(3,3), dtype=np.float32)\n",
    "tsvd = TruncatedSVD(n_components=50)\n",
    "\n",
    "dict_words_vecs = tfidf.fit_transform(words)\n",
    "dict_words_vecs = tsvd.fit_transform(dict_words_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.29 ms\n"
     ]
    }
   ],
   "source": [
    "w = 'I pulled out the words into a simple new-line-delimited text file. Which is more useful when building apps or importing into databases etc.'\n",
    "tokens = word_tokenize(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.84 ms\n"
     ]
    }
   ],
   "source": [
    "def load_text(doc_name='11758940.txt', path='/home/avsolatorio/WBG/NLP/WB/CORPUS/RAW/eap_files'):\n",
    "    fname = os.path.join(path, doc_name)\n",
    "    \n",
    "    with open(fname, 'rb') as fl:\n",
    "        text = fl.read()\n",
    "        text = u'' + text.decode('utf-8', errors='ignore')\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 11.5 s\n"
     ]
    }
   ],
   "source": [
    "text = load_text()\n",
    "tokens = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.71 s\n"
     ]
    }
   ],
   "source": [
    "spellchecker.set_tokens(tokens)\n",
    "errors = set()\n",
    "\n",
    "for err in spellchecker:\n",
    "    errors.add(err.word)\n",
    "    \n",
    "errors = list(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 49.8 ms\n"
     ]
    }
   ],
   "source": [
    "doc = tsvd.transform(tfidf.transform(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((466551, 50), (13654, 50))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.35 ms\n"
     ]
    }
   ],
   "source": [
    "dict_words_vecs.shape, doc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 11.1 s\n"
     ]
    }
   ],
   "source": [
    "sims = cosine_similarity(doc, dict_words_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 14 s\n"
     ]
    }
   ],
   "source": [
    "gg = list(zip([words[i] for i in sims.argmax(axis=1)], sims[:,sims.argmax(axis=1)].max(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Buu'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.29 ms\n"
     ]
    }
   ],
   "source": [
    "errors[60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, ('2', 0.0)),\n",
       " (1, ('Dilan', 0.9957872)),\n",
       " (2, ('2', 0.0)),\n",
       " (3, ('2', 0.0)),\n",
       " (4, ('Bai', 0.9142381)),\n",
       " (5, ('pocill', 0.99707234)),\n",
       " (6, ('2', 0.0)),\n",
       " (7, ('2', 0.0)),\n",
       " (8, ('2', 0.0)),\n",
       " (9, ('2', 0.0)),\n",
       " (10, ('2', 0.0)),\n",
       " (11, ('vill', 0.99930024)),\n",
       " (12, ('2', 0.0)),\n",
       " (13, ('2', 0.0)),\n",
       " (14, ('DAY', 0.9958306)),\n",
       " (15, ('2', 0.0)),\n",
       " (16, ('Y.M.C.A.', 0.9921591)),\n",
       " (17, ('2', 0.0)),\n",
       " (18, ('2', 0.0)),\n",
       " (19, ('2', 0.0)),\n",
       " (20, ('moggy', 0.95128155)),\n",
       " (21, ('2', 0.0)),\n",
       " (22, ('thiazides', 0.9641419)),\n",
       " (23, ('2', 0.0)),\n",
       " (24, ('trangam', 0.997847)),\n",
       " (25, ('2', 0.0)),\n",
       " (26, (\"l'tre\", 0.97923344)),\n",
       " (27, ('gung-ho', 0.84768647)),\n",
       " (28, ('kilhig', 0.7925369)),\n",
       " (29, ('servilities', 0.9794461)),\n",
       " (30, ('2', 0.0)),\n",
       " (31, ('Kim', 0.9884073)),\n",
       " (32, ('TRH', 0.9949274)),\n",
       " (33, ('2', 0.0)),\n",
       " (34, ('2', 0.0)),\n",
       " (35, ('2', 0.0)),\n",
       " (36, ('2', 0.0)),\n",
       " (37, ('2', 0.0)),\n",
       " (38, ('2', 0.0)),\n",
       " (39, ('2', 0.0)),\n",
       " (40, ('2', 0.0)),\n",
       " (41, ('2', 0.0)),\n",
       " (42, ('2', 0.0)),\n",
       " (43, ('workingwoman', 0.86992145)),\n",
       " (44, ('2', 0.0)),\n",
       " (45, ('Nekhbet', 0.89930433)),\n",
       " (46, ('PhL', 0.7384603)),\n",
       " (47, ('system', 0.99768025)),\n",
       " (48, ('2', 0.0)),\n",
       " (49, ('2', 0.0)),\n",
       " (50, ('treks', 0.99569416)),\n",
       " (51, ('2', 0.0)),\n",
       " (52, ('2', 0.0)),\n",
       " (53, (\"dhu'l-hijja\", 0.97120744)),\n",
       " (54, ('2', 0.0)),\n",
       " (55, ('2', 0.0)),\n",
       " (56, ('hmm', 0.9965555)),\n",
       " (57, ('2', 0.0)),\n",
       " (58, ('Iong', 0.9994543)),\n",
       " (59, ('VAN', 0.992546)),\n",
       " (60, ('incunabuulum', 0.9534646)),\n",
       " (61, ('2', 0.0)),\n",
       " (62, ('QED', 0.98335093)),\n",
       " (63, ('FFI', 0.9738074)),\n",
       " (64, ('DMS', 0.9859883)),\n",
       " (65, ('2', 0.0)),\n",
       " (66, ('2', 0.0)),\n",
       " (67, ('2', 0.0)),\n",
       " (68, ('Mila', 0.99591684)),\n",
       " (69, ('2', 0.0)),\n",
       " (70, ('2', 0.0)),\n",
       " (71, ('jekyll', 0.9245325)),\n",
       " (72, ('Wracs', 0.9966871)),\n",
       " (73, ('2', 0.0)),\n",
       " (74, ('2', 0.0)),\n",
       " (75, ('programmng', 0.9840575)),\n",
       " (76, ('Cong', 0.99838704)),\n",
       " (77, ('2', 0.0)),\n",
       " (78, ('2', 0.0)),\n",
       " (79, ('2', 0.0)),\n",
       " (80, ('DID', 0.99275184)),\n",
       " (81, ('subtotals', 0.9934531)),\n",
       " (82, ('2', 0.0)),\n",
       " (83, ('Tri', 0.9987598)),\n",
       " (84, ('2', 0.0)),\n",
       " (85, ('2', 0.0)),\n",
       " (86, ('2', 0.0)),\n",
       " (87, ('golliwogg', 0.9947452)),\n",
       " (88, ('LAO', 0.99517876)),\n",
       " (89, ('celiolymph', 0.8764231)),\n",
       " (90, ('D/O', 0.9944052)),\n",
       " (91, ('2', 0.0)),\n",
       " (92, ('SBLI', 0.98899156)),\n",
       " (93, ('Ria', 0.99829787)),\n",
       " (94, ('E.E.', 0.9926306)),\n",
       " (95, ('2', 0.0)),\n",
       " (96, ('2', 0.0)),\n",
       " (97, ('grinny', 0.9964406)),\n",
       " (98, ('DFMS', 0.9963941)),\n",
       " (99, ('2', 0.0))]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 7.44 ms\n"
     ]
    }
   ],
   "source": [
    "list(enumerate(gg[:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_data = pd.read_json('./data/words_dictionary.json', typ='ser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_data['philippines']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Ss s\".isalpha()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\n\\n\\n\\n\\n\\n<!DOCTYPE html>\\n<html lang=\"en\">\\n  <head>\\n    <meta charset=\"utf-8\">\\n  <link rel=\"dns-prefetch\" href=\"https://github.githubassets.com\">\\n  <link rel=\"dns-prefetch\" href=\"https://avatars0.githubusercontent.com\">\\n  <link rel=\"dns-prefetch\" href=\"https://avatars1.githubusercontent.com\">\\n  <link rel=\"dns-prefetch\" href=\"https://avatars2.githubusercontent.com\">\\n  <link rel=\"dns-prefetch\" href=\"https://avatars3.githubusercontent.com\">\\n  <link rel=\"dns-prefetch\" href=\"https://github-cloud.s3.amazonaws.com\">\\n  <link rel=\"dns-prefetch\" href=\"https://user-images.githubusercontent.com/\">\\n\\n\\n\\n  <link crossorigin=\"anonymous\" media=\"all\" integrity=\"sha512-3+HOqCwtQal5hOJQ+mdxiq5zmGOTjF6RhjDsPLxbKDYgGlLFeCwzoIanb7j5IiCuXKUqyC2q8FdkC4nmx2P2rA==\" rel=\"stylesheet\" href=\"https://github.githubassets.com/assets/frameworks-a2fba223d5af91496cac70d4ec3624df.css\" />\\n  <link crossorigin=\"anonymous\" media=\"all\" integrity=\"sha512-4ohd09bNnMlKWClfY22ZwyWNN3GJm6wo4ooCUoXfNecOKxjerWekbqlOj2amcXHSvPnhYhtDF15J5KgH'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}