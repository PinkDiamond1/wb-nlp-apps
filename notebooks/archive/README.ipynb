{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Semantic Similarity Project\n",
    "\n",
    "\n",
    "\n",
    "This project applies NLP techniques to build a semantic similarity engine that can identify similar documents from a corpus. Scripts for acquiring data, processing and cleaning the raw documents, generation of metadata, e.g., country and acronyms, training the models are available.\n",
    "\n",
    "The project is structured as follows:\n",
    "\n",
    "\n",
    "```\n",
    "ROOT\n",
    "├── APP\n",
    "│   ├── frontend\n",
    "│   ├── services\n",
    "│   └── views\n",
    "├── CORPUS\n",
    "│   ├── <CORPUS_ID>\n",
    "│   │   ├── PDF\n",
    "│   │   ├── TXT_CLEAN\n",
    "│   │   └── TXT_ORIG\n",
    "├── MODELS\n",
    "│   ├── LDA\n",
    "│   │   └── <CORPUS_ID>-<model_id>\n",
    "│   │       ├── data\n",
    "│   │       └── mallet\n",
    "│   └── WORD2VEC\n",
    "│       ├── <CORPUS_ID>-<model_id>\n",
    "│       │   └── data\n",
    "├── SCRIPTS\n",
    "│   ├── acronyms\n",
    "│   ├── cleaner\n",
    "│   ├── logs\n",
    "│   ├── models\n",
    "│   │   ├── lda\n",
    "│   │   ├── lsa\n",
    "│   │   ├── Mallet\n",
    "│   │   └── word2vec\n",
    "│   ├── ngrams\n",
    "│   ├── pdf2text\n",
    "│   ├── TIKA\n",
    "│   └── whitelists\n",
    "```\n",
    "\n",
    "\n",
    "## Corpus\n",
    "\n",
    "The `CORPUS` directory contains data acquired from different sources. Scripts to scrape or collect data from APIs are implemented for each corpus and can be found in the `SCRIPTS` directory. Text and pdf type documents can be collected as part of the corpus and must be stored in `CORPUS/<CORPUS_ID>/TXT_ORIG` and `CORPUS/<CORPUS_ID>/PDF`, respectively. \n",
    "\n",
    "The contents of the `CORPUS/<CORPUS_ID>/PDF` directory are converted into text by running scripts found in `SCRIPTS/pdf2text` (currently implemented for the IMF corpus). The converted texts are saved in the `CORPUS/<CORPUS_ID>/TXT_ORIG` directory. The same directory is used for storing raw data of type text from arbitrary corpus.\n",
    "\n",
    "## Preprocessing\n",
    "\n",
    "Scripts to augment originally obtained metadata from the corpus source are implemented. There are currently two scripts performing:\n",
    "\n",
    "- **Country detection** - detecting the presence of countries and regions in each document. This is implemented in the `SCRIPTS/country_detector.ipynb` notebook.\n",
    "    - The country detector depends on the `SCRIPTS/whitelists/whitelist_countries_multilingual.csv` file.\n",
    "    - The script generates a `CORPUS/<CORPUS_ID>/<corpus_id>_country_counts.csv` file.\n",
    "\n",
    "- **Acronyms detection** - detecting the acronyms in the documents. This is implemented in the `SCRIPTS/acronyms/acronym_detector.ipynb`.\n",
    "    - The script generates a `CORPUS/<CORPUS_ID>/<corpus_id>_detected_acronyms_prototypes.csv`.\n",
    "    - Used to build a whitelist of acronyms as basis for later online replacement of acronyms into full versions.\n",
    "\n",
    "## Cleaning\n",
    "\n",
    "Raw texts from a corpus are cleaned. The following sequence of steps are done for each document:\n",
    "\n",
    "- Load text from file (`CORPUS/<CORPUS_ID>/TXT_ORIG`)\n",
    "- Text with length less than `ignore_length` (default=50) are skipped.\n",
    "- Continuous whitespaces are normalized and treated as line breaks and replaced with a period (sentence stop).\n",
    "- Lemmatization of the text is done with an option to use SpaCy or NLTK (default=SpaCy).\n",
    "- A cleanup of the lemmatized text is performed.\n",
    "    - Removal of tokens with character length greater than or equal to 25.\n",
    "    - Removal of tokens having less than or equal to 2 characters.\n",
    "- Check text if it still has content after the initial normalization; if not, then skip.\n",
    "- Predict language of the text.\n",
    "- Check if the language is in the list of supported languages (default=en) and has a language probability greater than a threshold (default=0.98).\n",
    "- Perform spell check. The following tasks are executed:\n",
    "    - Use enchant to identify valid and invalid tokens.\n",
    "    - For invalid tokens, an inferencer for correction is executed to try to find a valid version if the token is misspelled.\n",
    "    - Apply plural to singular mapping using the whitelist file `SCRIPS/whitelists/whitelist_replacements_plurals_to_singular.csv`.\n",
    "    - Remove stopword lists.\n",
    "- Output:\n",
    "    - **lang**: Language detected and probability.\n",
    "    - **tokens**: Number of tokens in the document after cleaning.\n",
    "    - **text**: If no errors or exceptions, this will contain the cleaned text. If an error occurs in the process, the original text will be return.\n",
    "    - **spell_errors**: Tokens that are misspelled or not in the dictionary.\n",
    "    - **skipped**: Message for skipping the document.\n",
    "    - **exception**: Errors encountered.\n",
    "    - **write_status**: Flag indicating whether the returned text is clean or not. If `write_status` is `True`, then the text has been successfully cleaned.\n",
    "- Note: Only documents with `write_status == True` should be included in the `CORPUS/<CORPUS_ID>/<corpus_id>_metadata_complete.csv` file.\n",
    "\n",
    "Scripts for cleaning specific corpus are found in `SCRIPTS/cleaner`:\n",
    "- `wb_data_cleaner.ipynb`\n",
    "- `imf_data_cleaner.ipynb`\n",
    "\n",
    "## N-grams detection\n",
    "\n",
    "As part of preprocessing, an n-gram detector is used against the cleaned corpus. The detected ngrams will be stored in `CORPUS/<CORPUS_ID>/<corpus_id>_ngrams.csv` file. The ngram files for each corpus will be used to generate a master whitelist of ngrams. The master whitelist will be used during model training and online queries to dynamically convert individual tokens to their respective ngrams if present in the whitelist.\n",
    "\n",
    "## Models\n",
    "\n",
    "#### LDA\n",
    "\n",
    "Run `SCRIPTS/models/lda/lda_training.ipynb` and set the proper `CORPUS_ID` element of `WB` and `IMF`.\n",
    "\n",
    "#### Word2Vec\n",
    "\n",
    "Run `SCRIPTS/models/word2vec/word2vec_training.ipynb` and set the proper `CORPUS_ID` element of `WB` and `IMF`.\n",
    "\n",
    "#### LSA\n",
    "\n",
    "Run `SCRIPTS/models/lsa/lsa_training.ipynb` and set the proper `CORPUS_ID` element of `WB` and `IMF`.Run `SCRIPTS/models/lsa/lsa_training.ipynb` and set the proper `CORPUS_ID` element of `WB` and `IMF`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}