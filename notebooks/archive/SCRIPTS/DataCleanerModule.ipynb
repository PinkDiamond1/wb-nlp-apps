{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"text-align:center\">World Bank Documents and Reports Cleaner</div>\n",
    "\n",
    "This notebook implements the cleaner classes for the data from the **Documents and Reports API**. This cleaner module provides respelling functionality as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # Requirements:\n",
    "# # Please install spacy library and the `en` model\n",
    "# !~/anaconda3/bin/pip install spacy\n",
    "# !~/anaconda3/bin/python -m spacy download en\n",
    "# !~/anaconda3/bin/pip install contexttimer# # Requirements:\n",
    "# # Please install spacy library and the `en` model\n",
    "# !~/anaconda3/bin/pip install spacy\n",
    "# !~/anaconda3/bin/python -m spacy download en\n",
    "# !~/anaconda3/bin/pip install contexttimer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing pattern as alternative to pyenchant\n",
    "#### Note, the pattern module's spell checking function is quite slow!\n",
    "\n",
    "\n",
    "Clone first the development repo ([pypi version is outdated](https://github.com/clips/pattern/issues/217\n",
    "))\n",
    "- `git clone -b development https://github.com/clips/pattern`\n",
    "- `cd pattern/`\n",
    "- Commenting out `\"mysqlclient\"` inside the `setup.py` file may be necessary if errors are encountered in the next step.\n",
    "- `pip install .`\n",
    "\n",
    "Make sure that the `pip` that you use corresponds to the python installation that you will use to run the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "# from acronyms.AcronymModule import AcronymMapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "nltk.data.path.append(\"/R/nltk_data\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "acronyms_pattern = re.compile('(\\([A-Z]{2,}\\))')\n",
    "candidates_pattern = re.compile('([A-Z][a-z]+|\\([A-Z]{2,}\\))')\n",
    "newline_pattern = re.compile(b'[\\r\\n]+')\n",
    "whitespaces_pattern = re.compile('\\s+')\n",
    "alphabet_pattern = re.compile('[A-Z-a-z]+')\n",
    "stops = set(['the', 'of', 'in', 'and', 'or', 'for'])\n",
    "stops.update(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def extract_acronyms(txt):\n",
    "    acronyms = [i.strip('(').strip(')') for i in acronyms_pattern.findall(txt)]\n",
    "    keyword = '|'.join([f'\\({k}\\)' for k in acronyms])\n",
    "    candidates_acronym_pattern = \"((?:[a-zA-Z'-]+ ){0,5})(\" + keyword + \")\" #((?:[^a-zA-Z'-]+[a-zA-Z'-]+){0,5})\"\n",
    "    candidates_acronym_pattern = re.compile(candidates_acronym_pattern)\n",
    "    acronym_candidates_lists = candidates_acronym_pattern.findall(whitespaces_pattern.sub(' ', txt))\n",
    "    detected_acronyms = {}\n",
    "\n",
    "    for ip in acronym_candidates_lists:\n",
    "        candidate, acronym = ip\n",
    "        candidate = ' '.join(alphabet_pattern.findall(candidate))\n",
    "\n",
    "        full_name = []\n",
    "        formed_acronym = []\n",
    "        acr = acronym.strip('(').strip(')')\n",
    "        l = 0\n",
    "        acr_char_counts = Counter(acr)\n",
    "        for ix, c in enumerate(candidate.strip().split()[::-1]):\n",
    "            c_title = c.title()\n",
    "            if c in stops and l > 0:\n",
    "                full_name.append(c)\n",
    "            elif (c_title[0] in acr_char_counts) and (acr_char_counts[c_title[0]] > 0):\n",
    "                l += 1\n",
    "                full_name.append(c_title)\n",
    "                formed_acronym.append(c_title)\n",
    "                acr_char_counts[c_title[0]] -= 1\n",
    "            if l >= len(acr):\n",
    "                detected_acronyms[acr] = \" \".join(full_name[::-1])\n",
    "                break\n",
    "\n",
    "    return detected_acronyms\n",
    "\n",
    "\n",
    "def detect_acronyms(txt):\n",
    "    '''\n",
    "    This method extracts acronyms from a text document. An acronym can be detected if it's defined with similar form as follows:\n",
    "        National Population and Housing Census (NPHC)\n",
    "        Landscape Approach to Forest Restoration and Conservation (LAFREC)\n",
    "\n",
    "    Input:\n",
    "        text (str): string type object where acronyms will be detected from.\n",
    "\n",
    "    Output:\n",
    "        acronyms_map (dict): this is a dictionary that maps the acronym to a set of possible original forms of the acronym.\n",
    "\n",
    "    '''\n",
    "\n",
    "#     acronyms = acronyms_pattern.findall(txt)\n",
    "#     candidates = candidates_pattern.findall(txt)\n",
    "\n",
    "#     detected_acronyms = []\n",
    "\n",
    "#     for a in acronyms:\n",
    "#         if a in candidates:\n",
    "#             ix = candidates.index(a)\n",
    "#             l = len(a) - 2\n",
    "#             detected_acronyms.append((a[1:-1], ' '.join(candidates[ix - l:ix])))\n",
    "\n",
    "    detected_acronyms = extract_acronyms(txt).items()\n",
    "    acronyms_map = {}\n",
    "\n",
    "    for a, n in detected_acronyms:\n",
    "        if a in acronyms_map:\n",
    "            acronyms_map[a].add(n)\n",
    "        else:\n",
    "            acronyms_map[a] = set([n])\n",
    "\n",
    "    return acronyms_map\n",
    "\n",
    "\n",
    "def detect_acronyms_from_file(fpath):\n",
    "    try:\n",
    "        with open(fpath, 'rb') as fl:\n",
    "            txt = fl.read()\n",
    "            txt = newline_pattern.sub(b' ', txt)\n",
    "            txt = txt.decode('utf-8', errors='ignore')\n",
    "\n",
    "    except UnicodeDecodeError:\n",
    "        with open(fpath, 'rb') as fl:\n",
    "            txt = fl.read()\n",
    "            txt = newline_pattern.sub(b' ', txt)\n",
    "            txt = txt.decode('utf-8', errors='ignore')\n",
    "\n",
    "    return detect_acronyms(txt)\n",
    "\n",
    "\n",
    "def merge_corpora_acronyms_map(acronyms_maps):\n",
    "    merged_corpora_acronyms_map = {}\n",
    "\n",
    "    for acronyms_map in acronyms_maps:\n",
    "        if acronyms_map:\n",
    "            for a, payload in acronyms_map.items():\n",
    "                doc_freq = payload['doc_freq']\n",
    "                prototypes = payload['prototypes']\n",
    "\n",
    "                if a in merged_corpora_acronyms_map:\n",
    "                    for i in prototypes:\n",
    "                        if i in merged_corpora_acronyms_map[a]['prototypes']:\n",
    "                            merged_corpora_acronyms_map[a]['prototypes'] += prototypes[i]\n",
    "                        else:\n",
    "                            merged_corpora_acronyms_map[a]['prototypes'] = prototypes[i]\n",
    "\n",
    "                    merged_corpora_acronyms_map[a]['doc_freq'] += doc_freq\n",
    "                else:\n",
    "                    merged_corpora_acronyms_map[a] = {'prototypes': prototypes, 'doc_freq': doc_freq}\n",
    "\n",
    "    return merged_corpora_acronyms_map\n",
    "\n",
    "\n",
    "def get_corpus_top_acronym_prototypes(corpus_full_acronyms_map, prototypes=5):\n",
    "    acronyms_popular_prototype = []\n",
    "    columns=['acronym', 'doc_freq', 'full_name', 'percentage']\n",
    "\n",
    "#     for i in range(1, prototypes + 1):\n",
    "#         columns.append(f'popular_prototype_{i}')\n",
    "#         columns.append(f'doc_proportion_{i}')\n",
    "\n",
    "    for a, d in corpus_full_acronyms_map.items():\n",
    "        x = pd.Series(corpus_full_acronyms_map[a]['prototypes'])\n",
    "        y = (x / corpus_full_acronyms_map[a]['doc_freq']).sort_values(ascending=False)\n",
    "        doc_freq = corpus_full_acronyms_map[a]['doc_freq']\n",
    "\n",
    "        for ind in range(prototypes):\n",
    "            d = [a, doc_freq]\n",
    "            try:\n",
    "                i = y.index[ind]\n",
    "                v = y[i]\n",
    "            except:\n",
    "                break\n",
    "            d.append(i)\n",
    "            d.append(v)\n",
    "\n",
    "            acronyms_popular_prototype.append(d)\n",
    "\n",
    "    acronyms_popular_prototype = pd.DataFrame(\n",
    "        acronyms_popular_prototype,\n",
    "        columns=columns\n",
    "    )\n",
    "\n",
    "    acronyms_popular_prototype = acronyms_popular_prototype.sort_values('doc_freq', ascending=False).reset_index(drop='index')\n",
    "\n",
    "    return acronyms_popular_prototype\n",
    "\n",
    "\n",
    "class AcronymMapper:\n",
    "    def __init__(self, whitelist_file, sim_thresh=0.8):\n",
    "        whitelist_acronyms = pd.read_csv(whitelist_file, header=None)\n",
    "        self.whitelist_acronyms = whitelist_acronyms.rename(columns={0: 'acronym', 1: 'actual'})\n",
    "\n",
    "        self.hvec = HashingVectorizer()\n",
    "\n",
    "        # Can be in a dataframe but I don't want to stack the vectors every time we infer.\n",
    "        self.acronyms = self.whitelist_acronyms.acronym.values\n",
    "        self.actual = self.whitelist_acronyms.actual.values\n",
    "        self.actual_vectors = self.hvec.transform(self.actual)\n",
    "        self.sim_thresh = sim_thresh\n",
    "\n",
    "    def get_valid_doc_acronym(self, txt):\n",
    "\n",
    "        # Detect acronyms present in the document\n",
    "        doc_detected_acronyms = detect_acronyms(txt)\n",
    "        valid_doc_acronyms = {}\n",
    "        invalid_in_doc_to_actual = {}\n",
    "\n",
    "        for i in doc_detected_acronyms:\n",
    "            c = self.whitelist_acronyms[self.whitelist_acronyms.acronym == i]\n",
    "\n",
    "            if not c.empty:\n",
    "                # For now, this assumes that there will only be one detected full name for an acronym.\n",
    "                valid_candidate_acronyms = doc_detected_acronyms[i]\n",
    "                assert(len(valid_candidate_acronyms) == 1)\n",
    "\n",
    "                valid_candidate_acronyms_vec = self.hvec.transform(valid_candidate_acronyms)\n",
    "\n",
    "                sims = cosine_similarity(self.actual_vectors, valid_candidate_acronyms_vec)\n",
    "                max_index = sims.argmax()\n",
    "                max_sim = sims[max_index]\n",
    "\n",
    "                if max_sim > self.sim_thresh:\n",
    "                    valid_full = self.actual[max_index]\n",
    "                    valid_doc_acronyms[i] = valid_full\n",
    "\n",
    "                    doc_full = list(valid_candidate_acronyms)[0]\n",
    "\n",
    "                    if doc_full != valid_full:\n",
    "                        invalid_in_doc_to_actual[doc_full] = valid_full\n",
    "\n",
    "        return valid_doc_acronyms, invalid_in_doc_to_actual\n",
    "\n",
    "    def expand_doc_acronyms(self, txt):\n",
    "        valid_doc_acronyms, invalid_in_doc_to_actual = self.get_valid_doc_acronym(txt)\n",
    "\n",
    "        for acr in valid_doc_acronyms:\n",
    "            txt = txt.replace(f' ({acr})', ' ')\n",
    "            txt = txt.replace(f' {acr} ', f' {valid_doc_acronyms[acr]} ')\n",
    "\n",
    "        for invalid_full in invalid_in_doc_to_actual:\n",
    "            txt = txt.replace(invalid_full, invalid_in_doc_to_actual[invalid_full])\n",
    "\n",
    "        return txt\n",
    "\n",
    "    def expand_doc_acronyms_in_file(self, fname):\n",
    "        with open(fname) as fl:\n",
    "            txt = fl.read()\n",
    "\n",
    "        return self.expand_doc_acronyms(txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "nltk.data.path.append(\"/R/nltk_data\")\n",
    "\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from nltk.corpus import PlaintextCorpusReader, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "from langdetect import detect, detect_langs\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing as mp\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "# Spelling correction\n",
    "ENCHANT_INSTALLED = True\n",
    "try:\n",
    "    from enchant.checker import SpellChecker\n",
    "    from enchant import Dict\n",
    "except:\n",
    "    # Make sure that these are installed for the pattern module to work\n",
    "    for token in (\"stopwords\", \"wordnet\", \"wordnet_ic\", \"sentiwordnet\"):\n",
    "        nltk.download(token)\n",
    "    print('Using pattern module...')\n",
    "    import pattern.en\n",
    "    ENCHANT_INSTALLED = False\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.metrics.distance import edit_distance\n",
    "\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "import spacy\n",
    "\n",
    "from contexttimer import Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPACY_DATA_DIR = 'en_core_web_sm'  # '/R/spacy_data/en_core_web_sm/en_core_web_sm-2.0.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from sklearn.feature_extraction import stop_words\n",
    "\n",
    "roman_nums = set([\n",
    "    'i', 'ii', 'iii', 'iv', 'v', 'vi', 'vii', 'viii', 'ix', 'x', 'xi',\n",
    "    'xii', 'xiii', 'xiv', 'xv', 'xvi', 'xvii', 'xviii', 'xix', 'xx',\n",
    "    'xxi', 'xxii', 'xxiii', 'xxiv', 'xxv', 'xxvi', 'xxvii', 'xxviii',\n",
    "    'xxix', 'xxx', 'xxxi', 'xxxii', 'xxxiii', 'xxxiv', 'xxxv', 'xxxvi',\n",
    "    'xxxvii', 'xxxviii', 'xxxix', 'xl', 'xli', 'xlii', 'xliii', 'xliv',\n",
    "    'xlv', 'xlvi', 'xlvii', 'xlviii', 'xlix', 'l', 'li', 'lii', 'liii',\n",
    "    'liv', 'lv', 'lvi', 'lvii', 'lviii', 'lix', 'lx', 'lxi', 'lxii',\n",
    "    'lxiii', 'lxiv', 'lxv', 'lxvi', 'lxvii', 'lxviii', 'lxix', 'lxx',\n",
    "    'lxxi', 'lxxii', 'lxxiii', 'lxxiv', 'lxxv', 'lxxvi', 'lxxvii',\n",
    "    'lxxviii', 'lxxix', 'lxstopwordsxx', 'lxxxi', 'lxxxii', 'lxxxiii', 'lxxxiv',\n",
    "    'lxxxv', 'lxxxvi', 'lxxxvii', 'lxxxviii', 'lxxxix', 'xc', 'xci',\n",
    "    'xcii', 'xciii', 'xciv', 'xcv', 'xcvi', 'xcvii', 'xcviii', 'xcix', 'c'\n",
    "])\n",
    "try:\n",
    "    stopwords = set(nltk_stopwords.words('english'))\n",
    "except:\n",
    "    stopwords = set()\n",
    "    print('Warning: NLTK stopwords not used! Please check if the nltk stopwords corpus is avaialable in your system.')\n",
    "stopwords.update(stop_words.ENGLISH_STOP_WORDS)\n",
    "stopwords.update(roman_nums)\n",
    "\n",
    "stopwords = list(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spelling correction module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "respell_error = []\n",
    "\n",
    "class Respeller:\n",
    "\n",
    "    en_dict = Dict('en_US') if ENCHANT_INSTALLED else pattern.en\n",
    "    WORKERS = os.cpu_count() - 1\n",
    "\n",
    "    def __init__(self, dictionary_file=None, spell_threshold=0.3, spell_cache=None):\n",
    "        self.spell_cache = spell_cache if spell_cache is not None else {}  # pd.Series()\n",
    "        self.dictionary_file = dictionary_file\n",
    "        self.spell_threshold = spell_threshold\n",
    "        self.stopwords = stopwords\n",
    "        \n",
    "        if (self.dictionary_file is not None) and os.path.isfile(self.dictionary_file):\n",
    "                self.spell_cache = pd.read_csv(self.dictionary_file)\n",
    "\n",
    "    def save_spell_cache(self):\n",
    "        pd.Series(self.spell_cache).to_csv(self.dictionary_file)\n",
    "\n",
    "    def morph_word(self, word):\n",
    "        # word = word.replace(' ', '')  # Check if compound word suggestion matches the misspelled word\n",
    "        m_word = word + ''.join(sorted(word)) # Perform this opperation to add more robustness to the matching\n",
    "\n",
    "        return m_word\n",
    "\n",
    "    def infer_correct_word(self, word, sim_thresh=0.0, print_log=False, min_len=3, use_suggest_score=True):\n",
    "        if word not in self.spell_cache:\n",
    "            correct_word = None\n",
    "            score = -1\n",
    "\n",
    "            payload = dict(word=word,correct_word=correct_word, score=score)\n",
    "            \n",
    "            self.spell_cache[word] = payload\n",
    "\n",
    "            if len(word) <= min_len:\n",
    "                return self.spell_cache[word]\n",
    "\n",
    "            candidates = self.en_dict.suggest(word)\n",
    "            if not ENCHANT_INSTALLED:\n",
    "                # Do this since pattern returns a tuple of (word, score)\n",
    "                candidates = [w for w, sim in candidates if sim > 0.1]\n",
    "\n",
    "            if use_suggest_score:\n",
    "                suggest_score = 1 / rankdata(range(len(candidates)))**0.5\n",
    "            else:\n",
    "                suggest_score = np.ones(len(candidates))\n",
    "            \n",
    "            if candidates:\n",
    "                try:\n",
    "                    m_word = self.morph_word(word)\n",
    "                    m_candidates = [self.morph_word(c.lower()) for c in candidates]\n",
    "\n",
    "                    tfidf = TfidfVectorizer(analyzer='char', ngram_range=(2, 4))\n",
    "                    candX = tfidf.fit_transform(m_candidates)\n",
    "                    wordX = tfidf.transform([m_word])\n",
    "\n",
    "                    r = 1.0 / rankdata([edit_distance(m_word, x) for x in m_candidates])\n",
    "\n",
    "                    sim = cosine_similarity(candX, wordX)\n",
    "                    sim_r = sim * r.reshape(-1, 1) * suggest_score.reshape(-1, 1)\n",
    "\n",
    "                    sim_ind = sim_r.argmax()\n",
    "                    score = sim_r[sim_ind]\n",
    "                    if score > sim_thresh:\n",
    "                        correct_word = candidates[sim_ind]\n",
    "                except Exception as e:\n",
    "                    # raise ValueError(word)\n",
    "                    print(f\"Error word: {word}\")\n",
    "\n",
    "            if print_log:\n",
    "                print(sim_r)\n",
    "                print(r)\n",
    "                print(word)\n",
    "                print(candidates)\n",
    "                print(candidates[sim_ind])\n",
    "\n",
    "            payload['correct_word'] = correct_word\n",
    "            payload['score'] = float(score)\n",
    "            \n",
    "            self.spell_cache[word] = payload\n",
    "\n",
    "        return self.spell_cache[word]\n",
    "    \n",
    "    def qualified_word(self, word):\n",
    "        stopwords = set(self.stopwords)\n",
    "        is_valid = (\n",
    "            (word not in stopwords) and\n",
    "            (not word[0].isupper()) and\n",
    "            len(word) > 2\n",
    "        )\n",
    "        \n",
    "        return is_valid\n",
    "    \n",
    "    def parallel_infer_correct_word(self, words, num_workers):\n",
    "        respelled_set = {}\n",
    "        \n",
    "        respell_results = [self.infer_correct_word(ew) for ew in words]\n",
    "\n",
    "        words = set([])\n",
    "\n",
    "        for res in respell_results:\n",
    "            word = res['word']\n",
    "            correct_word = res['correct_word']\n",
    "            score = res['score']\n",
    "\n",
    "            if correct_word and score > self.spell_threshold:\n",
    "                if correct_word.istitle():\n",
    "                    # If the respelling results to a `Title` word\n",
    "                    # it implies that the word is a proper noun, therefore, omit.\n",
    "                    words.add(word)\n",
    "                else:\n",
    "                    # Split and filter since some words are compound terms.\n",
    "                    respelled_set[word] = [i for i in correct_word.split() if self.qualified_word(i)]\n",
    "            else:\n",
    "                words.add(word)\n",
    "\n",
    "        return words, respelled_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimized cleaner with internal parallelization support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prevent this error in spacy\n",
    "# ---------------------------------------------------\n",
    "# Text of length 1213412 exceeds maximum of 1000000.\n",
    "# The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input.\n",
    "# This means long texts may cause memory allocation errors.\n",
    "# If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit.\n",
    "# The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.\"\n",
    "# ---------------------------------------------------\n",
    "SPACY_MAX_LENGTH = 10 * 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enchant.checker import SpellChecker\n",
    "\n",
    "\n",
    "class Cleaner:\n",
    "\n",
    "    input_folder = ''\n",
    "    output_folder = ''\n",
    "    custom_stopwords = []\n",
    "    spellchecker = None\n",
    "    respeller = None\n",
    "    acronym_mapper = None\n",
    "    lemma = None\n",
    "    space_normalize_text_pattern = re.compile('[.\\s]{2,}')\n",
    "    noise_normalize_text_pattern = re.compile('[^a-zA-Z\\'\\.\\?\\!\\s]+')\n",
    "    short_valid_tokens_pattern = re.compile('[a-zA-Z]{3,}')\n",
    "    long_invalid_tokens_pattern = re.compile('\\w{25,}')\n",
    "\n",
    "    # spell_cache_manager = multiprocessing.Manager()\n",
    "    spell_cache_dict = {}  # spell_cache_manager.dict()\n",
    "    \n",
    "    def __init__(\n",
    "        self, use_spellchecker=False, use_respeller=False,\n",
    "        use_lemmatizer=False, num_workers=None,\n",
    "        ignore_length=50, use_spacy=True,\n",
    "        replacements_plurals_to_singular_file=None,\n",
    "        acronyms_file=None,\n",
    "        min_en_lang_prob=0.98,\n",
    "        supported_lang=('en',),\n",
    "        logger=None,\n",
    "        check_language=True,\n",
    "        extract_only_pos=None,\n",
    "    ):\n",
    "        self.data=[]\n",
    "        self.check_language = check_language\n",
    "        self.use_spellchecker = use_spellchecker\n",
    "        self.use_lemmatizer = use_lemmatizer\n",
    "        self.use_respeller = use_respeller\n",
    "        self.num_workers = max(1, os.cpu_count() - 4) if num_workers is None else num_workers\n",
    "        self.patterns = []\n",
    "        self.lemma_cache = {}\n",
    "        self.respelled_set = {}\n",
    "        self.use_spacy_lemmatizer = use_spacy\n",
    "        self.ignore_length = ignore_length\n",
    "        self.ENCHANT_INSTALLED = ENCHANT_INSTALLED\n",
    "        self.replacements_plurals_to_singular_file = replacements_plurals_to_singular_file\n",
    "        self.acronyms_file = acronyms_file\n",
    "        self.min_en_lang_prob = min_en_lang_prob\n",
    "        self.supported_lang = supported_lang\n",
    "        \n",
    "        # This parameter can take a tuple of SpaCy Part-of-Speech codes or None\n",
    "        # This is ignored if SpaCy is not used.\n",
    "        self.extract_only_pos = extract_only_pos\n",
    "        \n",
    "        if logger:\n",
    "            self.logger = logger.error\n",
    "        else:\n",
    "            self.logger = print\n",
    "        \n",
    "        self.plural_singular_map = {}\n",
    "        \n",
    "        if self.replacements_plurals_to_singular_file is not None:\n",
    "            self.build_plurals_to_singular_map()\n",
    "\n",
    "        if self.use_spellchecker:\n",
    "            self.spellchecker = SpellChecker(\"en_US\") if ENCHANT_INSTALLED else pattern.en\n",
    "        \n",
    "        if self.use_lemmatizer:\n",
    "            self.lmtzr_spacy = None  # spacy.load('en')\n",
    "            self.lmtzr_wordnet = None if self.use_spacy_lemmatizer else WordNetLemmatizer()\n",
    "\n",
    "        if self.use_respeller:\n",
    "            self.respeller = Respeller(spell_threshold=0.7, spell_cache=self.spell_cache_dict)\n",
    "            \n",
    "        if self.acronyms_file is not None:\n",
    "            self.acronym_mapper = AcronymMapper(whitelist_file=self.acronyms_file, sim_thresh=0.8)\n",
    "            \n",
    "        self.stopwords = stopwords\n",
    "        \n",
    "        # initialize clean_text\n",
    "        self.clean_text('initialize cleaner')\n",
    "\n",
    "    def build_plurals_to_singular_map(self):\n",
    "        '''\n",
    "        Assume that the whitelist is a two column excel file without a header: first col - plural, second col - singular.\n",
    "        Don't catch exception such that any errors will be apparent.\n",
    "        '''\n",
    "        plural_singular_map = pd.read_csv(self.replacements_plurals_to_singular_file, header=None, index_col=0).dropna()[1]\n",
    "        self.plural_singular_map = dict(plural_singular_map)\n",
    "\n",
    "    def set_input_folder(self,input_folder):\n",
    "        self.input_folder = input_folder\n",
    "\n",
    "    def set_output_folder(self,output_folder):\n",
    "        self.output_folder = output_folder\n",
    "        if not os.path.isdir(self.output_folder):\n",
    "            os.makedirs(self.output_folder)\n",
    "\n",
    "    def set_custom_stopwords(self, stopwords):\n",
    "        self.custom_stopwords = stopwords\n",
    "\n",
    "    # remove noise words\n",
    "    def remove_noise(self, text):\n",
    "        text = text.lower()\n",
    "        text = self.long_invalid_tokens_pattern.sub('', text)\n",
    "        text = ' '.join(self.short_valid_tokens_pattern.findall(text))\n",
    "\n",
    "        return text\n",
    "    \n",
    "    def get_lemma(self, word, word_pos):\n",
    "        stopwords = set(self.stopwords)\n",
    "        if word in stopwords:\n",
    "            return None\n",
    "        \n",
    "        key = (word, word_pos)\n",
    "        \n",
    "        if key not in self.lemma_cache:\n",
    "            lemma = self.lmtzr_wordnet.lemmatize(word, word_pos)\n",
    "            self.lemma_cache[key] = lemma\n",
    "            \n",
    "        return self.lemma_cache[key]\n",
    "\n",
    "    def lemmatize_text_wordnet(self, text):\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "\n",
    "        txt_out = ''\n",
    "\n",
    "        # Before lemmatizing, we tag words (part-of-speech tagging)\n",
    "        tagged_tokens = pos_tag(tokens)    \n",
    "\n",
    "        # We now lemmatize based on a simplified list of POS tags\n",
    "        for tagged_token in tagged_tokens:\n",
    "            word = tagged_token[0]\n",
    "            word_pos = tagged_token[1]\n",
    "\n",
    "            # We recode NLTK tagging for consistency with wordnet \n",
    "            if tagged_token[1].startswith('J'):\n",
    "                word_pos = wordnet.ADJ\n",
    "            elif tagged_token[1].startswith('V'):\n",
    "                word_pos = wordnet.VERB\n",
    "            elif tagged_token[1].startswith('N'):\n",
    "                word_pos = wordnet.NOUN\n",
    "            elif tagged_token[1].startswith('R'):\n",
    "                word_pos = wordnet.ADV\n",
    "            else:\n",
    "                word_pos = wordnet.NOUN # Assume noun if other  \n",
    "            \n",
    "            # We now lemmatize, taking the POS tag into account\n",
    "            lemma = self.get_lemma(word, word_pos)\n",
    "            \n",
    "            if lemma is not None:\n",
    "                txt_out = txt_out + lemma + ' '\n",
    "\n",
    "        return txt_out  \n",
    "\n",
    "    def lemmatize_text_spacy(self, text):\n",
    "        stopwords = set(self.stopwords)\n",
    "        try:\n",
    "            lmtzr_spacy = spacy.load('en_core_web_sm', disable=['parser', 'ner', 'textcat'], max_length=SPACY_MAX_LENGTH)  # spacy.load('en', disable=['parser', 'ner', 'textcat'])\n",
    "        except OSError:\n",
    "            lmtzr_spacy = spacy.load(SPACY_DATA_DIR, disable=['parser', 'ner', 'textcat'], max_length=SPACY_MAX_LENGTH)\n",
    "\n",
    "        doc = lmtzr_spacy(text.lower())\n",
    "            # ' '.join(re.findall('[a-z0-9]+', text.lower())))\n",
    "        \n",
    "        txt_out = ''\n",
    "        \n",
    "        for token in doc:\n",
    "            if token.lemma_ in stopwords:\n",
    "                continue\n",
    "                \n",
    "            if (self.extract_only_pos is not None) and (token.pos_ not in self.extract_only_pos):\n",
    "                continue\n",
    "                \n",
    "            txt_out = txt_out + token.lemma_ + ' '\n",
    "            \n",
    "        txt_out = txt_out.replace('-PRON-', '')\n",
    "\n",
    "        return txt_out.strip()\n",
    "    \n",
    "    def space_normalize_text(self, text, lower=False):\n",
    "        if lower:\n",
    "            text = text.lower()\n",
    "        text = self.space_normalize_text_pattern.sub(' . ', text)\n",
    "        text = self.noise_normalize_text_pattern.sub(' ', text)\n",
    "        return text\n",
    "\n",
    "    def lemmatize_text(self, text, lower=False):\n",
    "        # Perform preliminary removal of noise\n",
    "        text = self.space_normalize_text(text, lower=lower)\n",
    "\n",
    "        txt_out = ''\n",
    "        if self.use_spacy_lemmatizer:\n",
    "            txt_out = self.lemmatize_text_spacy(text)\n",
    "        else:\n",
    "            txt_out = self.lemmatize_text_wordnet(text)\n",
    "            \n",
    "        return txt_out\n",
    "    \n",
    "    def get_misspelled_tokens(self, text):\n",
    "        if self.spellchecker is None:\n",
    "            raise ValueError('Spellchecker is not enabled')\n",
    "        \n",
    "        errors = set([])\n",
    "\n",
    "        if ENCHANT_INSTALLED:\n",
    "            # Input is a text\n",
    "            self.spellchecker.set_text(text)\n",
    "\n",
    "            for err in self.spellchecker:\n",
    "                #print (err.word)\n",
    "                if err.word not in self.respelled_set:\n",
    "                    errors.add(err.word)\n",
    "        else:\n",
    "            # Input is a list of tokens\n",
    "            text_tokens = set(text)\n",
    "            for token in text_tokens:\n",
    "                suggestions = self.spellchecker.suggest(token)\n",
    "                \n",
    "                # If suggestions are available, make sure that the first\n",
    "                # suggestion is similar to the token to make sure\n",
    "                # that the token being testing is a legit word.\n",
    "                if suggestions and (suggestions[0][0] == token and suggestions[0][1] > 0.9):\n",
    "                    continue\n",
    "                else:\n",
    "                    errors.add(token)\n",
    "        return errors\n",
    "\n",
    "    # Run spell checker on text to keep words found in dictionary only\n",
    "    def spellcheck_text(self, text):\n",
    "        text_tokens=word_tokenize(text)\n",
    "        errors = self.get_misspelled_tokens(text if ENCHANT_INSTALLED else text_tokens)\n",
    "\n",
    "        if errors and self.respeller:\n",
    "            errors, respelled_set = self.respeller.parallel_infer_correct_word(errors, self.num_workers * 2)  # max((self.num_workers // 2), 1))\n",
    "            # print(respelled_set)\n",
    "            self.respelled_set.update(respelled_set)\n",
    "\n",
    "        errors_set=set(errors)\n",
    "        cleaned_text = []\n",
    "        \n",
    "        for x in text_tokens:\n",
    "            if (x in errors_set):\n",
    "                continue\n",
    "            \n",
    "            elif x in self.respelled_set:\n",
    "                for x in self.respelled_set[x]:\n",
    "                    x = self.plural_singular_map.get(x, x)\n",
    "                    cleaned_text.append(x)\n",
    "\n",
    "            elif (x in self.stopwords):\n",
    "                continue\n",
    "\n",
    "            else:\n",
    "                x = self.plural_singular_map.get(x, x)\n",
    "                cleaned_text.append(x)\n",
    "\n",
    "        output={}\n",
    "        output['text']=\" \".join(cleaned_text)\n",
    "        output['errors']=errors\n",
    "   \n",
    "        return output\n",
    "\n",
    "    def load_existing_and_extract_metadata(self, fileid, filepath, save_docs, process_output_dict=None):\n",
    "        proc_fileid = fileid\n",
    "        \n",
    "        filename = filepath.split('/')[-1]\n",
    "        fileid = filename.strip('.txt')\n",
    "        \n",
    "        with open(filepath) as fl:\n",
    "            text = fl.read()\n",
    "        \n",
    "        lang_log = ('ERROR', 0)\n",
    "        token_log = 0\n",
    "        skipped_log = ''\n",
    "        text_log = ''\n",
    "        spell_errors = []\n",
    "        exp = None\n",
    "        write_status = True\n",
    "\n",
    "        predict_lang = detect_langs(text)[0]\n",
    "        lang_log = (predict_lang.lang, predict_lang.prob)\n",
    "        # Log tokens count\n",
    "        token_log = len(word_tokenize(text))\n",
    "        text_log = text\n",
    "\n",
    "        cleaning_output = dict(\n",
    "            lang=lang_log,\n",
    "            token=token_log,\n",
    "            text=text_log,\n",
    "            skipped=skipped_log,\n",
    "            spell_errors=spell_errors,\n",
    "            exception=exp,\n",
    "            write_status=write_status,\n",
    "        )\n",
    "\n",
    "        # log statistics\n",
    "        lang_log = {}  # lang info per document - uses the format - lang_log[fileid]={'score','lang'}\n",
    "        spell_errors = {}\n",
    "        token_log = {}  # Tokens count per document\n",
    "        text_errors = {}\n",
    "        text_log = {}\n",
    "        skipped_log = {}\n",
    "        exception_log = {}\n",
    "        write_status_log = {}\n",
    "\n",
    "        lang_log[fileid] = cleaning_output['lang']\n",
    "        token_log[fileid] = cleaning_output['token']\n",
    "        skipped_log[fileid] = cleaning_output['skipped']\n",
    "        text_log[fileid] = cleaning_output['text']\n",
    "        spell_errors[fileid] = cleaning_output['spell_errors']\n",
    "        exception_log[fileid] = cleaning_output['exception']\n",
    "        write_status_log[fileid] = cleaning_output['write_status']\n",
    "        \n",
    "        # return logs\n",
    "        output_log = {}\n",
    "        output_log['lang'] = lang_log\n",
    "        output_log['tokens'] = token_log\n",
    "        output_log['text'] = text_log\n",
    "        output_log['spell_errors'] = spell_errors\n",
    "        output_log['skipped'] = skipped_log\n",
    "        output_log['exception'] = exception_log\n",
    "        output_log['write_status'] = write_status_log\n",
    "        \n",
    "        if process_output_dict is not None:\n",
    "            process_output_dict[proc_fileid] = output_log\n",
    "        else: \n",
    "            return output_log\n",
    "\n",
    "    def clean_text(self, text, filen=None):\n",
    "        lang_log = ('ERROR', 0)\n",
    "        token_log = 0\n",
    "        skipped_log = ''\n",
    "        text_log = ''\n",
    "        spell_errors = []\n",
    "        exp = None\n",
    "        write_status = False\n",
    "        \n",
    "        if self.acronym_mapper is not None:\n",
    "            text = self.acronym_mapper.expand_doc_acronyms(text)\n",
    "\n",
    "        len_text = len(text)\n",
    "        \n",
    "        if len_text > self.ignore_length:\n",
    "            \n",
    "            if self.use_lemmatizer:\n",
    "                # Apply lemmatizer\n",
    "                try:\n",
    "                    text = self.lemmatize_text(text, lower=False)\n",
    "                except Exception as excp:\n",
    "                    self.logger(f'Failed lemmatization for {filen}')\n",
    "                    exp = excp.args[0]\n",
    "\n",
    "            text = text.lower()\n",
    "            if exp is None:\n",
    "                # Remove noise words e.g. punctuation, numbers, non-utf characters etc\n",
    "                text = self.remove_noise(text)\n",
    "\n",
    "                # Skip documents with no content\n",
    "                if len(text) > 0:      \n",
    "                    # Detect majority language of the document \n",
    "                    try:\n",
    "                        predict_lang = detect_langs(text)[0]\n",
    "\n",
    "                        lang_log = (predict_lang.lang, predict_lang.prob)\n",
    "\n",
    "                        if self.check_language:\n",
    "                            if (any([predict_lang.lang == lg for lg in self.supported_lang])) and (predict_lang.prob >= self.min_en_lang_prob):   # Only process documents in English                            \n",
    "                                if self.use_spellchecker:\n",
    "                                    # Run spell check and keep only the words found in dictionary\n",
    "                                    spell_data = self.spellcheck_text(text)\n",
    "                                    spell_errors = spell_data['errors']\n",
    "                                    text = spell_data['text']\n",
    "\n",
    "                                # Log tokens count\n",
    "                                token_log = len(word_tokenize(text))\n",
    "                                write_status = True\n",
    "                            else:\n",
    "                                #not in english\n",
    "                                skipped_log = f'Not in english | {predict_lang}'\n",
    "                        else:\n",
    "                            if self.use_spellchecker:\n",
    "                                # Run spell check and keep only the words found in dictionary\n",
    "                                spell_data = self.spellcheck_text(text)\n",
    "                                spell_errors = spell_data['errors']\n",
    "                                text = spell_data['text']\n",
    "                                \n",
    "                            # Log tokens count\n",
    "                            token_log = len(word_tokenize(text))\n",
    "                            write_status = True\n",
    "\n",
    "                    except Exception as excp:\n",
    "                        skipped_log = f\"Error detecting language for = {filen}. {excp.args[0]}\"\n",
    "                        self.logger(skipped_log)\n",
    "                        exp = excp.args[0]\n",
    "                else:\n",
    "                    skipped_log = f\"Empty doc post lemmatizer = {filen}\"\n",
    "                    self.logger(skipped_log)\n",
    "                    # Log tokens count\n",
    "        else:\n",
    "            skipped_log = f\"Doclen {len_text} < {self.ignore_length} = {filen}\"\n",
    "            self.logger(skipped_log)\n",
    "            # Log tokens count\n",
    "            token_log = 0\n",
    "\n",
    "        text_log = text\n",
    "\n",
    "        payload = dict(\n",
    "            lang=lang_log,\n",
    "            token=token_log,\n",
    "            text=text_log,\n",
    "            skipped=skipped_log,\n",
    "            spell_errors=spell_errors,\n",
    "            exception=exp,\n",
    "            write_status=write_status,\n",
    "        )\n",
    "\n",
    "        payload = {k: v if not isinstance(v, set) else list(v) for k, v in payload.items()}\n",
    "        return payload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorpusCleaner(Cleaner):\n",
    "\n",
    "    # Clean documents using spell checker\n",
    "    def batch_clean_docs(self, doclist, batch_size=None, save_docs=False, collect_text_log=False, collect_spell_errors=False, skip_existing=True, default_docs_per_worker=20):\n",
    "        if batch_size is None:\n",
    "            # Use a multiplier for efficient usage of workers\n",
    "            batch_size = default_docs_per_worker * self.num_workers\n",
    "\n",
    "        file_counter_x = 0\n",
    "        input_folder  = self.input_folder\n",
    "        output_folder = self.output_folder\n",
    "        \n",
    "        #log statistics\n",
    "        lang_log = {} # Lang info per document - uses the format - lang_log[fileid]=('lang', 'score')\n",
    "        text_log = {} # Errors count per document\n",
    "        token_log = {} # Tokens count per document\n",
    "        skipped_log = {} # Documents not processed\n",
    "        spell_errors = {}\n",
    "        exception_log = {}\n",
    "        write_status_log = {}\n",
    "        \n",
    "        log_interval = batch_size\n",
    "\n",
    "        with Parallel(n_jobs=self.num_workers, backend='multiprocessing') as pool:\n",
    "            # Cleaning all text files found in input in folder\n",
    "            batch = []\n",
    "            for ix, fileid in enumerate(doclist):\n",
    "                if ix % log_interval == 0:\n",
    "                    self.logger(f'Docset {ix}')\n",
    "\n",
    "                file_counter_x += 1\n",
    "                if fileid.endswith('.txt'):    # text files only \n",
    "\n",
    "                    filen = os.path.join(input_folder, fileid)     # input file \n",
    "                    newfile = os.path.join(output_folder, fileid)   # output file\n",
    "                    \n",
    "                    if not os.path.isfile(filen):\n",
    "                        self.logger(f\"No input file: {fileid}\")\n",
    "                        continue\n",
    "\n",
    "                    # Skip if output file already exists\n",
    "                    if os.path.isfile(newfile) and skip_existing:\n",
    "                        # self.logger(f\"Output file exists: {fileid}. Skipping...\")\n",
    "                        continue\n",
    "\n",
    "                    if len(batch) != batch_size:\n",
    "                        batch.append(filen)\n",
    "\n",
    "                    else:\n",
    "                        with Timer() as timer:\n",
    "                            doc_outputs = pool((delayed(self.clean_doc)(fln, save_doc=save_docs) for fln in batch))\n",
    "                            # doc_outputs = Parallel(n_jobs=self.num_workers, backend='multiprocessing')(delayed(self.clean_doc)(fln, save_doc=save_docs) for fln in batch)\n",
    "                            # doc_outputs = pool.map(self.clean_doc, [(fln, save_docs) for fln in batch], chunksize=batch_size)\n",
    "\n",
    "                            for doc_output in doc_outputs:\n",
    "\n",
    "                                lang_log.update(doc_output['lang'])\n",
    "                                token_log.update(doc_output['tokens'])\n",
    "                                skipped_log.update(doc_output['skipped'])  \n",
    "                                exception_log.update(doc_output['exception'])\n",
    "                                write_status_log.update(doc_output['write_status'])\n",
    "\n",
    "                                if collect_text_log:\n",
    "                                    # Don't do this if you're processing a lot of docs\n",
    "                                    text_log.update(doc_output['text'])\n",
    "                                if collect_spell_errors:\n",
    "                                    spell_errors.update(doc_output['spell_errors'])\n",
    "\n",
    "                            batch = []\n",
    "                            \n",
    "                        self.logger(f'Set {ix}: {log_interval} items for {timer.elapsed:.2f} seconds.')\n",
    "\n",
    "            if batch:\n",
    "                doc_outputs = pool((delayed(self.clean_doc)(fln, save_doc=save_docs) for fln in batch))\n",
    "                # doc_outputs = Parallel(n_jobs=self.num_workers, backend='multiprocessing')(delayed(self.clean_doc)(fln, save_doc=save_docs) for fln in batch)\n",
    "                # doc_outputs = pool.map(self.clean_doc, [(fln, save_docs) for fln in batch], chunksize=batch_size)\n",
    "\n",
    "                for doc_output in doc_outputs:\n",
    "\n",
    "                    lang_log.update(doc_output['lang'])\n",
    "                    token_log.update(doc_output['tokens'])\n",
    "                    skipped_log.update(doc_output['skipped']) \n",
    "                    exception_log.update(doc_output['exception'])\n",
    "                    write_status_log.update(doc_output['write_status'])\n",
    "\n",
    "                    if collect_text_log:\n",
    "                        # Don't do this if you're processing a lot of docs\n",
    "                        text_log.update(doc_output['text'])\n",
    "                    if collect_spell_errors:\n",
    "                        spell_errors.update(doc_output['spell_errors'])\n",
    "\n",
    "        output_log = {}\n",
    "        output_log['lang'] = lang_log\n",
    "        output_log['tokens'] = token_log\n",
    "        output_log['text'] = text_log\n",
    "        output_log['spell_errors'] = spell_errors\n",
    "        output_log['skipped'] = skipped_log\n",
    "        output_log['exception'] = exception_log\n",
    "        output_log['write_status'] = write_status_log\n",
    "\n",
    "        return output_log\n",
    "\n",
    "    # Clean a single document using spell checker    \n",
    "    def clean_doc(self, filepath, save_doc=False):  # args):\n",
    "        # filepath, *save_doc = args\n",
    "\n",
    "        if save_doc is None:\n",
    "            save_doc = False\n",
    "\n",
    "        # log statistics\n",
    "        lang_log = {}  # lang info per document - uses the format - lang_log[fileid]={'score','lang'}\n",
    "        spell_errors = {}\n",
    "        token_log = {}  # Tokens count per document\n",
    "        text_errors = {}\n",
    "        text_log = {}\n",
    "        skipped_log = {}\n",
    "        exception_log = {}\n",
    "        write_status_log = {}\n",
    "\n",
    "        filename = filepath.split('/')[-1]\n",
    "    \n",
    "        fileid = filename.strip('.txt')\n",
    "        \n",
    "        with open(filepath, 'rb') as fl:\n",
    "            # Use context so that the file will be closed automatically upon exit from the context.\n",
    "            text = fl.read()\n",
    "            text = text.decode('utf-8', errors='ignore')\n",
    "        \n",
    "        cleaning_output = self.clean_text(text, filen=fileid)\n",
    "        text = cleaning_output['text']\n",
    "        \n",
    "        lang_log[fileid] = cleaning_output['lang']\n",
    "        token_log[fileid] = cleaning_output['token']\n",
    "        skipped_log[fileid] = cleaning_output['skipped']\n",
    "        text_log[fileid] = text\n",
    "        spell_errors[fileid] = cleaning_output['spell_errors']\n",
    "        exception_log[fileid] = cleaning_output['exception']\n",
    "        write_status_log[fileid] = cleaning_output['write_status']\n",
    "        \n",
    "        if save_doc and cleaning_output['write_status']:\n",
    "            with open(os.path.join(self.output_folder, filename), 'w') as fl:\n",
    "                fl.write(text)\n",
    "\n",
    "        # return logs\n",
    "        output_log = {}\n",
    "        output_log['lang'] = lang_log\n",
    "        output_log['tokens'] = token_log\n",
    "        output_log['text'] = text_log\n",
    "        output_log['spell_errors'] = spell_errors\n",
    "        output_log['skipped'] = skipped_log\n",
    "        output_log['exception'] = exception_log\n",
    "        output_log['write_status'] = write_status_log\n",
    "\n",
    "        return output_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class ParallelCorpusCleaner(Cleaner):\n",
    "\n",
    "    # Clean documents using spell checker    \n",
    "    def batch_clean_docs(self, doclist, batch_size=None, save_docs=False, collect_text_log=False, process_output_manager=None, collect_spell_errors=False, skip_existing=True):\n",
    "        if batch_size is None:\n",
    "            # Use a multiplier for efficient usage of workers\n",
    "            batch_size = 4 * self.num_workers\n",
    "\n",
    "        file_counter_x = 0\n",
    "        input_folder  = self.input_folder\n",
    "        output_folder = self.output_folder\n",
    "        \n",
    "        #log statistics\n",
    "        lang_log = {} # Lang info per document - uses the format - lang_log[fileid]=('lang', 'score')\n",
    "        text_log = {} # Errors count per document\n",
    "        token_log = {} # Tokens count per document\n",
    "        skipped_log = {} # Documents not processed\n",
    "        spell_errors = {}\n",
    "        exception_log = {}\n",
    "        write_status_log = {}\n",
    "        \n",
    "        log_interval = batch_size\n",
    "        \n",
    "        if process_output_manager is None:\n",
    "            process_output_manager = multiprocessing.Manager()\n",
    "\n",
    "        process_output_dict = process_output_manager.dict()\n",
    "\n",
    "        batch = {}\n",
    "        ix = 0\n",
    "        while True:\n",
    "            try:\n",
    "                if len(batch) < batch_size:\n",
    "                    fileid = doclist.pop(0)\n",
    "                    # print(f'Processing {ix}: {fileid}')\n",
    "                    ix += 1\n",
    "\n",
    "                    if ix % log_interval == 0:\n",
    "                        self.logger(f'Docset {ix}')\n",
    "\n",
    "                    if fileid.endswith('.txt'):    # text files only \n",
    "                        filen = os.path.join(input_folder, fileid)     # input file \n",
    "                        newfile = os.path.join(output_folder, fileid)   # output file\n",
    "\n",
    "                        if not os.path.isfile(filen):\n",
    "                            self.logger(f\"No input file: {fileid}\")\n",
    "                            continue\n",
    "\n",
    "                        # Skip if output file already exists\n",
    "                        if os.path.isfile(newfile) and skip_existing:\n",
    "                            p = multiprocessing.Process(target=self.load_existing_and_extract_metadata, args=(fileid, newfile, save_docs, process_output_dict))  # , kwargs=kwargs)\n",
    "                            batch[fileid] = p\n",
    "                            p.start()\n",
    "                            # self.logger(f\"Output file exists: {fileid}. Skipping...\")\n",
    "                            continue\n",
    "\n",
    "                        # kwargs = {'process_output_dict': process_output_dict, 'save_doc': save_docs}                    \n",
    "                        p = multiprocessing.Process(target=self.clean_doc, args=(fileid, filen, save_docs, process_output_dict))  # , kwargs=kwargs)\n",
    "                        batch[fileid] = p\n",
    "                        p.start()\n",
    "                else:\n",
    "                    completed_data = set(process_output_dict.keys())\n",
    "\n",
    "                    for fld in completed_data:\n",
    "                        # print(f'Completed {fld}')\n",
    "                        if fld not in batch:\n",
    "                            continue\n",
    "                        pk = batch.pop(fld)\n",
    "                        pk.join()\n",
    "\n",
    "                        # This shouldn't be necessary but still doing this just to be safe... :)\n",
    "                        if pk.is_alive():\n",
    "                            pk.terminate()\n",
    "                            pk.join()\n",
    "\n",
    "                        fileid = doclist.pop(0)\n",
    "                        ix += 1\n",
    "                        \n",
    "                        # print(f'Starting {fileid}')\n",
    "\n",
    "                        if ix % log_interval == 0:\n",
    "                            self.logger(f'Docset {ix}')\n",
    "\n",
    "                        if fileid.endswith('.txt'):    # text files only \n",
    "                            filen = os.path.join(input_folder, fileid)     # input file \n",
    "                            newfile = os.path.join(output_folder, fileid)   # output file\n",
    "\n",
    "                            if not os.path.isfile(filen):\n",
    "                                self.logger(f\"No input file: {fileid}\")\n",
    "                                continue\n",
    "\n",
    "                            # Skip if output file already exists\n",
    "                            if os.path.isfile(newfile) and skip_existing:\n",
    "                                p = multiprocessing.Process(target=self.load_existing_and_extract_metadata, args=(fileid, newfile, save_docs, process_output_dict))  # , kwargs=kwargs)\n",
    "                                batch[fileid] = p\n",
    "                                p.start()\n",
    "                                # self.logger(f\"Output file exists: {fileid}. Skipping...\")\n",
    "                                continue\n",
    "\n",
    "                            # kwargs = {'process_output_dict': process_output_dict, 'save_doc': save_docs}                    \n",
    "                            p = multiprocessing.Process(target=self.clean_doc, args=(fileid, filen, save_docs, process_output_dict))  # , kwargs=kwargs)\n",
    "                            batch[fileid] = p\n",
    "                            p.start()\n",
    "                            \n",
    "            except Exception as e:\n",
    "                print(f'Exception received: {e.args[0]}')\n",
    "                bfileids = set(batch.keys())\n",
    "                for fileid in bfileids:\n",
    "                    p = batch.pop(fileid)\n",
    "                    p.join()\n",
    "\n",
    "                    if p.is_alive():\n",
    "                        p.terminate()\n",
    "                        p.join()\n",
    "                break\n",
    "\n",
    "        # Cleanup just in case... \n",
    "        bfileids = set(batch.keys())\n",
    "        for fileid in bfileids:\n",
    "            p = batch.pop(fileid)\n",
    "            p.join()\n",
    "\n",
    "            if p.is_alive():\n",
    "                p.terminate()\n",
    "                p.join()\n",
    "                \n",
    "        for fileid in process_output_dict.keys():\n",
    "            doc_output = process_output_dict[fileid]\n",
    "\n",
    "            lang_log.update(doc_output['lang'])\n",
    "            token_log.update(doc_output['tokens'])\n",
    "            skipped_log.update(doc_output['skipped'])  \n",
    "            exception_log.update(doc_output['exception'])\n",
    "            write_status_log.update(doc_output['write_status'])\n",
    "\n",
    "            if collect_text_log:\n",
    "                # Don't do this if you're processing a lot of docs\n",
    "                text_log.update(doc_output['text'])\n",
    "            if collect_spell_errors:\n",
    "                spell_errors.update(doc_output['spell_errors'])\n",
    "\n",
    "        output_log = {}\n",
    "        output_log['lang'] = lang_log\n",
    "        output_log['tokens'] = token_log\n",
    "        output_log['text'] = text_log\n",
    "        output_log['spell_errors'] = spell_errors\n",
    "        output_log['skipped'] = skipped_log\n",
    "        output_log['exception'] = exception_log\n",
    "        output_log['write_status'] = write_status_log\n",
    "\n",
    "        return output_log\n",
    "\n",
    "    # Clean a single document using spell checker    \n",
    "    def clean_doc(self, fileid, filepath, save_doc=False, process_output_dict=None):  # args):\n",
    "        proc_fileid = fileid\n",
    "        # filepath, *save_doc = args\n",
    "\n",
    "        if save_doc is None:\n",
    "            save_doc = False\n",
    "\n",
    "        # log statistics\n",
    "        lang_log = {}  # lang info per document - uses the format - lang_log[fileid]={'score','lang'}\n",
    "        spell_errors = {}\n",
    "        token_log = {}  # Tokens count per document\n",
    "        text_errors = {}\n",
    "        text_log = {}\n",
    "        skipped_log = {}\n",
    "        exception_log = {}\n",
    "        write_status_log = {}\n",
    "\n",
    "        filename = filepath.split('/')[-1]\n",
    "    \n",
    "        fileid = filename.strip('.txt')\n",
    "        \n",
    "        with open(filepath, 'rb') as fl:\n",
    "            # Use context so that the file will be closed automatically upon exit from the context.\n",
    "            text = fl.read()\n",
    "            text = text.decode('utf-8', errors='ignore')\n",
    "            text = text.lower()\n",
    "        \n",
    "        cleaning_output = self.clean_text(text, filen=fileid)\n",
    "        text = cleaning_output['text']\n",
    "        \n",
    "        lang_log[fileid] = cleaning_output['lang']\n",
    "        token_log[fileid] = cleaning_output['token']\n",
    "        skipped_log[fileid] = cleaning_output['skipped']\n",
    "        text_log[fileid] = text\n",
    "        spell_errors[fileid] = cleaning_output['spell_errors']\n",
    "        exception_log[fileid] = cleaning_output['exception']\n",
    "        write_status_log[fileid] = cleaning_output['write_status']\n",
    "        \n",
    "        if save_doc and cleaning_output['write_status']:\n",
    "            with open(os.path.join(self.output_folder, filename), 'w') as fl:\n",
    "                fl.write(text)\n",
    "\n",
    "        # return logs\n",
    "        output_log = {}\n",
    "        output_log['lang'] = lang_log\n",
    "        output_log['tokens'] = token_log\n",
    "        output_log['text'] = text_log\n",
    "        output_log['spell_errors'] = spell_errors\n",
    "        output_log['skipped'] = skipped_log\n",
    "        output_log['exception'] = exception_log\n",
    "        output_log['write_status'] = write_status_log\n",
    "        \n",
    "        if process_output_dict is not None:\n",
    "            process_output_dict[proc_fileid] = output_log\n",
    "        else: \n",
    "            return output_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# payload = dict(\n",
    "#     lang=lang_log,\n",
    "#     token=token_log,\n",
    "#     text=text_log,\n",
    "#     skipped=skipped_log,\n",
    "#     spell_errors=spell_errors,\n",
    "#     exception=exp,\n",
    "#     write_status=write_status,\n",
    "# )\n",
    "\n",
    "# payload = {k: v if not isinstance(v, set) else list(v) for k, v in payload.items()}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Input document text and process documents with at least `ignore_length` characters.\n",
    "\n",
    "Step 1: Detect countries from the document if a country map file is provided.\n",
    "Step 2: Apply lemmatization if specified (lemmatizer options: spacy or nltk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}