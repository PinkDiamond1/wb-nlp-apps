#!/usr/bin/env python
# -*- coding: utf-8 -*-
'''
This script processes the text data to generate candidate phrases based on part-of-speech patterns.
'''
import logging
from pathlib import Path

import json
import re

import click

import nltk
from gensim import utils
from joblib import Parallel, delayed
import joblib
import wb_nlp
from wb_nlp.utils.scripts import configure_logger, create_dask_cluster

# logging.basicConfig(stream=sys.stdout,
#                     level=logging.INFO,
#                     datefmt='%Y-%m-%d %H:%M',
#                     format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

MAX_LENGTH = 1000000


'''
Notes:
1. Download entities data from https://dumps.wikimedia.org/wikidatawiki/entities/latest-all.json.bz2 and save in /data/external/wikipedia/latest-all.json.bz2
2. Run `python -u ./scripts/wikipedia/extract_wikimedia_entities_titles.py --input-file ./data/external/wikipedia/latest-all.json.bz2 --output-file ./data/external/wikipedia/latest-all.wikimedia-titles.json  |& tee ./logs/extract_wikimedia_entities_titles.py.log

Algorithm:
1. For each entity, check if not in excluded items or doesn't contain properties in the excluded properties list.
2. If OK in step 1, check the numbeer of wikipedia pages that are available about the entity. General concepts should, ideally, have wikipedia articles in the major languages.
3. Titles should not have accented characters.
4.

Excluded lists:
1. Names of people
2. Names of places
3. Everything that is tied with a country, e.g., companies, work of art, etc.

Use all words in the title with at least 3 characters.

JSON data schema description: https://www.mediawiki.org/wiki/Wikibase/DataModel/JSON
'''

P_INTRO_NOISE_PATTERN = re.compile(r'[^a-zA-Z0-9\-., ]')
P_BDAY = re.compile(
    '(?:([A-Z][a-z]+ [0-9]{1,2}, [0-9]{1,4})|([0-9]{1,2} [A-Z][a-z]+ [0-9]{1,4}))')
P_ASCII = re.compile('^[ -~]+$')
GENSIM_BOLD_PATTERN = "'''"

EXCLUDE_LIST_PROPERTIES = set([
    # https://www.wikidata.org/wiki/Property:P495
    'P495',     # - country of origin
    'P21',      # - sex or gender
    'P27',      # - country of citizenship
    'P17',      # - country
    'P276',     # - location
])

EXCLUDE_LIST_ITEMS = set([
    # https://www.wikidata.org/wiki/Q23810017
    'Q5',            # - human
    'Q23810017',     # - decree
    'Q35749',        # - parliament
])

# https://www.babbel.com/en/magazine/the-10-most-spoken-languages-in-the-world
MAJOR_LANG_WIKIS = set([
    'enwiki',  # English
    'zhwiki',  # Chinese
    'hiwiki',  # Hindi
    'eswiki',  # Spanish
    'frwiki',  # French
    'arwiki',  # Arabic
    'bnwiki',  # Bangla/Bengali
    'ruwiki',  # Russian
    'ptwiki',  # Portuguese
    'idwiki',  # Indonesian
])


def process_data_entry(data_entry):
    '''
    This function extracts the title and first line of the intro of the wiki article.
    There's an assumption that the first line of a wiki intro succinctly describes the article itself.
    This succinct description, together with the title, may be used as reference to filter useful phrases.

    The return format is a tuple. The first value is the title and the second is the intro. This is designed
    so that it's easy to generate a dictionary from the list of such tuples, i.e., result of joblib parallel processing.
    '''
    entry = json.loads(data_entry)

    if (entry['type'] != 'item' or
        entry['id'] in EXCLUDE_LIST_ITEMS or
            set(entry['claims']).intersection(EXCLUDE_LIST_PROPERTIES) or
            entry['sitelinks'].get('enwiki', None) is None or
            len(entry['sitelinks'])):
        return (entry['id'], None)

    section_texts = article['section_texts']

    if (title.isdigit() or title.isupper() or title[0].isdigit() or len(title) <= 2 or len(section_texts) == 0 or P_ASCII.search(title) is None):
        return (title, None)

    intro = section_texts[0]

    # Use the ''' convention in gensim processing to identify the general start of intro.
    # Example, the first line of the entry for "Albedo" is the description of an image. We use
    # The above convention to make sure we get the correct first line.

    intro = P_INTRO_NOISE_PATTERN.sub(
        ' ', GENSIM_BOLD_PATTERN.join(intro.split(GENSIM_BOLD_PATTERN)[1:]))
    sents = nltk.sent_tokenize(intro)

    intro = None

    if sents:
        intro = sents[0]
        if P_BDAY.search(intro):
            return (title, None)

    return (title, intro)


_logger = logging.getLogger(__file__)


@ click.command()
@ click.option('--input-file', 'input_file', required=True,
               type=click.Path(exists=True), help='path to wiki articles generated by gensim.scripts.segment_wiki')
@ click.option('--output-file', 'output_file', required=True,
               type=click.Path(exists=False), help='path to output json file of article titles and intros')
@ click.option('--quiet', 'log_level', flag_value=logging.WARNING, default=True)
@ click.option('-v', '--verbose', 'log_level', flag_value=logging.INFO)
@ click.option('-vv', '--very-verbose', 'log_level', flag_value=logging.DEBUG)
@ click.option('--n-workers', 'n_workers', required=False, default=None)
@ click.option('--batch-size', 'batch_size', required=False, default=None)
@ click.version_option(wb_nlp.__version__)
def main(input_file: Path, output_file: Path, log_level: int, n_workers: int = None, batch_size: int = None):
    '''
    Entry point for wikimedia data filtering and title extraction script.
    '''
    configure_logger(log_level)

    # YOUR CODE GOES HERE! Keep the main functionality in src/wb_nlp
    input_file = Path(input_file)
    output_file = Path(output_file)

    logging.info('Checking files...')
    if not input_file.exists():
        raise ValueError("Input file doesn't exist!")

    if not output_file.resolve().parent.exists():
        output_file.resolve().parent.mkdir(parents=True)

    client = create_dask_cluster(_logger)
    _logger.info(client)

    _logger.info('Starting joblib tasks...')

    with utils.open(input_file, 'rb') as wiki_gz:
        with joblib.parallel_backend('dask'):
        batch_size = 'auto' if batch_size is None else int(batch_size)

        res = Parallel(verbose=10, batch_size=batch_size)(
            delayed(process_data_entry)(line) for line in wiki_gz)

    with open(output_file, 'w') as out:
        json.dump(dict(filter(lambda x: x[1] is not None, res)), out)

    _logger.info('Processed all: %s', all(res))

# Parameters:
# - Location of input data
# - Directory of * .txt files
# - MongoDB database


if __name__ == '__main__':
    # python -u ./scripts/wikipedia/extract_wikipedia_titles.py --input-file ./data/external/wikipedia/enwiki-latest.json.gz --output-file ./data/external/wikipedia/enwiki-latest.titles-intro.json  |& tee ./logs/extract_wikipedia_titles.py.log
    main()


# '''
# https://www.wikidata.org/w/index.php?title=Special:WhatLinksHere/Q5&limit=500
# https://www.wikidata.org/w/index.php?title=Special:WhatLinksHere/Q5&namespace=0&limit=500&from=7130&back=4035
# https://meta.wikimedia.org/wiki/Data_dumps
# https://www.mediawiki.org/wiki/Wikibase/DataModel/JSON


# '''
