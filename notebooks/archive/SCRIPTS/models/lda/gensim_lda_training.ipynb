{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sudo apt-get install default-jdk\n",
    "sudo apt-get install ant\n",
    "git clone git@github.com:mimno/Mallet.git\n",
    "cd Mallet/\n",
    "ant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import gc\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# LdaMallet\n",
    "# Dictionary (gensim)\n",
    "# build_docs\n",
    "# transform_dt\n",
    "# get_tw\n",
    "# get_top_words\n",
    "%run ./LDAModule.ipynb\n",
    "\n",
    "# DocsManager\n",
    "# build_docs\n",
    "%run ../../DocsManager.ipynb\n",
    "## Jupyter.notebook.save_checkpoint()\n",
    "\n",
    "# get_corpus_path\n",
    "# get_txt_clean_path\n",
    "%run ../../path_manager.ipynb\n",
    "\n",
    "# CorpusCleaner\n",
    "%run ../../DataCleanerModule.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models.wrappers.ldamallet import malletmodel2ldamodel, LdaMallet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/wb536061/wb-nlp/CORPUS/IMF'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_corpus_path('IMF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_PATH = get_models_path('GENSIMLDA').replace('MODELS', 'MODELS.gensim.staging')\n",
    "MALLET_BINARY_PATH = \"../Mallet/bin/mallet\"\n",
    "\n",
    "NUM_WORKERS = get_workers()\n",
    "NUM_ITERS = 196\n",
    "MIN_TOKEN_COUNT = 100  # Exclude documents with less than 100 tokens\n",
    "NGRAM_FILE =  '../../whitelists/whitelist_ngrams_cleaned.csv'  # '../../whitelists/whitelist_ngrams_truncated_cleaned.csv'\n",
    "DOC_PROCESSING_WORKERS = 2 * max(1, os.cpu_count() - 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/wb536061/wb-nlp/MODELS.gensim.staging/GENSIMLDA'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODELS_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(MODELS_PATH):\n",
    "    os.makedirs(MODELS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import gc\n",
    "\n",
    "TRAINING_MODEL_ID = 'GENSIMLDA'\n",
    "\n",
    "logging.basicConfig(filename=f'./{TRAINING_MODEL_ID.lower()}-iters_{NUM_ITERS}.log', format='%(asctime)s - %(levelname)s - %(message)s', level=logging.INFO)\n",
    "\n",
    "logger = logging.getLogger(f'{TRAINING_MODEL_ID.lower()}-logger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# for CORPUS_ID in ['IMF', 'WB']:\n",
    "for CORPUS_ID in ['WB']:\n",
    "    if CORPUS_ID == 'WB':\n",
    "        region_partitions = []  # ['AFR', 'EAP', 'ECA', 'LAC', 'MENA', 'RoW', 'SAR', 'WLD']\n",
    "        doctype_partitions = []  # ['BD', 'CF', 'ESW', 'PD', 'PR']\n",
    "        doctype_partitions = doctype_partitions[::-1]\n",
    "        corpus_partitions = ['ALL'] + doctype_partitions + region_partitions\n",
    "    else:\n",
    "        corpus_partitions = ['ALL']\n",
    "\n",
    "    num_topics = [50, 100, 200]  # [25, 50, 100]\n",
    "\n",
    "    for CORPUS_PART in corpus_partitions:\n",
    "        docs = build_docs(\n",
    "            metadata_filename=os.path.join(get_corpus_path(CORPUS_ID), f'{CORPUS_ID.lower()}_metadata_complete.csv'),\n",
    "            cleaned_files_dir=get_txt_clean_path(CORPUS_ID),\n",
    "            model_output_dir=MODELS_PATH  # Use flat directory as discussed...\n",
    "        )\n",
    "\n",
    "        logger.info(f'Creating partitioned docs and loading files for {CORPUS_ID}-{CORPUS_PART}...')\n",
    "\n",
    "        # docs.set_ngram_mapper('../../whitelists/whitelist_ngrams_cleaned.csv', cleaner=None)\n",
    "        docs.set_ngram_mapper(NGRAM_FILE, cleaner=None)\n",
    "        \n",
    "        docs.set_min_token_count(MIN_TOKEN_COUNT)\n",
    "        docs_filtered, meta = docs.filter_doclist(CORPUS_PART, corpus_id=CORPUS_ID, save=True, return_meta=True, pool_workers=DOC_PROCESSING_WORKERS)\n",
    "\n",
    "        print(docs_filtered.shape)\n",
    "        logger.info(f'Building model for {docs_filtered.shape[0]} documents...')\n",
    "        if docs_filtered.empty:\n",
    "            continue\n",
    "\n",
    "        logger.info('Building dictionary...')\n",
    "        g_dict = Dictionary(docs_filtered.text.str.split())\n",
    "        g_dict.filter_extremes(no_below=10, no_above=0.99, keep_n=200000, keep_tokens=None)  # Exclude words appearing in less than 10 docs.\n",
    "        g_dict.id2token = {id: token for token, id in g_dict.token2id.items()}\n",
    "\n",
    "        logger.info('Performing doc2bow...')\n",
    "        # corpus = [g_dict.doc2bow(c.split()) for c in docs_filtered.text]\n",
    "\n",
    "        with mp.Pool(NUM_WORKERS) as pool:\n",
    "            # pool = mp.Pool(processes=10)\n",
    "            logger.info('Performing parallel doc2bow...')\n",
    "            corpus = pool.map(g_dict.doc2bow, docs_filtered.text.str.split())\n",
    "            logger.info('Completed parallel doc2bow...')\n",
    "            # pool.close()\n",
    "            # pool.join()\n",
    "\n",
    "        for NUM_TOPICS in num_topics:\n",
    "\n",
    "            MODEL_ID = f\"{CORPUS_PART}_{NUM_TOPICS}\"\n",
    "            logger.info(f'Starting process for {MODEL_ID}...')\n",
    "\n",
    "            MODEL_FOLDER = os.path.join(MODELS_PATH, f'{CORPUS_ID}-{MODEL_ID}')\n",
    "\n",
    "            MODEL_DATA_FOLDER = os.path.join(MODEL_FOLDER, 'data')\n",
    "            MODEL_MALLET_FOLDER = os.path.join(MODEL_FOLDER, 'mallet')\n",
    "\n",
    "            if not os.path.isdir(MODEL_DATA_FOLDER):\n",
    "                os.makedirs(MODEL_DATA_FOLDER)\n",
    "\n",
    "            if not os.path.isdir(MODEL_MALLET_FOLDER):\n",
    "                os.makedirs(MODEL_MALLET_FOLDER)\n",
    "\n",
    "            # Set logging\n",
    "            lfh = logging.FileHandler(f'./{CORPUS_ID.lower()}-iters_{NUM_ITERS}-{MODEL_ID}.log')\n",
    "            lfh.setLevel(logging.INFO)\n",
    "            formatter = logging.Formatter('%(asctime)s %(levelname)-8s %(message)s')\n",
    "            lfh.setFormatter(formatter)\n",
    "            logger.addHandler(lfh)\n",
    "            # End logging setup\n",
    "\n",
    "            logger.info('Training mallet LDA model...')\n",
    "\n",
    "            model = LdaModel(\n",
    "                corpus=corpus, num_topics=NUM_TOPICS, id2word=g_dict.id2token,\n",
    "                distributed=False,\n",
    "                chunksize=10000, passes=1, update_every=1,\n",
    "                alpha='symmetric', eta=None,\n",
    "                decay=0.5, offset=1.0, eval_every=10, iterations=50, gamma_threshold=0.001,\n",
    "                minimum_probability=0.01, random_state=1029, ns_conf=None,\n",
    "                minimum_phi_value=0.01,\n",
    "                per_word_topics=False, callbacks=None\n",
    "            )\n",
    "\n",
    "#             model = LdaMulticore(\n",
    "#                 corpus=corpus, num_topics=NUM_TOPICS, id2word=g_dict.id2token,\n",
    "#                 workers=int(NUM_WORKERS / 2),  # set workers directly to the number of your real cores (not hyperthreads) minus one, for optimal performance\n",
    "#                 chunksize=2000, passes=1,\n",
    "#                 batch=False, alpha='symmetric', eta=None,\n",
    "#                 decay=0.5, offset=1.0, eval_every=10, iterations=NUM_ITERS,\n",
    "#                 gamma_threshold=0.001, random_state=1029, minimum_probability=0.01,\n",
    "#                 minimum_phi_value=0.01, per_word_topics=False\n",
    "#             )\n",
    "            \n",
    "            logger.info('Completed training Multicore LDA model...')\n",
    "\n",
    "#             dt = pd.read_csv(\n",
    "#                 model.fdoctopics(), delimiter='\\t', header=None,\n",
    "#                 names=[i for i in range(model.num_topics)], index_col=None,\n",
    "#                 usecols=[i + 2 for i in range(model.num_topics)],\n",
    "#             )\n",
    "\n",
    "#             dt.index = docs_filtered['id']\n",
    "#             dt = dt.divide(dt.min(axis=1), axis=0).astype(int) - 1\n",
    "\n",
    "#             logger.info('Generating dfr-browser data...')\n",
    "#             ddt = transform_dt(dt.as_matrix().T)\n",
    "#             ttw = get_tw(model)\n",
    "\n",
    "#             with open(os.path.join(MODEL_DATA_FOLDER, 'tw.json'), 'w') as fl:\n",
    "#                 json.dump(ttw, fl)\n",
    "\n",
    "#             with open(os.path.join(MODEL_DATA_FOLDER, 'dt.json'), 'w') as fl:\n",
    "#                 json.dump(ddt, fl)\n",
    "\n",
    "#             info_json = {\n",
    "#                 \"title\": f\"Topics in <em>{CORPUS_ID} {MODEL_ID}<\\/em>\",\n",
    "#                 \"meta_info\": \"This site is the working demo for <a href=\\\"/\\\">dfr-browser</a>, a browsing interface for topic models of journal articles or other text.\",\n",
    "#                 \"VIS\": {\n",
    "#                     \"condition\": {\n",
    "#                         \"type\": \"time\",\n",
    "#                         \"spec\": {\n",
    "#                             \"unit\": \"year\",\n",
    "#                             \"n\": 1\n",
    "#                         }\n",
    "#                     },\n",
    "#                     \"bib_sort\": {\n",
    "#                         \"major\": \"year\",\n",
    "#                         \"minor\": \"alpha\"\n",
    "#                     },\n",
    "#                     \"model_view\": {\n",
    "#                         \"plot\": {\n",
    "#                             \"words\": 6,\n",
    "#                             \"size_range\": [6, 14]\n",
    "#                         } \n",
    "#                     }\n",
    "#                 }\n",
    "#             }\n",
    "\n",
    "#             with open(os.path.join(MODEL_DATA_FOLDER, 'info.json'), 'w') as fl:\n",
    "#                 json.dump(info_json, fl)\n",
    "\n",
    "#             # Generation of key LDA files\n",
    "#             # doc_topics\n",
    "#             logger.info('Storing doc_topics...')\n",
    "#             dt.to_csv(\n",
    "#                 os.path.join(MODEL_DATA_FOLDER, f'doc_topics_{MODEL_ID}.csv'), \n",
    "#                 header=False,  # Change to True if topic id should be present as the header\n",
    "#                 index=False  # Change to True if the uid should be present as the index\n",
    "#             )\n",
    "#             dt.to_csv(\n",
    "#                 os.path.join(MODEL_DATA_FOLDER, f'doc_topics_{MODEL_ID}_with_details.csv'), \n",
    "#                 header=True,  # Change to True if topic id should be present as the header\n",
    "#                 index=True  # Change to True if the uid should be present as the index\n",
    "#             )\n",
    "\n",
    "#             # topic_words\n",
    "#             word_topics = pd.DataFrame(model.word_topics, columns=range(model.word_topics.shape[1]), index=range(1, model.word_topics.shape[0] + 1))\n",
    "#             word_topics = word_topics.rename(columns=model.id2word)\n",
    "\n",
    "#             logger.info('Storing word_topics...')\n",
    "#             word_topics.astype(int).to_csv(\n",
    "#                 os.path.join(MODEL_DATA_FOLDER, f'topic_words_{MODEL_ID}.csv'), \n",
    "#                 header=False,  # Change to True if actual word should be present as the header\n",
    "#                 index=False  # Sorted order by topic id\n",
    "#             )\n",
    "#             word_topics.astype(int).to_csv(\n",
    "#                 os.path.join(MODEL_DATA_FOLDER, f'topic_words_{MODEL_ID}_with_details.csv'), \n",
    "#                 header=True,  # Change to True if actual word should be present as the header\n",
    "#                 index=False  # Sorted order by topic id\n",
    "#             )\n",
    "\n",
    "#             logger.info('Storing top_words...')\n",
    "#             top_words = get_top_words(word_topics, topic=None, topn=NUM_TOPICS)\n",
    "#             top_words.to_csv(\n",
    "#                 os.path.join(MODEL_DATA_FOLDER, f'top_words_{MODEL_ID}.csv'), \n",
    "#                 index=False\n",
    "#             )\n",
    "\n",
    "            logger.info('Saving lda model...')\n",
    "            model.save(os.path.join(MODEL_DATA_FOLDER, f'{CORPUS_ID}_lda_model_{MODEL_ID}.mallet.lda'))\n",
    "\n",
    "            logger.info(f'lda model for {CORPUS_ID} completed: {MODEL_ID}...')\n",
    "            logger.removeHandler(lfh)\n",
    "\n",
    "            del(model)\n",
    "            gc.collect()\n",
    "\n",
    "        del(docs_filtered)\n",
    "        del(docs.doclist)\n",
    "        del(docs)\n",
    "        del(meta)\n",
    "        del(g_dict)\n",
    "        del(corpus)\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.26 s, sys: 0 ns, total: 4.26 s\n",
      "Wall time: 4.36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "docs = build_docs(\n",
    "    metadata_filename=os.path.join(get_corpus_path(CORPUS_ID), f'{CORPUS_ID.lower()}_metadata_complete.csv'),\n",
    "    cleaned_files_dir=get_txt_clean_path(CORPUS_ID),\n",
    "    model_output_dir=MODELS_PATH  # Use flat directory as discussed...\n",
    ")\n",
    "\n",
    "logger.info(f'Creating partitioned docs and loading files for {CORPUS_ID}-{CORPUS_PART}...')\n",
    "\n",
    "# docs.set_ngram_mapper('../../whitelists/whitelist_ngrams_cleaned.csv', cleaner=None)\n",
    "docs.set_ngram_mapper(NGRAM_FILE, cleaner=None)\n",
    "\n",
    "docs.set_min_token_count(MIN_TOKEN_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 50s, sys: 5min 1s, total: 13min 51s\n",
      "Wall time: 15min 28s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# docs_filtered, meta = docs.filter_doclist(CORPUS_PART, corpus_id=CORPUS_ID, save=True, return_meta=True, pool_workers=DOC_PROCESSING_WORKERS)\n",
    "# 'world_bank' in docs_filtered[docs_filtered['id'] == 'wb_10000660'].text.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min 37s, sys: 1min 33s, total: 11min 11s\n",
      "Wall time: 11min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# g_dict = Dictionary(docs_filtered.text.str.split())\n",
    "# g_dict.filter_extremes(no_below=10, no_above=0.99, keep_n=200000, keep_tokens=None)  # Exclude words appearing in less than 10 docs.\n",
    "# g_dict.id2token = {id: token for token, id in g_dict.token2id.items()}\n",
    "# g_dict.id2token[152], g_dict.token2id['purchasing_power_parity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/home/wb536061/wb-nlp/CORPUS/WB/TXT_CLEAN/wb_10000660.txt') as fl:\n",
    "#     d = fl.read()\n",
    "    \n",
    "# dd = docs.load_text('/home/wb536061/wb-nlp/CORPUS/WB/TXT_CLEAN/wb_10000660.txt')\n",
    "# 'world_bank' in dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LdaModel.load(os.path.join(MODEL_DATA_FOLDER, f'{CORPUS_ID}_lda_model_{MODEL_ID}.mallet.lda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = '/home/wb536061/wb-nlp/MODELS.gensim.staging/GENSIMLDA/WB-ALL_50/data/WB_lda_model_ALL_50.mallet.lda'\n",
    "model_path = '/home/wb536061/wb-nlp/MODELS/LDA/WB-ALL_50/data/WB_lda_model_ALL_50.mallet.lda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LdaMallet.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'stunting' in [model.id2word[i] for i in model.id2word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.05 s, sys: 0 ns, total: 1.05 s\n",
      "Wall time: 34.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    1225.0\n",
       "1       1.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# model[g_dict.doc2bow(docs_filtered.iloc[0].text.split())]\n",
    "pd.DataFrame(sorted(model[g_dict.doc2bow(docs_filtered.iloc[23].text.split())], key=lambda x: x[1])[::-1]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.056*\"city\" + 0.041*\"urban\" + 0.019*\"housing\" + 0.018*\"area\" + 0.018*\"project\" + 0.017*\"county\" + 0.016*\"construction\" + 0.014*\"taking_land\" + 0.014*\"local\" + 0.014*\"municipal\"'"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.print_topic(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 478 ms, sys: 0 ns, total: 478 ms\n",
      "Wall time: 477 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "glda = malletmodel2ldamodel(model, gamma_threshold=0.00001, iterations=500)\n",
    "glda.minimum_probability = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.029*\"agreement\" + 0.029*\"project\" + 0.023*\"association\" + 0.022*\"borrower\" + 0.022*\"bank\" + 0.021*\"section\" + 0.014*\"date\" + 0.011*\"make\" + 0.011*\"nonperforming_loan\" + 0.011*\"recipient\"'"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glda.print_topic(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.74 s, sys: 856 ms, total: 2.6 s\n",
      "Wall time: 37.8 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    1225.0\n",
       "1       1.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "pd.DataFrame(sorted(glda[g_dict.doc2bow(docs_filtered.iloc[23].text.split())], key=lambda x: x[1])[::-1]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'weekly global economic brief number information org gem strong growth united state japan help offset impact weak growth euro area fiscal tightening financial turmoil weak confidence cloud growth prospect assume dramatic deterioration fiscal crisis forward look indicator suggest manufacture output remain weak month ahead output euro area likely contract external short_term debt developing_country increase trade finance relate increase china country high level short_term debt subject roll risk especially today febrile economic climate strong growth united state japan rebound growth japan compensate expect weakness euro area growth accelerate compensate euro area weakness united state household percentage point contribution growth business spend japan economy boom net trade estimate domestic demand net inventory export bounce relative strength economy reflect domestic final sale recovery earthquake contrast activity euro area remain weak inventory continue build amid anemic domestic demand united state sustain growth pace help solid domestic demand momentum boost inventory firm draw inventory sharply united euro japan united euro japan quarter fiscal tightening kick early year state area state area expect moderate growth fiscal tightening financial estimate turmoil weak confidence euro area economy source world_bank prospect group depressed purchasing manager index suggest manufacture output sentiment manufacturer growth remain weak month ahead global manufacturing consistent weak output growth index record increase surpass purchasing manager index point growth mark slight improvement sentiment result modest increase new order index point increase finished good inventory index signal weakness ahead china particular euro area industrial_production likely contract month ahead decline sixth straight month tumble point weakness global forward look indicator reading indicate expansion consistent slowdown global industrial_production signal contraction momentum observe august rise financial market turmoil flood disrupt global supply_chain create headwind auto manufacturer united jun source world_bank prospect group state announce low output shortage external short_term debt rise late lead china developing_country outstanding external short_term debt china account surge external short_term debt develop reach trillion billion country increase china recent build short billion change short_term debt stock term debt country consistent rapid trade growth observe period trade finance account china country china short_term debt run external short_term debt occur monetary policy tighten country anecdotal evidence suggest firm seek external financing compensate tight domestic credit condition reaction government cut short_term foreign debt quota uncertainty global financial market developing_country high level short_term debt run refinancing difficulty overall developing_country external financing requirement source bis world_bank staff calculation estimate trillion material present gem website product staff world_bank finding interpretation conclusion express necessarily reflect view board executive_director world_bank government represent content quotation unless specifically authorize write world_bank permission contact org economic development indicator express industrial_production quarterly figure jun industrial_production world high income country developing_country east_pacific east china central middle_east south sub inflation high income country developing_country east_pacific central middle_east south sub inflation calculate median group trade finance indicator express international reserve trade quarterly figure jun export nominal world high income country developing_country east_pacific central middle_east south sub import nominal world high income country developing_country east_pacific central middle_east south sub international reserve high income country developing_country east_pacific central middle_east south sub weekly global economic brief financial market chg rate feed fund effective month month treasury yield spread basis point corporate high yield bond emerging_market middle_east stock index end period global high income index united state euro area japan develop market exchange_rate high income euro area japan develop china south memo nominal effective rate recent value index calculate data_availability change express level rate spread percent change stock market exchange_rate commodity_price chg oil price nominal non oil index food index metal mineral index dry index simple average base date base date weekly global economic brief'"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_filtered.iloc[19].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.89 ms, sys: 0 ns, total: 5.89 ms\n",
      "Wall time: 3.82 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(2, 0.20384429903016774),\n",
       " (3, 0.1358098533911168),\n",
       " (4, 0.1122730150862996),\n",
       " (20, 0.028335651206527986),\n",
       " (26, 0.012637532333221502),\n",
       " (37, 0.012046880751905275),\n",
       " (40, 0.21122673420712978),\n",
       " (47, 0.04032272962006966),\n",
       " (49, 0.04689379618932216)]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "glda[g_dict.doc2bow(docs_filtered.iloc[0].text.split())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.056*\"city\" + 0.041*\"urban\" + 0.019*\"housing\" + 0.018*\"area\" + 0.018*\"project\" + 0.017*\"county\" + 0.016*\"construction\" + 0.014*\"taking_land\" + 0.014*\"local\" + 0.014*\"municipal\"'"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.print_topic(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.029*\"bank\" + 0.025*\"panel\" + 0.025*\"project\" + 0.021*\"request\" + 0.020*\"report\" + 0.019*\"management\" + 0.013*\"issue\" + 0.010*\"response\" + 0.009*\"review\" + 0.009*\"information\"'"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.print_topic(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(124,\n",
       "  '0.056*\"migration\" + 0.056*\"remittance\" + 0.049*\"migrant\" + 0.027*\"country\" + 0.016*\"worker\" + 0.011*\"percent\" + 0.008*\"international\" + 0.007*\"flow\" + 0.007*\"send\" + 0.007*\"immigrant\"'),\n",
       " (130,\n",
       "  '0.379*\"phase\" + 0.039*\"salaam\" + 0.013*\"dart\" + 0.009*\"soc\" + 0.007*\"study\" + 0.006*\"project\" + 0.006*\"annex\" + 0.006*\"report\" + 0.005*\"cost\" + 0.005*\"got\"'),\n",
       " (90,\n",
       "  '0.034*\"food\" + 0.030*\"agricultural\" + 0.024*\"agriculture\" + 0.021*\"price\" + 0.014*\"production\" + 0.012*\"rice\" + 0.011*\"increase\" + 0.011*\"market\" + 0.009*\"policy\" + 0.009*\"farmer\"'),\n",
       " (14,\n",
       "  '0.017*\"project\" + 0.008*\"water\" + 0.007*\"area\" + 0.007*\"cost\" + 0.006*\"use\" + 0.005*\"land\" + 0.005*\"total\" + 0.004*\"impact\" + 0.004*\"development\" + 0.004*\"include\"'),\n",
       " (66,\n",
       "  '0.083*\"town\" + 0.028*\"wadi\" + 0.026*\"project\" + 0.014*\"lot\" + 0.013*\"royal\" + 0.011*\"initial\" + 0.010*\"text\" + 0.009*\"area\" + 0.009*\"supply\" + 0.009*\"consultant\"'),\n",
       " (83,\n",
       "  '0.048*\"actual\" + 0.046*\"date\" + 0.039*\"project\" + 0.031*\"value\" + 0.029*\"target\" + 0.027*\"current\" + 0.026*\"baseline\" + 0.026*\"end\" + 0.025*\"previous\" + 0.023*\"jun\"'),\n",
       " (89,\n",
       "  '0.062*\"school\" + 0.053*\"education\" + 0.045*\"teacher\" + 0.017*\"student\" + 0.011*\"quality\" + 0.011*\"program\" + 0.009*\"project\" + 0.008*\"training\" + 0.008*\"learn\" + 0.008*\"support\"'),\n",
       " (172,\n",
       "  '0.164*\"sierra\" + 0.119*\"guinea\" + 0.028*\"western\" + 0.028*\"council\" + 0.010*\"country\" + 0.009*\"project\" + 0.008*\"ward\" + 0.008*\"local\" + 0.008*\"chiefdom\" + 0.008*\"development\"'),\n",
       " (133,\n",
       "  '0.014*\"project\" + 0.005*\"sector\" + 0.005*\"policy\" + 0.005*\"year\" + 0.005*\"include\" + 0.005*\"area\" + 0.005*\"percent\" + 0.005*\"government\" + 0.005*\"management\" + 0.004*\"use\"'),\n",
       " (68,\n",
       "  '0.019*\"sector\" + 0.011*\"service\" + 0.011*\"project\" + 0.011*\"support\" + 0.011*\"public\" + 0.010*\"bank\" + 0.010*\"government\" + 0.009*\"improve\" + 0.008*\"area\" + 0.008*\"management\"'),\n",
       " (17,\n",
       "  '0.252*\"scheme\" + 0.024*\"gram\" + 0.024*\"sip\" + 0.021*\"project\" + 0.010*\"circle\" + 0.008*\"division\" + 0.007*\"volume\" + 0.006*\"study\" + 0.006*\"work\" + 0.006*\"rate\"'),\n",
       " (111,\n",
       "  '0.183*\"mope\" + 0.019*\"project\" + 0.010*\"education\" + 0.010*\"development\" + 0.009*\"half\" + 0.009*\"area\" + 0.008*\"early\" + 0.006*\"institution\" + 0.006*\"opportunity\" + 0.006*\"school\"'),\n",
       " (107,\n",
       "  '0.063*\"tourism\" + 0.017*\"tourist\" + 0.013*\"hotel\" + 0.012*\"development\" + 0.011*\"sector\" + 0.010*\"industry\" + 0.009*\"investment\" + 0.008*\"local\" + 0.008*\"market\" + 0.008*\"visitor\"'),\n",
       " (143,\n",
       "  '0.096*\"doc\" + 0.077*\"rev\" + 0.055*\"rep\" + 0.052*\"comp\" + 0.043*\"int\" + 0.023*\"bps\" + 0.020*\"revision\" + 0.018*\"final\" + 0.016*\"val\" + 0.010*\"prep\"'),\n",
       " (54,\n",
       "  '0.022*\"child\" + 0.021*\"school\" + 0.019*\"student\" + 0.013*\"education\" + 0.012*\"skill\" + 0.010*\"use\" + 0.010*\"test\" + 0.009*\"score\" + 0.007*\"teacher\" + 0.007*\"study\"'),\n",
       " (95,\n",
       "  '0.120*\"page\" + 0.031*\"north\" + 0.020*\"power_plant\" + 0.019*\"pipeline\" + 0.011*\"point\" + 0.011*\"bio_gas\" + 0.009*\"risk\" + 0.009*\"figure\" + 0.009*\"power\" + 0.008*\"cycle\"'),\n",
       " (72,\n",
       "  '0.078*\"project\" + 0.026*\"management\" + 0.014*\"activity\" + 0.014*\"water\" + 0.013*\"watershed\" + 0.012*\"component\" + 0.012*\"implementation\" + 0.011*\"basin\" + 0.009*\"resource\" + 0.009*\"development\"'),\n",
       " (171,\n",
       "  '0.020*\"government\" + 0.017*\"donor\" + 0.016*\"support\" + 0.015*\"sector\" + 0.014*\"program\" + 0.010*\"bank\" + 0.010*\"development\" + 0.009*\"economic\" + 0.008*\"management\" + 0.007*\"improve\"'),\n",
       " (18,\n",
       "  '0.053*\"shall\" + 0.029*\"agreement\" + 0.028*\"section\" + 0.028*\"borrower\" + 0.025*\"association\" + 0.019*\"project\" + 0.016*\"bank\" + 0.013*\"paragraph\" + 0.012*\"date\" + 0.012*\"credit\"'),\n",
       " (167,\n",
       "  '0.051*\"port\" + 0.022*\"project\" + 0.014*\"cost\" + 0.011*\"ship\" + 0.010*\"total\" + 0.010*\"cargo\" + 0.010*\"million\" + 0.008*\"traffic\" + 0.008*\"construction\" + 0.008*\"year\"')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36222"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(model.id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# !../Mallet/bin/mallet import-file --preserve-case --keep-sequence --remove-stopwords --token-regex \"\\S+\" --input /R/NLP/MODELS/LDA/IMF-ALL_20/mallet/IMF-ALL_20_corpus.txt --output /R/NLP/MODELS/LDA/IMF-ALL_20/mallet/IMF-ALL_20_corpus.mallet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# http://microdatahub.com/topicsmodeling/dfr/topic_browser/browser.php?model=data50_SAR&type=SAR&topic_count=50#/doc/9622\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}