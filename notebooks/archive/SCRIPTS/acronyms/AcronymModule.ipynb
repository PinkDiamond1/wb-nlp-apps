{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "nltk.data.path.append(\"/R/nltk_data\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "acronyms_pattern = re.compile('(\\([A-Z]{2,}\\))')\n",
    "candidates_pattern = re.compile('([A-Z][a-z]+|\\([A-Z]{2,}\\))')\n",
    "newline_pattern = re.compile(b'[\\r\\n]+')\n",
    "whitespaces_pattern = re.compile('\\s+')\n",
    "alphabet_pattern = re.compile('[A-Z-a-z]+')\n",
    "stops = set(['the', 'of', 'in', 'and', 'or', 'for'])\n",
    "stops.update(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def extract_acronyms(txt):\n",
    "    acronyms = [i.strip('(').strip(')') for i in acronyms_pattern.findall(txt)]\n",
    "    keyword = '|'.join([f'\\({k}\\)' for k in acronyms])\n",
    "    candidates_acronym_pattern = \"((?:[a-zA-Z'-]+ ){0,5})(\" + keyword + \")\" #((?:[^a-zA-Z'-]+[a-zA-Z'-]+){0,5})\"\n",
    "    candidates_acronym_pattern = re.compile(candidates_acronym_pattern)\n",
    "    acronym_candidates_lists = candidates_acronym_pattern.findall(whitespaces_pattern.sub(' ', txt))\n",
    "    detected_acronyms = {}\n",
    "\n",
    "    for ip in acronym_candidates_lists:\n",
    "        candidate, acronym = ip\n",
    "        candidate = ' '.join(alphabet_pattern.findall(candidate))\n",
    "\n",
    "        full_name = []\n",
    "        formed_acronym = []\n",
    "        acr = acronym.strip('(').strip(')')\n",
    "        l = 0\n",
    "        acr_char_counts = Counter(acr)\n",
    "        for ix, c in enumerate(candidate.strip().split()[::-1]):\n",
    "            c_title = c.title()\n",
    "            if c in stops and l > 0:\n",
    "                full_name.append(c)\n",
    "            elif (c_title[0] in acr_char_counts) and (acr_char_counts[c_title[0]] > 0):\n",
    "                l += 1\n",
    "                full_name.append(c_title)\n",
    "                formed_acronym.append(c_title)\n",
    "                acr_char_counts[c_title[0]] -= 1\n",
    "            if l >= len(acr):\n",
    "                detected_acronyms[acr] = \" \".join(full_name[::-1])\n",
    "                break\n",
    "                \n",
    "    return detected_acronyms\n",
    "\n",
    "\n",
    "def detect_acronyms(txt):\n",
    "    '''\n",
    "    This method extracts acronyms from a text document. An acronym can be detected if it's defined with similar form as follows:\n",
    "        National Population and Housing Census (NPHC)\n",
    "        Landscape Approach to Forest Restoration and Conservation (LAFREC)\n",
    "    \n",
    "    Input:\n",
    "        text (str): string type object where acronyms will be detected from.\n",
    "    \n",
    "    Output:\n",
    "        acronyms_map (dict): this is a dictionary that maps the acronym to a set of possible original forms of the acronym.\n",
    "\n",
    "    '''\n",
    "\n",
    "#     acronyms = acronyms_pattern.findall(txt)\n",
    "#     candidates = candidates_pattern.findall(txt)\n",
    "\n",
    "#     detected_acronyms = []\n",
    "\n",
    "#     for a in acronyms:\n",
    "#         if a in candidates:\n",
    "#             ix = candidates.index(a)\n",
    "#             l = len(a) - 2\n",
    "#             detected_acronyms.append((a[1:-1], ' '.join(candidates[ix - l:ix])))\n",
    "\n",
    "    detected_acronyms = extract_acronyms(txt).items()\n",
    "    acronyms_map = {}\n",
    "\n",
    "    for a, n in detected_acronyms:\n",
    "        if a in acronyms_map:\n",
    "            acronyms_map[a].add(n)\n",
    "        else:\n",
    "            acronyms_map[a] = set([n])\n",
    "            \n",
    "    return acronyms_map\n",
    "\n",
    "\n",
    "def detect_acronyms_from_file(fpath):\n",
    "    try:\n",
    "        with open(fpath, 'rb') as fl:\n",
    "            txt = fl.read()\n",
    "            txt = newline_pattern.sub(b' ', txt)\n",
    "            txt = txt.decode('utf-8', errors='ignore')\n",
    "\n",
    "    except UnicodeDecodeError:\n",
    "        with open(fpath, 'rb') as fl:\n",
    "            txt = fl.read()\n",
    "            txt = newline_pattern.sub(b' ', txt)\n",
    "            txt = txt.decode('utf-8', errors='ignore')\n",
    "\n",
    "    return detect_acronyms(txt)\n",
    "\n",
    "\n",
    "def merge_corpora_acronyms_map(acronyms_maps):\n",
    "    merged_corpora_acronyms_map = {}\n",
    "\n",
    "    for acronyms_map in acronyms_maps:\n",
    "        if acronyms_map:\n",
    "            for a, payload in acronyms_map.items():\n",
    "                doc_freq = payload['doc_freq']\n",
    "                prototypes = payload['prototypes']\n",
    "\n",
    "                if a in merged_corpora_acronyms_map:\n",
    "                    for i in prototypes:\n",
    "                        if i in merged_corpora_acronyms_map[a]['prototypes']:\n",
    "                            merged_corpora_acronyms_map[a]['prototypes'] += prototypes[i]\n",
    "                        else:\n",
    "                            merged_corpora_acronyms_map[a]['prototypes'] = prototypes[i]\n",
    "                    \n",
    "                    merged_corpora_acronyms_map[a]['doc_freq'] += doc_freq\n",
    "                else:\n",
    "                    merged_corpora_acronyms_map[a] = {'prototypes': prototypes, 'doc_freq': doc_freq}      \n",
    "    \n",
    "    return merged_corpora_acronyms_map\n",
    "\n",
    "\n",
    "def get_corpus_top_acronym_prototypes(corpus_full_acronyms_map, prototypes=5):\n",
    "    acronyms_popular_prototype = []\n",
    "    columns=['acronym', 'doc_freq', 'full_name', 'percentage']\n",
    "    \n",
    "#     for i in range(1, prototypes + 1):\n",
    "#         columns.append(f'popular_prototype_{i}')\n",
    "#         columns.append(f'doc_proportion_{i}')\n",
    "        \n",
    "    for a, d in corpus_full_acronyms_map.items():\n",
    "        x = pd.Series(corpus_full_acronyms_map[a]['prototypes'])\n",
    "        y = (x / corpus_full_acronyms_map[a]['doc_freq']).sort_values(ascending=False)\n",
    "        doc_freq = corpus_full_acronyms_map[a]['doc_freq']\n",
    "\n",
    "        for ind in range(prototypes):\n",
    "            d = [a, doc_freq]\n",
    "            try:\n",
    "                i = y.index[ind]\n",
    "                v = y[i]\n",
    "            except:\n",
    "                break\n",
    "            d.append(i)\n",
    "            d.append(v)\n",
    "            \n",
    "            acronyms_popular_prototype.append(d)\n",
    "\n",
    "    acronyms_popular_prototype = pd.DataFrame(\n",
    "        acronyms_popular_prototype,\n",
    "        columns=columns\n",
    "    )\n",
    "\n",
    "    acronyms_popular_prototype = acronyms_popular_prototype.sort_values('doc_freq', ascending=False).reset_index(drop='index')\n",
    "    \n",
    "    return acronyms_popular_prototype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AcronymMapper:\n",
    "    def __init__(self, whitelist_file, sim_thresh=0.8):\n",
    "        whitelist_acronyms = pd.read_csv(whitelist_file, header=None)\n",
    "        self.whitelist_acronyms = whitelist_acronyms.rename(columns={0: 'acronym', 1: 'actual'})\n",
    "\n",
    "        self.hvec = HashingVectorizer()\n",
    "        \n",
    "        # Can be in a dataframe but I don't want to stack the vectors every time we infer.\n",
    "        self.acronyms = self.whitelist_acronyms.acronym.values\n",
    "        self.actual = self.whitelist_acronyms.actual.values\n",
    "        self.actual_vectors = self.hvec.transform(self.actual)\n",
    "        self.sim_thresh = sim_thresh\n",
    "        \n",
    "    def get_valid_doc_acronym(self, txt):\n",
    "        \n",
    "        # Detect acronyms present in the document\n",
    "        doc_detected_acronyms = detect_acronyms(txt)\n",
    "        valid_doc_acronyms = {}\n",
    "        invalid_in_doc_to_actual = {}\n",
    "\n",
    "        for i in doc_detected_acronyms:\n",
    "            c = self.whitelist_acronyms[self.whitelist_acronyms.acronym == i]\n",
    "            \n",
    "            if not c.empty:\n",
    "                # For now, this assumes that there will only be one detected full name for an acronym.\n",
    "                valid_candidate_acronyms = doc_detected_acronyms[i]\n",
    "                assert(len(valid_candidate_acronyms) == 1)\n",
    "                \n",
    "                valid_candidate_acronyms_vec = self.hvec.transform(valid_candidate_acronyms)\n",
    "\n",
    "                sims = cosine_similarity(self.actual_vectors, valid_candidate_acronyms_vec)\n",
    "                max_index = sims.argmax()\n",
    "                max_sim = sims[max_index]\n",
    "\n",
    "                if max_sim > self.sim_thresh:\n",
    "                    valid_full = self.actual[max_index]\n",
    "                    valid_doc_acronyms[i] = valid_full\n",
    "                    \n",
    "                    doc_full = list(valid_candidate_acronyms)[0]\n",
    "                    \n",
    "                    if doc_full != valid_full:\n",
    "                        invalid_in_doc_to_actual[doc_full] = valid_full\n",
    "                    \n",
    "        return valid_doc_acronyms, invalid_in_doc_to_actual\n",
    "    \n",
    "    def expand_doc_acronyms(self, txt):\n",
    "        valid_doc_acronyms, invalid_in_doc_to_actual = self.get_valid_doc_acronym(txt)\n",
    "        \n",
    "        for acr in valid_doc_acronyms:\n",
    "            txt = txt.replace(f' ({acr})', ' ')\n",
    "            txt = txt.replace(f' {acr} ', f' {valid_doc_acronyms[acr]} ')\n",
    "        \n",
    "        for invalid_full in invalid_in_doc_to_actual:\n",
    "            txt = txt.replace(invalid_full, invalid_in_doc_to_actual[invalid_full])\n",
    "            \n",
    "        return txt\n",
    "    \n",
    "    def expand_doc_acronyms_in_file(self, fname):\n",
    "        with open(fname) as fl:\n",
    "            txt = fl.read()\n",
    "            \n",
    "        return self.expand_doc_acronyms(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acronyms_pattern = re.compile('(\\([A-Z]{2,}\\))')\n",
    "# candidates_pattern = re.compile('([A-Z][a-z]+|\\([A-Z]{2,}\\))')\n",
    "# newline_pattern = re.compile(b'[\\r\\n]+')\n",
    "\n",
    "\n",
    "# def detect_acronyms(txt):\n",
    "#     '''\n",
    "#     This method extracts acronyms from a text document. An acronym can be detected if it's defined with similar form as follows:\n",
    "#         National Population and Housing Census (NPHC)\n",
    "#         Landscape Approach to Forest Restoration and Conservation (LAFREC)\n",
    "    \n",
    "#     Input:\n",
    "#         text (str): string type object where acronyms will be detected from.\n",
    "    \n",
    "#     Output:\n",
    "#         acronyms_map (dict): this is a dictionary that maps the acronym to a set of possible original forms of the acronym.\n",
    "\n",
    "#     '''\n",
    "\n",
    "#     acronyms = acronyms_pattern.findall(txt)\n",
    "#     candidates = candidates_pattern.findall(txt)\n",
    "\n",
    "#     detected_acronyms = []\n",
    "\n",
    "#     for a in acronyms:\n",
    "#         if a in candidates:\n",
    "#             ix = candidates.index(a)\n",
    "#             l = len(a) - 2\n",
    "#             detected_acronyms.append((a[1:-1], ' '.join(candidates[ix - l:ix])))\n",
    "\n",
    "#     acronyms_map = {}\n",
    "\n",
    "#     for a, n in detected_acronyms:\n",
    "#         if a in acronyms_map:\n",
    "#             acronyms_map[a].add(n)\n",
    "#         else:\n",
    "#             acronyms_map[a] = set([n])\n",
    "            \n",
    "#     return acronyms_map\n",
    "\n",
    "\n",
    "# def detect_acronyms_from_file(fpath):\n",
    "#     try:\n",
    "#         with open(fpath, 'rb') as fl:\n",
    "#             txt = fl.read()\n",
    "#             txt = newline_pattern.sub(b' ', txt)\n",
    "#             txt = txt.decode('utf-8', errors='ignore')\n",
    "\n",
    "#     except UnicodeDecodeError:\n",
    "#         with open(fpath, 'rb') as fl:\n",
    "#             txt = fl.read()\n",
    "#             txt = newline_pattern.sub(b' ', txt)\n",
    "#             txt = txt.decode('utf-8', errors='ignore')\n",
    "\n",
    "#     return detect_acronyms(txt)\n",
    "\n",
    "\n",
    "# def merge_corpora_acronyms_map(acronyms_maps):\n",
    "#     merged_corpora_acronyms_map = {}\n",
    "\n",
    "#     for acronyms_map in acronyms_maps:\n",
    "#         if acronyms_map:\n",
    "#             for a, payload in acronyms_map.items():\n",
    "#                 doc_freq = payload['doc_freq']\n",
    "#                 prototypes = payload['prototypes']\n",
    "\n",
    "#                 if a in merged_corpora_acronyms_map:\n",
    "#                     for i in prototypes:\n",
    "#                         if i in merged_corpora_acronyms_map[a]['prototypes']:\n",
    "#                             merged_corpora_acronyms_map[a]['prototypes'] += prototypes[i]\n",
    "#                         else:\n",
    "#                             merged_corpora_acronyms_map[a]['prototypes'] = prototypes[i]\n",
    "                    \n",
    "#                     merged_corpora_acronyms_map[a]['doc_freq'] += doc_freq\n",
    "#                 else:\n",
    "#                     merged_corpora_acronyms_map[a] = {'prototypes': prototypes, 'doc_freq': doc_freq}      \n",
    "    \n",
    "#     return merged_corpora_acronyms_map\n",
    "\n",
    "\n",
    "# def get_corpus_top_acronym_prototypes(corpus_full_acronyms_map, prototypes=5):\n",
    "#     acronyms_popular_prototype = []\n",
    "#     columns=['acronym', 'doc_freq']\n",
    "    \n",
    "#     for i in range(1, prototypes + 1):\n",
    "#         columns.append(f'popular_prototype_{i}')\n",
    "#         columns.append(f'doc_proportion_{i}')\n",
    "        \n",
    "#     for a, d in corpus_full_acronyms_map.items():\n",
    "#         x = pd.Series(corpus_full_acronyms_map[a]['prototypes'])\n",
    "#         y = (x / corpus_full_acronyms_map[a]['doc_freq']).sort_values(ascending=False)\n",
    "#         doc_freq = corpus_full_acronyms_map[a]['doc_freq']\n",
    "#         d = [a, doc_freq]\n",
    "\n",
    "#         for ind in range(prototypes):\n",
    "#             try:\n",
    "#                 i = y.index[ind]\n",
    "#                 v = y[i]\n",
    "#             except:\n",
    "#                 i = ''\n",
    "#                 v = 0\n",
    "#             d.append(i)\n",
    "#             d.append(v)\n",
    "\n",
    "#         acronyms_popular_prototype.append(d)\n",
    "\n",
    "#     acronyms_popular_prototype = pd.DataFrame(\n",
    "#         acronyms_popular_prototype,\n",
    "#         columns=columns\n",
    "#     )\n",
    "\n",
    "#     acronyms_popular_prototype = acronyms_popular_prototype.sort_values('doc_freq', ascending=False).reset_index(drop='index')\n",
    "    \n",
    "#     return acronyms_popular_prototype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}