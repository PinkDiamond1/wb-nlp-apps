{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import importlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from elasticsearch.helpers import scan\n",
    "\n",
    "from wb_nlp.interfaces import elasticsearch\n",
    "from wb_nlp.interfaces import mongodb\n",
    "from wb_nlp.types import metadata as meta_type\n",
    "from wb_nlp.types import metadata_enums\n",
    "from wb_nlp import dir_manager\n",
    "\n",
    "importlib.reload(meta_type)\n",
    "importlib.reload(metadata_enums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# World Bank Metadata\n",
    "\n",
    "Load the metadata from the API scraper notebook: `wb_nlp/notebooks/scrapers/wb_api_scraper_latest.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fname = f\"wb_metadata-{datetime.now()}.csv\"\n",
    "# fname = \"wb_metadata-2021-03-27 19:17:44.764136.csv\"\n",
    "# wb_metadata = pd.read_csv(dir_manager.get_data_dir(\"corpus\", \"WB\", fname))\n",
    "\n",
    "# collection = mongodb.get_collection(\"test_nlp\", \"docs_metadata\")\n",
    "# errors_list = []\n",
    "# no_url = []\n",
    "# metadata_list = []\n",
    "# for i, row in wb_metadata.iterrows():\n",
    "#     if i % 10000 == 0:\n",
    "#         print(i)\n",
    "\n",
    "#     row[\"_id\"] = row[\"id\"]\n",
    "\n",
    "#     if row[\"url_txt\"] == \"not to be displayed--\":\n",
    "#         print(row[\"id\"])\n",
    "#         no_url.append(row)\n",
    "#         continue\n",
    "\n",
    "#     try:\n",
    "#         meta = json.loads(meta_type.make_metadata_model_from_nlp_schema(row.fillna(\"\")).json())\n",
    "#         meta[\"_id\"] = meta[\"id\"]\n",
    "#         metadata_list.append(meta)\n",
    "#     except:\n",
    "#         errors_list.append(row)\n",
    "\n",
    "# collection.insert_many(metadata_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fname = \"wb_metadata-2021-03-27 19:17:44.764136.csv\"  # without abstract\n",
    "fname = \"wb_metadata-2021-04-06 16:49:38.570800.csv\"  # with abstract\n",
    "wb_metadata = pd.read_csv(dir_manager.get_data_dir(\"corpus\", \"WB\", fname))\n",
    "wb_metadata[\"year\"] = wb_metadata[\"year\"].fillna(np.nan).astype(\"Int64\")\n",
    "collection = mongodb.get_collection(\"test_nlp\", \"docs_metadata\")\n",
    "errors_list = []\n",
    "no_url = []\n",
    "metadata_list = []\n",
    "ids = set()\n",
    "dup_ids = set()\n",
    "\n",
    "for i, row in wb_metadata.iterrows():\n",
    "    if i % 10000 == 0:\n",
    "        print(i, len(ids), len(errors_list))\n",
    "    row[\"_id\"] = row[\"id\"]\n",
    "    if row[\"url_txt\"] == \"not to be displayed--\":\n",
    "        print(row[\"id\"])\n",
    "        no_url.append(row)\n",
    "        continue\n",
    "    try:\n",
    "        meta = json.loads(meta_type.make_metadata_model_from_nlp_schema(row.fillna(\"\").astype(str)).json())\n",
    "        if meta[\"path_original\"] is None or \"TXT_ORIG\" in meta[\"path_original\"]:\n",
    "            meta[\"path_original\"] = meta[\"path_original\"]\n",
    "        else:\n",
    "            p = Path(meta[\"path_original\"])\n",
    "            meta[\"path_original\"] = str(p.parent / \"TXT_ORIG\" / p.name)\n",
    "        meta[\"_id\"] = meta[\"id\"]\n",
    "        if meta[\"_id\"] in ids:\n",
    "            dup_ids.add(meta[\"_id\"])\n",
    "            continue\n",
    "        metadata_list.append(meta)\n",
    "        ids.add(meta[\"_id\"])\n",
    "    except:\n",
    "        errors_list.append(row)\n",
    "\n",
    "print(len(ids))\n",
    "filter_obj = {\"id\": {\"$in\": list(ids)}}\n",
    "del_res = collection.delete_many(filter_obj)\n",
    "ins_res = collection.insert_many(metadata_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_nlp_docs_from_docs_metadata(docs_metadata, ignore_existing):\n",
    "    es = elasticsearch.get_client()\n",
    "    existing_ids = set()\n",
    "    if ignore_existing: \n",
    "        for dobj in scan(es, query={\"query\": {\"match_all\": {}}, \"fields\": [\"_id\"]}, size=5000, index=\"nlp-documents\"):\n",
    "            ids.add(dobj[\"_id\"])\n",
    "    root_path = Path(dir_manager.get_path_from_root())\n",
    "    for ix, data in enumerate(docs_metadata):\n",
    "        if ix and ix % 10000 == 0:\n",
    "            print(ix)\n",
    "        if data[\"_id\"] in existing_ids:\n",
    "            continue\n",
    "        doc_path = root_path / data[\"path_original\"]\n",
    "        if not doc_path.exists():\n",
    "            continue\n",
    "        # create and save and article\n",
    "        nlp_doc = elasticsearch.NLPDoc(meta={'id': data[\"_id\"]}, **data)\n",
    "        with open(doc_path, 'rb') as open_file:\n",
    "            doc = open_file.read().decode('utf-8', errors='ignore')\n",
    "            nlp_doc.body = doc\n",
    "        nlp_doc.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wb_nlp.interfaces import elasticsearch\n",
    "\n",
    "es = elasticsearch.get_client()\n",
    "es.indices.refresh(index_name)\n",
    "es.cat.count(index_name, params={\"format\": \"json\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes and scratch\n",
    "\n",
    "1. Copy raw data from sandbox server to gw1. Then copy to server7. Make sure the files are in the proper format <corpus_id>_<doc_id>.*\n",
    "2. Extract covers of pdf.\n",
    "3. Run filter by language script.\n",
    "4. Set up config for the cleaning and the models. Then load the configs to db.\n",
    "5. Define the scripts to run the models in model_runs/<model_name>/<model_name>-run-<run_number>.sh and also create the cleaning script (run_<model_name__cleaning.sh) for the intended model configuration.\n",
    "6. Execute cleaning and traning: bash run_<model_name__cleaning.sh && bash model_runs/<model_name>/<model_name>-run-<run_number>.sh\n",
    "7. Update permission of the data and models directories (inside the container).\n",
    "8. Tar the needed files and models.\n",
    "9. rsync to gw1, then rsync to sandbox/dev server.\n",
    "10. copy files to local directories, i.e., data/ and models/.\n",
    "11. Load the metadata to db.\n",
    "12. Make a softlink from EN_TXT_ORIG to TXT_ORIG\n",
    "13. Build the document vectors.\n",
    "14. Dump the documents to elasticsearch.\n",
    "15. Update the default model ids in app/app_kcp/src/config.js\n",
    "\n",
    "##############################\n",
    "\n",
    "https://stackoverflow.com/a/44683248\n",
    "\n",
    "```FROM ubuntu:xenial-20170214\n",
    "ARG UNAME=testuser\n",
    "ARG UID=1000\n",
    "ARG GID=1000\n",
    "RUN groupadd -g $GID -o $UNAME\n",
    "RUN useradd -m -u $UID -g $GID -o -s /bin/bash $UNAME\n",
    "USER $UNAME\n",
    "CMD /bin/bash\n",
    "\n",
    "\n",
    "docker build --build-arg UID=$(id -u) --build-arg GID=$(id -g) \\\n",
    "  -f bb.dockerfile -t testimg .\n",
    "```\n",
    "\n",
    "```\n",
    "##############################\n",
    "sudo docker-compose -f docker-compose.yml -f docker-compose.w0lxsnlp01.yml up -d --no-deps --build tika\n",
    "sudo docker-compose -f docker-compose.yml -f docker-compose.w0lxsnlp01.yml up -d --no-deps --build redis\n",
    "sudo docker-compose -f docker-compose.yml -f docker-compose.w0lxsnlp01.yml up -d --no-deps --build mongodb\n",
    "sudo docker-compose -f docker-compose.yml -f docker-compose.w0lxsnlp01.yml up -d --no-deps --build milvus\n",
    "\n",
    "sudo docker-compose -f docker-compose.yml -f docker-compose.w0lxsnlp01.yml up -d --no-deps --build nlp_api\n",
    "sudo docker-compose -f docker-compose.yml -f docker-compose.w0lxsnlp01.yml up -d --no-deps --build app_kcp\n",
    "\n",
    "Step 1: Copy nlp-metadata-wbes2474-20201007.json to data/raw\n",
    "Step 2: Place sample data in data/raw/sample_data/TXT_ORIG\n",
    "\n",
    "sudo docker exec -it wb_nlp_nlp_api_1 /bin/bash\n",
    "\n",
    "# Then run scripts run scripts\n",
    "```\n",
    "\n",
    "### To copy data from sandbox to gw1\n",
    "```\n",
    "###################################\n",
    "\n",
    "python ~/kcp-scripts/copy_raw_data.py\n",
    "\n",
    "# Manually rsync data for ADB\n",
    "rsync -avP w0lxsnlp01:/decfile2/Modeling/NLP/ihsn_scrapers/scrapers/adb/adb_files/full /Documentum/Aivin/corpus/ADB/\n",
    "mv /Documentum/Aivin/corpus/ADB/full /Documentum/Aivin/corpus/ADB/PDF_ORIG\n",
    "\n",
    "```\n",
    "\n",
    "### Generate wikipedia vocab\n",
    "Download data from: https://dumps.wikimedia.org/enwiki/latest/\n",
    "enwiki-latest-pages-articles.xml.bz2: https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2\n",
    "```\n",
    "###################################\n",
    "cd /data/wb536061/wikipedia-word-frequency\n",
    "./gather_wordfreq.py ../wb_nlp/data/external/wikipedia/enwiki-latest-pages-articles.xml.bz2 > wordfreq-enwiki-latest-pages-articles.xml.bz2.txt\n",
    "\n",
    "sudo /usr/local/bin/docker-compose -f docker-compose.yml -f docker-compose.server7.yml up -d --no-deps --build tika\n",
    "sudo /usr/local/bin/docker-compose -f docker-compose.yml -f docker-compose.server7.yml up -d --no-deps --build redis\n",
    "sudo /usr/local/bin/docker-compose -f docker-compose.yml -f docker-compose.server7.yml up -d --no-deps --build mongodb\n",
    "sudo /usr/local/bin/docker-compose -f docker-compose.yml -f docker-compose.server7.yml up -d --no-deps --build milvus\n",
    "\n",
    "sudo /usr/local/bin/docker-compose -f docker-compose.yml -f docker-compose.server7.yml up -d --no-deps --build nlp_api\n",
    "sudo /usr/local/bin/docker-compose -f docker-compose.yml -f docker-compose.server7.yml up -d --no-deps --build app_kcp\n",
    "\n",
    "sudo docker exec -it wb_nlp_nlp_api_1 /bin/bash\n",
    "\n",
    "python -u ./scripts/cleaning/convert_pdf2txt_corpus.py --input-dir ./data/corpus --recursive -vv |& tee ./logs/convert_pdf2txt_corpus.py.log\n",
    "\n",
    "bash run_word2vec_cleaning.sh && bash model_runs/word2vec/word2vec-run-01.sh\n",
    "```\n",
    "\n",
    "### Build a tar of trained models and WB data\n",
    "```\n",
    "# tar -czf KCP_WB_DATA_MODELS.tar.gz data/corpus/WB/COVER data/corpus/WB/wb_metadata-2021-03-27\\ 19\\:17\\:44.764136.csv data/corpus/WB/EN_TXT_ORIG data/corpus/cleaned/229abf370f281efa7c9f3c4ddc20159d/WB models/word2vec models/lda\n",
    "\n",
    "tar -czvf KCP_WB_DATA_MODELS.tar.gz data/corpus/WB/COVER data/corpus/WB/wb_metadata-*.csv data/corpus/WB/EN_TXT_ORIG data/corpus/cleaned/*/WB models/word2vec models/lda models/mallet\n",
    "```\n",
    "\n",
    "### Build Word2vec vector index\n",
    "```\n",
    "###################### W2V\n",
    "from wb_nlp.models import word2vec_base\n",
    "import logging\n",
    "\n",
    "wvec_model = word2vec_base.Word2VecModel(\n",
    "    model_config_id=\"702984027cfedde344961b8b9461bfd3\",\n",
    "    cleaning_config_id=\"229abf370f281efa7c9f3c4ddc20159d\",\n",
    "    model_run_info_id=\"854ae5f9cdda093265212c435d1ddfd4\",\n",
    "    raise_empty_doc_status=False,\n",
    "    log_level=logging.DEBUG)\n",
    "\n",
    "wvec_model.build_doc_vecs(pool_workers=22)\n",
    "```\n",
    "\n",
    "### Build Gensim LDA vector index\n",
    "```\n",
    "###################### LDA\n",
    "from wb_nlp.models import lda_base\n",
    "import logging\n",
    "\n",
    "lda_model = lda_base.LDAModel(\n",
    "    model_config_id=\"43f9977dbee49d7d942f0f9988de4426\",\n",
    "    cleaning_config_id=\"e70ad4f61cf2053e4a15f570c5f82b67\",\n",
    "    model_run_info_id=\"749573cedb4b06aedcba2ec89bc46b46\",\n",
    "    raise_empty_doc_status=False,\n",
    "    log_level=logging.DEBUG)\n",
    "\n",
    "lda_model.build_doc_vecs(pool_workers=22)\n",
    "```\n",
    "\n",
    "### Build WDI vector index\n",
    "```\n",
    "###################### WDI\n",
    "import pandas as pd\n",
    "import logging\n",
    "from wb_nlp import dir_manager\n",
    "from wb_nlp.models import word2vec_base\n",
    "\n",
    "\n",
    "wvec_model = word2vec_base.Word2VecModel(\n",
    "    model_config_id=\"702984027cfedde344961b8b9461bfd3\",\n",
    "    cleaning_config_id=\"229abf370f281efa7c9f3c4ddc20159d\",\n",
    "    model_run_info_id=\"854ae5f9cdda093265212c435d1ddfd4\",\n",
    "    raise_empty_doc_status=False,\n",
    "    log_level=logging.DEBUG)\n",
    "\n",
    "wdi_df = pd.read_csv(dir_manager.get_data_dir(\n",
    "    \"preprocessed\", \"timeseries\", \"wdi_time_series_metadata.csv\"))\n",
    "\n",
    "wdi_df[\"text\"] = wdi_df[\"txt_meta\"].map(wvec_model.clean_text)\n",
    "\n",
    "wdi_df[\"vector\"] = wdi_df.apply(\n",
    "    lambda x: wvec_model.process_doc(x)[\"doc_vec\"], axis=1)\n",
    "\n",
    "wdi_df.to_pickle(\n",
    "    f\"/workspace/models/wdi/wdi_time_series_metadata-{wvec_model.model_id}.pickle\")```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### w0lxsnlp01: model_run_info backup\n",
    "\n",
    "```\n",
    "from wb_nlp.interfaces import mongodb\n",
    "mric = mongodb.get_model_runs_info_collection()\n",
    "\n",
    "In [8]: str(list(mric.find({}))).replace(\"'\", '\"')\n",
    "[\n",
    "  {\n",
    "    \"_id\": \"b93af59ca0150576ebd97d3086ff0324\",\n",
    "    \"model_run_info_id\": \"b93af59ca0150576ebd97d3086ff0324\",\n",
    "    \"model_name\": \"lda\",\n",
    "    \"model_config_id\": \"ef0ab0459e9c28de8657f3c4f5b2cd86\",\n",
    "    \"cleaning_config_id\": \"23f78350192d924e4a8f75278aca0e1c\",\n",
    "    \"processed_corpus_id\": \"f8cc0e4e17e45f3d247383b3aa9813a1_5a80eb483f11c3b899d8cba7237215f9\",\n",
    "    \"description\": \"\",\n",
    "    \"model_file_name\": \"models/lda/b93af59ca0150576ebd97d3086ff0324/lda_b93af59ca0150576ebd97d3086ff0324.bz2\"\n",
    "  },\n",
    "  {\n",
    "    \"_id\": \"41129a1b7ac4187779cc4847b1c6a43d\",\n",
    "    \"model_run_info_id\": \"41129a1b7ac4187779cc4847b1c6a43d\",\n",
    "    \"model_name\": \"word2vec\",\n",
    "    \"model_config_id\": \"702984027cfedde344961b8b9461bfd3\",\n",
    "    \"cleaning_config_id\": \"23f78350192d924e4a8f75278aca0e1c\",\n",
    "    \"processed_corpus_id\": \"f8cc0e4e17e45f3d247383b3aa9813a1\",\n",
    "    \"description\": \"\",\n",
    "    \"model_file_name\": \"models/word2vec/41129a1b7ac4187779cc4847b1c6a43d/word2vec_41129a1b7ac4187779cc4847b1c6a43d.bz2\"\n",
    "  },\n",
    "  {\n",
    "    \"_id\": \"6694f3a38bc16dee91be5ccf4a64b6d8\",\n",
    "    \"model_run_info_id\": \"6694f3a38bc16dee91be5ccf4a64b6d8\",\n",
    "    \"model_name\": \"lda\",\n",
    "    \"model_config_id\": \"ef0ab0459e9c28de8657f3c4f5b2cd86\",\n",
    "    \"cleaning_config_id\": \"23f78350192d924e4a8f75278aca0e1c\",\n",
    "    \"processed_corpus_id\": \"531c1e4f358efbc07b97a58815558c53_5a80eb483f11c3b899d8cba7237215f9\",\n",
    "    \"description\": \"\",\n",
    "    \"model_file_name\": \"models/lda/6694f3a38bc16dee91be5ccf4a64b6d8/lda_6694f3a38bc16dee91be5ccf4a64b6d8.bz2\"\n",
    "  },\n",
    "  {\n",
    "    \"_id\": \"777a9cf47411f6c4932e8941f177f90a\",\n",
    "    \"model_run_info_id\": \"777a9cf47411f6c4932e8941f177f90a\",\n",
    "    \"model_name\": \"word2vec\",\n",
    "    \"model_config_id\": \"702984027cfedde344961b8b9461bfd3\",\n",
    "    \"cleaning_config_id\": \"23f78350192d924e4a8f75278aca0e1c\",\n",
    "    \"processed_corpus_id\": \"531c1e4f358efbc07b97a58815558c53\",\n",
    "    \"description\": \"\",\n",
    "    \"model_file_name\": \"models/word2vec/777a9cf47411f6c4932e8941f177f90a/word2vec_777a9cf47411f6c4932e8941f177f90a.bz2\"\n",
    "  },\n",
    "  {\n",
    "    \"_id\": \"854ae5f9cdda093265212c435d1ddfd4\",\n",
    "    \"model_run_info_id\": \"854ae5f9cdda093265212c435d1ddfd4\",\n",
    "    \"model_name\": \"word2vec\",\n",
    "    \"model_config_id\": \"702984027cfedde344961b8b9461bfd3\",\n",
    "    \"cleaning_config_id\": \"229abf370f281efa7c9f3c4ddc20159d\",\n",
    "    \"processed_corpus_id\": \"d41d8cd98f00b204e9800998ecf8427e\",\n",
    "    \"description\": \"Word2vec D100 W5 N5 SG Full Corpus\",\n",
    "    \"model_file_name\": \"models/word2vec/854ae5f9cdda093265212c435d1ddfd4/word2vec_854ae5f9cdda093265212c435d1ddfd4.bz2\"\n",
    "  },\n",
    "  {\n",
    "    \"_id\": \"749573cedb4b06aedcba2ec89bc46b46\",\n",
    "    \"model_run_info_id\": \"749573cedb4b06aedcba2ec89bc46b46\",\n",
    "    \"model_name\": \"lda\",\n",
    "    \"model_config_id\": \"43f9977dbee49d7d942f0f9988de4426\",\n",
    "    \"cleaning_config_id\": \"e70ad4f61cf2053e4a15f570c5f82b67\",\n",
    "    \"processed_corpus_id\": \"d41d8cd98f00b204e9800998ecf8427e_5a80eb483f11c3b899d8cba7237215f9\",\n",
    "    \"description\": \"LDA topic model with 75 topics - full corpus\",\n",
    "    \"model_file_name\": \"models/lda/749573cedb4b06aedcba2ec89bc46b46/lda_749573cedb4b06aedcba2ec89bc46b46.bz2\"\n",
    "  }\n",
    "]\n",
    "```\n",
    "\n",
    "\n",
    "### w1lxbdatad07: model_run_info backup\n",
    "\n",
    "```\n",
    "[\n",
    "  {\n",
    "    \"_id\": \"854ae5f9cdda093265212c435d1ddfd4\",\n",
    "    \"model_run_info_id\": \"854ae5f9cdda093265212c435d1ddfd4\",\n",
    "    \"model_name\": \"word2vec\",\n",
    "    \"model_config_id\": \"702984027cfedde344961b8b9461bfd3\",\n",
    "    \"cleaning_config_id\": \"229abf370f281efa7c9f3c4ddc20159d\",\n",
    "    \"processed_corpus_id\": \"d41d8cd98f00b204e9800998ecf8427e\",\n",
    "    \"description\": \"Word2vec D100 W5 N5 SG Full Corpus\",\n",
    "    \"model_file_name\": \"models/word2vec/854ae5f9cdda093265212c435d1ddfd4/word2vec_854ae5f9cdda093265212c435d1ddfd4.bz2\"\n",
    "  },\n",
    "  {\n",
    "    \"_id\": \"749573cedb4b06aedcba2ec89bc46b46\",\n",
    "    \"model_run_info_id\": \"749573cedb4b06aedcba2ec89bc46b46\",\n",
    "    \"model_name\": \"lda\",\n",
    "    \"model_config_id\": \"43f9977dbee49d7d942f0f9988de4426\",\n",
    "    \"cleaning_config_id\": \"e70ad4f61cf2053e4a15f570c5f82b67\",\n",
    "    \"processed_corpus_id\": \"d41d8cd98f00b204e9800998ecf8427e_5a80eb483f11c3b899d8cba7237215f9\",\n",
    "    \"description\": \"LDA topic model with 75 topics - full corpus\",\n",
    "    \"model_file_name\": \"models/lda/749573cedb4b06aedcba2ec89bc46b46/lda_749573cedb4b06aedcba2ec89bc46b46.bz2\"\n",
    "  },\n",
    "  {\n",
    "    \"_id\": \"2617e5cf327e60cc8955189110e7f21d\",\n",
    "    \"model_run_info_id\": \"2617e5cf327e60cc8955189110e7f21d\",\n",
    "    \"model_name\": \"word2vec\",\n",
    "    \"model_config_id\": \"68e384251f875204e1263dafa25fade6\",\n",
    "    \"cleaning_config_id\": \"229abf370f281efa7c9f3c4ddc20159d\",\n",
    "    \"processed_corpus_id\": \"d41d8cd98f00b204e9800998ecf8427e\",\n",
    "    \"description\": \"Word2vec D100 W5 N15 I10 SG Full Corpus\",\n",
    "    \"model_file_name\": \"models/word2vec/2617e5cf327e60cc8955189110e7f21d/word2vec_2617e5cf327e60cc8955189110e7f21d.bz2\"\n",
    "  },\n",
    "  {\n",
    "    \"_id\": \"6fd8b418cbe4af7a1b3d24debfafa1ee\",\n",
    "    \"model_run_info_id\": \"6fd8b418cbe4af7a1b3d24debfafa1ee\",\n",
    "    \"model_name\": \"mallet\",\n",
    "    \"model_config_id\": \"5e26b5090bdd91d7aee4e5e89753a33b\",\n",
    "    \"cleaning_config_id\": \"e70ad4f61cf2053e4a15f570c5f82b67\",\n",
    "    \"processed_corpus_id\": \"d41d8cd98f00b204e9800998ecf8427e_5a80eb483f11c3b899d8cba7237215f9\",\n",
    "    \"description\": \"Mallet LDA topic model with 75 topics - full corpus\",\n",
    "    \"model_file_name\": \"models/mallet/6fd8b418cbe4af7a1b3d24debfafa1ee/mallet_6fd8b418cbe4af7a1b3d24debfafa1ee.bz2\"\n",
    "  }\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eids = [obj[\"_id\"] for obj in elasticsearch.scan(elasticsearch.get_client(), query=dict(query=dict(match_all={}), _source=False), size=5000, index=elasticsearch.DOC_INDEX)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wb_nlp",
   "language": "python",
   "name": "wb_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
