{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Install tesserocr: https://pypi.org/project/tesserocr/\n",
    "## Install pdf2imge: pip install yn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install this version of pdfminer to solve the \"TypeError: '<' not supported between instances of 'tuple' and 'int'\" issue.\n",
    "# https://github.com/pdfminer/pdfminer.six/pull/134\n",
    "# !pip install git+git://github.com/pdfminer/pdfminer.six.git@1ea9446bd64ea038551ad3eef55b328791a5bb6b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2image import convert_from_path\n",
    "import tesserocr\n",
    "import glob\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import time\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(filename='./logs/imf-pdf2txt-output-imf-managed-tika-pdfminer-rev2.log', format='%(asctime)s - %(levelname)s - %(message)s', level=logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger('imf-pdf2text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_text_wrapper(index, page, process_output_dict):\n",
    "    process_output_dict[index] = tesserocr.image_to_text(page)\n",
    "\n",
    "\n",
    "def parallel_image_to_text(pages, page_timeout=None, page_delimiter='\\n'):\n",
    "    process_output_manager = multiprocessing.Manager()\n",
    "    process_output_dict = process_output_manager.dict()\n",
    "    \n",
    "    ################\n",
    "    for ix, page in enumerate(pages):\n",
    "        p = multiprocessing.Process(\n",
    "            target=image_to_text_wrapper,\n",
    "            name=\"parallel_image_to_text\",\n",
    "            args=(ix, page, process_output_dict)\n",
    "        )\n",
    "        p.start()\n",
    "\n",
    "        # Maximum of `page_timeout` second parse time per page.\n",
    "        p.join(page_timeout)\n",
    "\n",
    "        # If thread is active\n",
    "        if p.is_alive():\n",
    "            # Terminate if the process is still unfinished after the timeout.\n",
    "            p.terminate()\n",
    "            p.join()\n",
    "\n",
    "    text = page_delimiter.join(\n",
    "        [process_output_dict[i] for i in sorted(process_output_dict.keys())]\n",
    "    )\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that the Tika server is up before proceeding!!\n",
    "\n",
    "`java -jar tika-server-1.18.jar`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['TIKA_SERVER_JAR'] = '0.0.0.0:8000/tika-server-1.18.jar'\n",
    "os.environ['TIKA_PATH'] = '/R/NLP/SCRIPTS/TIKA/'\n",
    "os.environ['TIKA_CLIENT_ONLY'] = 'True'\n",
    "\n",
    "import tika\n",
    "tika.initVM()\n",
    "\n",
    "from tika import parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://stanford.edu/~mgorkove/cgi-bin/rpython_tutorials/Using%20Python%20to%20Convert%20PDFs%20to%20Text%20Files.php#4\n",
    "from io import StringIO\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "import os\n",
    "import sys\n",
    "import getopt\n",
    "import re\n",
    "from joblib import Parallel, delayed\n",
    "import os\n",
    "\n",
    "\n",
    "text_pattern = re.compile('[a-zA-Z]+')\n",
    "FREE_CORES = 2\n",
    "\n",
    "\n",
    "# def timed_process_page(page, pagenum, thread_output_dict, thread_status_dict=None):\n",
    "#     output = StringIO()\n",
    "#     manager = PDFResourceManager()\n",
    "#     converter = TextConverter(manager, output, laparams=LAParams())\n",
    "#     interpreter = PDFPageInterpreter(manager, converter)\n",
    "\n",
    "#     interpreter.process_page(page)\n",
    "#     thread_output_dict[pagenum] = output.getvalue()\n",
    "    \n",
    "# #     converter.close()\n",
    "# #     output.close()\n",
    "    \n",
    "#     if thread_status_dict is not None:\n",
    "#         thread_status_dict[pagenum] = 0\n",
    "\n",
    "\n",
    "# def convert(fname, pages=None, page_delimiter='\\n', page_timeout=4, verbose=False, threads=32):\n",
    "#     # Note: pages that are set horizontally are processed quite slowly by this converter.\n",
    "#     # Example: '../data/pdf/00dcaefff8a5d12bccd5061980394da3401ae713.pdf' pages 34-38.\n",
    "#     # A timed processing is used to skip these pages.\n",
    "    \n",
    "#     if not pages:\n",
    "#         pagenums = set()\n",
    "#     else:\n",
    "#         pagenums = set(pages)\n",
    "\n",
    "#     thread_output_dict = {}\n",
    "#     thread_status_dict = {}\n",
    "    \n",
    "#     output = StringIO()\n",
    "#     manager = PDFResourceManager()\n",
    "#     converter = TextConverter(manager, output, laparams=LAParams())\n",
    "#     interpreter = PDFPageInterpreter(manager, converter)\n",
    "    \n",
    "#     infile = open(fname, 'rb')\n",
    "#     pages = list(PDFPage.get_pages(infile, pagenums))\n",
    "#     logger.error(f'Total pages in doc {fname} :: {len(pages)}')\n",
    "\n",
    "#     pages_rotate = []\n",
    "\n",
    "#     batch = {}\n",
    "#     pagenum = 0\n",
    "\n",
    "#     while True:\n",
    "#         try:\n",
    "#             if len(batch) < threads:\n",
    "#                 page = pages.pop(0)\n",
    "#                 pagenum += 1\n",
    "#                 p = threading.Thread(target=timed_process_page, name=\"timed_process_page\", args=(page, pagenum, thread_output_dict, thread_status_dict))\n",
    "#                 pages_rotate.append(page)\n",
    "#                 p.start()\n",
    "#                 batch[pagenum] = p\n",
    "                \n",
    "#             completed_data = [cpagenum for cpagenum in thread_status_dict.keys()]\n",
    "\n",
    "#             for cpagenum in completed_data:\n",
    "#                 status = process_status_dict.pop(cpagenum)\n",
    "#                 pk = batch.pop(cpagenum)\n",
    "#                 pk.join()\n",
    "                \n",
    "#                 page = pages.pop(0)\n",
    "#                 pagenum += 1\n",
    "#                 p = threading.Thread(target=timed_process_page, name=\"timed_process_page\", args=(page, pagenum, thread_output_dict, thread_status_dict))\n",
    "#                 pages_rotate.append(page)\n",
    "#                 p.start()\n",
    "#                 batch[pagenum] = p\n",
    "#         except:\n",
    "#             keys = list(batch.keys())\n",
    "#             for cpagenum in keys:\n",
    "#                 p = batch.pop(cpagenum)\n",
    "#                 p.join()  # timeout=page_timeout)\n",
    "                \n",
    "#             break\n",
    "\n",
    "#     # Cleanup just in case... \n",
    "#     keys = list(batch.keys())\n",
    "#     for cpagenum in keys:\n",
    "#         p = batch.pop(cpagenum)\n",
    "#         p.join()  # timeout=page_timeout)    \n",
    "\n",
    "\n",
    "#     #################################################################\n",
    "#     ##### Try reprocessing missed pages due to possibly rotated orientation.... \n",
    "#     #################################################################\n",
    "    \n",
    "#     keys = set(thread_output_dict.keys())\n",
    "\n",
    "#     batch = {}\n",
    "#     pagenum = 0\n",
    "\n",
    "#     while True:\n",
    "#         try:\n",
    "#             if len(batch) < threads:\n",
    "#                 page = pages_rotate.pop(0)\n",
    "#                 pagenum += 1\n",
    "#                 if pagenum in keys:\n",
    "#                     continue\n",
    "                    \n",
    "#                 p = threading.Thread(target=timed_process_page, name=\"timed_process_page\", args=(page, pagenum, thread_output_dict, thread_status_dict))\n",
    "#                 p.start()\n",
    "#                 batch[pagenum] = p\n",
    "                \n",
    "#             completed_data = [cpagenum for cpagenum in thread_status_dict.keys()]\n",
    "\n",
    "#             for cpagenum in completed_data:\n",
    "#                 status = process_status_dict.pop(cpagenum)\n",
    "#                 pk = batch.pop(cpagenum)\n",
    "#                 pk.join()\n",
    "                \n",
    "#                 page = pages_rotate.pop(0)\n",
    "#                 pagenum += 1\n",
    "#                 if pagenum in keys:\n",
    "#                     continue\n",
    "\n",
    "#                 p = threading.Thread(target=timed_process_page, name=\"timed_process_page\", args=(page, pagenum, thread_output_dict, thread_status_dict))\n",
    "#                 p.start()\n",
    "#                 batch[pagenum] = p\n",
    "#         except:\n",
    "#             keys = list(batch.keys())\n",
    "#             for cpagenum in keys:\n",
    "#                 p = batch.pop(cpagenum)\n",
    "#                 p.join()  # timeout=page_timeout)\n",
    "                \n",
    "#             break\n",
    "\n",
    "#     # Cleanup just in case... \n",
    "#     keys = list(batch.keys())\n",
    "#     for cpagenum in keys:\n",
    "#         p = batch.pop(cpagenum)\n",
    "#         p.join()  # timeout=page_timeout)    \n",
    "\n",
    "#     infile.close()\n",
    "#     converter.close()\n",
    "#     output.close()\n",
    "    \n",
    "#     if len(thread_output_dict) != pagenum:\n",
    "#         logger.error(f'WARNING: {fname} incomplete. Only {len(thread_output_dict)} / {pagenum} pages parsed!')\n",
    "\n",
    "#     text = page_delimiter.join([thread_output_dict[i] for i in sorted(thread_output_dict.keys())])\n",
    "\n",
    "#     return text\n",
    "\n",
    "\n",
    "\n",
    "def timed_process_page(page, pagenum, process_output_dict):\n",
    "    output = StringIO()\n",
    "    manager = PDFResourceManager()\n",
    "    converter = TextConverter(manager, output, laparams=LAParams())\n",
    "    interpreter = PDFPageInterpreter(manager, converter)\n",
    "\n",
    "    interpreter.process_page(page)\n",
    "    process_output_dict[pagenum] = output.getvalue()\n",
    "\n",
    "    \n",
    "def convert_tika(fname):\n",
    "    parsedPDF = parser.from_file(fname)\n",
    "    return parsedPDF.get('content', '')\n",
    "\n",
    "\n",
    "def convert(fname, pages=None, page_delimiter='\\n', page_timeout=4, verbose=False):\n",
    "    # Note: pages that are set horizontally are processed quite slowly by this converter.\n",
    "    # Example: '../data/pdf/00dcaefff8a5d12bccd5061980394da3401ae713.pdf' pages 34-38.\n",
    "    # A timed processing is used to skip these pages.\n",
    "    \n",
    "    if not pages:\n",
    "        pagenums = set()\n",
    "    else:\n",
    "        pagenums = set(pages)\n",
    "\n",
    "    process_output_manager = multiprocessing.Manager()\n",
    "    process_output_dict = process_output_manager.dict()\n",
    "\n",
    "    output = StringIO()\n",
    "    manager = PDFResourceManager()\n",
    "    converter = TextConverter(manager, output, laparams=LAParams())\n",
    "    interpreter = PDFPageInterpreter(manager, converter)\n",
    "\n",
    "    processes = []\n",
    "    \n",
    "    infile = open(fname, 'rb')\n",
    "    pages = list(PDFPage.get_pages(infile, pagenums))\n",
    "    logger.error(f'Total pages in doc {fname} :: {len(pages)}')\n",
    "    for pagenum, page in enumerate(pages, 1):\n",
    "        if verbose:\n",
    "            print(f'{fname}: page {pagenum}')\n",
    "        # p = multiprocessing.Process(target=timed_process_page, name=\"timed_process_page\", args=(interpreter, page, pagenum, process_output_dict, output))\n",
    "        p = threading.Thread(target=timed_process_page, name=\"timed_process_page\", args=(page, pagenum, process_output_dict))\n",
    "        p.start()\n",
    "        \n",
    "        processes.append(p)\n",
    "\n",
    "    for p in processes:\n",
    "        # Maximum of `page_timeout` second parse time per page.\n",
    "        p.join(page_timeout)\n",
    "\n",
    "#         # If thread is active\n",
    "#         if p.is_alive():\n",
    "#             # Terminate if the process is still unfinished after the timeout.\n",
    "#             p.terminate()\n",
    "#             p.join()\n",
    "\n",
    "    processes = []\n",
    "    \n",
    "    keys = set(process_output_dict.keys())\n",
    "\n",
    "    # Rotate to recover the landscape pages that are quite slow to parse when not properly oriented.\n",
    "    for pagenum, page in enumerate(pages, 1):\n",
    "        if pagenum in keys:\n",
    "            continue\n",
    "\n",
    "        page.rotate = 90\n",
    "        p = threading.Thread(target=timed_process_page, name=\"timed_process_page\", args=(page, pagenum, process_output_dict))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "        \n",
    "    for p in processes:\n",
    "        p.join(page_timeout)\n",
    "    \n",
    "    infile.close()\n",
    "    converter.close()\n",
    "    output.close()\n",
    "    \n",
    "    if len(process_output_dict) != pagenum:\n",
    "        logger.error(f'WARNING: {fname} incomplete. Only {len(process_output_dict)} / {pagenum} pages parsed!')\n",
    "\n",
    "    text = page_delimiter.join([process_output_dict[i] for i in sorted(process_output_dict.keys())])\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def convert_image(fname, thread_count=20, page_delimiter='\\n', dpi=200, fmt='ppm', parallel_im2txt=True, cpu_count=None):\n",
    "    pages = convert_from_path(fname, fmt=fmt, dpi=dpi, thread_count=thread_count)\n",
    "\n",
    "    # print('Completed comvert_from_path...')\n",
    "    cpu_count = cpu_count if cpu_count is not None else (os.cpu_count() - FREE_CORES)\n",
    "    doc = ''\n",
    "    \n",
    "    if not parallel_im2txt:\n",
    "        for p in pages:\n",
    "            doc += tesserocr.image_to_text(p) + page_delimiter\n",
    "    else:\n",
    "        doc = parallel_image_to_text(pages, page_timeout=10, page_delimiter='\\n')\n",
    "#         page_txt = Parallel(n_jobs=cpu_count, backend='threading')(\n",
    "#             delayed(image_to_text_wrapper)(ix, p) for ix, p in enumerate(pages)\n",
    "#         )\n",
    "        \n",
    "#         page_txt = [p for ix, p in sorted(page_txt)]\n",
    "#         doc = page_delimiter.join(page_txt)\n",
    "        \n",
    "    return doc\n",
    "\n",
    "\n",
    "def auto_convert(fname, min_valid_chars=1000, thread_count=20, use_tika=True):\n",
    "    # Convider 1000 chars or ~1kb as the acceptable file size. (min_valid_chars=1000)\n",
    "    pdf_type = 'text'\n",
    "    \n",
    "    if use_tika:\n",
    "        try:\n",
    "            text = convert_tika(fname)\n",
    "            pdf_type = 'text-tika'\n",
    "        except:\n",
    "            # Fallback to pdfminer if tika fails.\n",
    "            logger.error(f'Method tika raised an exception for {fname}! Using pdfminer...')\n",
    "            text = convert(fname)\n",
    "            pdf_type = 'text-pdfminer'\n",
    "    else:\n",
    "        text = convert(fname)\n",
    "        pdf_type = 'text-pdfminer'\n",
    "\n",
    "    if len(text_pattern.findall(text)) < min_valid_chars:\n",
    "        text = convert_image(fname, thread_count=thread_count, parallel_im2txt=False)\n",
    "        pdf_type = 'image'\n",
    "    \n",
    "    return text, pdf_type\n",
    "\n",
    "\n",
    "def convert_and_dump(input_fname, txt_dir, min_valid_chars=1000, replace_existing=False):\n",
    "    logger.error(f'Processing {os.path.basename(input_fname)}...')\n",
    "    output_fname = os.path.join(txt_dir, 'imf_' + os.path.basename(input_fname).replace('.pdf', '') + '.txt')\n",
    "    \n",
    "    if os.path.isfile(output_fname) and (not replace_existing):\n",
    "        logger.error(f'Document {os.path.basename(input_fname)} already processed...')\n",
    "        return 0\n",
    "    \n",
    "    try:\n",
    "        text, pdf_type = auto_convert(input_fname, min_valid_chars=min_valid_chars)\n",
    "    except Exception as e:\n",
    "        logger.error(f'Error processing ({e.args[0]}): {os.path.basename(input_fname)}...')\n",
    "        return 1\n",
    "    \n",
    "    with open(output_fname, 'w') as fl:\n",
    "        fl.write(text)\n",
    "        \n",
    "    logger.error(f'Successfully converted: {os.path.basename(input_fname)} from {pdf_type} pdf.')\n",
    "    \n",
    "    return 0\n",
    "    \n",
    "\n",
    "def convert_pdfs_in_dir(pdf_dir, txt_dir, parallel=True, threads=None):\n",
    "    if not parallel:\n",
    "        for input_fname in glob.iglob(os.path.join(pdf_dir, '*.pdf')):\n",
    "            start = time.time()\n",
    "\n",
    "            convert_and_dump(input_fname, txt_dir)\n",
    "\n",
    "            print(f'Finished processing {fname} with {len(text)} characters after {time.time() - start:0.2f}')\n",
    "\n",
    "    else:\n",
    "        cpu_counts = threads if threads is not None else (os.cpu_count() - FREE_CORES)\n",
    "        \n",
    "        Parallel(n_jobs=cpu_counts, backend='threading')(\n",
    "            delayed(convert_and_dump)(input_fname, txt_dir) for input_fname in glob.iglob(\n",
    "                os.path.join(pdf_dir, '*.pdf')\n",
    "            )\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_dir = '/R/Corpus/IMF/pdf/'\n",
    "txt_dir = '/R/Corpus/IMF/tika_imf_pdf_txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1481"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import os\n",
    "\n",
    "fname = '/R/Corpus/IMF/pdf_txt/f058c865ecdde4845f176c860f0e7c6f46bae242.pdf.txt' # 000dc1fcdc7b36b8014131971909b6b4952eb517.pdf'\n",
    "os.path.getsize(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def managed_convert_and_dump(input_fname, txt_dir, process_status_dict, min_valid_chars=1000, replace_existing=False):\n",
    "    convert_and_dump(input_fname, txt_dir, min_valid_chars, replace_existing)\n",
    "    process_status_dict[input_fname] = 0\n",
    "    \n",
    "    \n",
    "def is_outfile_present(fname, txt_dir):\n",
    "    ofname = os.path.join(txt_dir, 'imf_' + os.path.basename(fname).replace('.pdf', '') + '.txt')\n",
    "    return os.path.isfile(ofname)\n",
    "    \n",
    "    \n",
    "def managed_dynamic_main(pdf_dir, txt_dir, workers=24, file_ids=None, min_valid_chars=1000, replace_existing=False):\n",
    "    process_status_manager = multiprocessing.Manager()\n",
    "    process_status_dict = process_status_manager.dict()\n",
    "\n",
    "    print('Sorting files by size...')\n",
    "    \n",
    "    if file_ids is not None:\n",
    "        fnames = [os.path.join(pdf_dir, f\"{fid.replace('imf_', '')}.pdf\") for fid in file_ids]\n",
    "    else:\n",
    "        fnames = glob.iglob(os.path.join(pdf_dir, '*.pdf'))\n",
    "    fnames = sorted(fnames, key=lambda x: os.path.getsize(x))\n",
    "    print('Start processing pdfs...')\n",
    "    \n",
    "    batch = {}\n",
    "    while True:\n",
    "        try:\n",
    "            if len(batch) < workers:\n",
    "                fname = fnames.pop(0)\n",
    "                if is_outfile_present(fname, txt_dir) and (not replace_existing):\n",
    "                    logger.error(f'Document {os.path.basename(fname)} already processed...')\n",
    "                    continue\n",
    "                    \n",
    "                p = multiprocessing.Process(target=managed_convert_and_dump, args=(fname, txt_dir, process_status_dict, min_valid_chars, replace_existing))\n",
    "                p.start()\n",
    "                batch[fname] = p\n",
    "                \n",
    "            completed_data = [cfname for cfname in process_status_dict.keys()]\n",
    "\n",
    "            for cfname in completed_data:\n",
    "                status = process_status_dict.pop(cfname)\n",
    "                pk = batch.pop(cfname)\n",
    "                pk.join()\n",
    "                \n",
    "                if pk.is_alive():\n",
    "                    pk.terminate()\n",
    "                    pk.join()\n",
    "\n",
    "                fname = fnames.pop(0)\n",
    "                if is_outfile_present(fname, txt_dir) and (not replace_existing):\n",
    "                    logger.error(f'Document {os.path.basename(fname)} already processed...')\n",
    "                    continue\n",
    "                    \n",
    "                p = multiprocessing.Process(target=managed_convert_and_dump, args=(fname, txt_dir, process_status_dict, min_valid_chars, replace_existing))\n",
    "                p.start()\n",
    "                batch[fname] = p\n",
    "        except:\n",
    "            for fname in batch:\n",
    "                p = batch[fname]\n",
    "                p.join()\n",
    "                \n",
    "                if p.is_alive():\n",
    "                    p.terminate()\n",
    "                    p.join()\n",
    "   \n",
    "            break\n",
    "\n",
    "    # Cleanup just in case... \n",
    "    for fname in batch:\n",
    "        p = batch[fname]\n",
    "        p.join()\n",
    "\n",
    "        if p.is_alive():\n",
    "            p.terminate()\n",
    "            p.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorting files by size...\n",
      "Start processing pdfs...\n",
      "CPU times: user 2.05 s, sys: 1.43 s, total: 3.48 s\n",
      "Wall time: 55.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Not well parsed by commandline tika?: ccb71ccee9e1cdbd4310591a12f9b7227e1199c2 :(\n",
    "# Not well parsed by tika and tesseract: b315d239370d9b6780982fffd4bc924895ff83f9.pdf :(\n",
    "if __name__ == '__main__':\n",
    "    managed_dynamic_main(pdf_dir, txt_dir, workers=48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging.basicConfig(filename='./logs/imf-pdf2txt-output-imf-managed-tika-pdfminer-review.log', format='%(asctime)s - %(levelname)s - %(message)s', level=logging.ERROR)\n",
    "# logger = logging.getLogger('imf-review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "359\n",
      "CPU times: user 385 ms, sys: 462 ms, total: 847 ms\n",
      "Wall time: 24.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "MIN_VALID_CHARS = 11000\n",
    "\n",
    "file_ids = []\n",
    "\n",
    "for forig_name in glob.iglob('../CORPUS/IMF/TXT_ORIG/imf_*.txt'):\n",
    "    if os.path.getsize(forig_name) < MIN_VALID_CHARS:\n",
    "        file_ids.append(os.path.basename(forig_name).replace('.txt', ''))\n",
    "# os.path.getsize('../CORPUS/IMF/TXT_ORIG/imf_95193976af5c2fc556fd99f6dab80070f938b3fe.txt')\n",
    "\n",
    "print(len(file_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorting files by size...\n",
      "Start processing pdfs...\n",
      "CPU times: user 3min 17s, sys: 42.5 s, total: 3min 59s\n",
      "Wall time: 56min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Rerun for possibly false-negative images\n",
    "# Not well parsed by commandline tika?: ccb71ccee9e1cdbd4310591a12f9b7227e1199c2 :(\n",
    "# Not well parsed by tika and tesseract: b315d239370d9b6780982fffd4bc924895ff83f9.pdf :(\n",
    "if __name__ == '__main__':\n",
    "    managed_dynamic_main(pdf_dir, txt_dir, workers=48, file_ids=file_ids, min_valid_chars=MIN_VALID_CHARS, replace_existing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    tasks = []\n",
    "    workers = 96\n",
    "    print('Sorting files by size...')\n",
    "    fnames = sorted(glob.iglob(os.path.join(pdf_dir, '*.pdf')), key=lambda x: os.path.getsize(x))\n",
    "    print('Start processing pdfs...')\n",
    "    \n",
    "    for fname in fnames:\n",
    "        ofname = os.path.join(txt_dir, 'imf_' + os.path.basename(fname).replace('.pdf', '') + '.txt')\n",
    "        if os.path.isfile(ofname):\n",
    "            logger.error(f'Document {os.path.basename(fname)} already processed...')\n",
    "            continue\n",
    "            \n",
    "        if len(tasks) < workers:\n",
    "            tasks.append(fname)\n",
    "        else:\n",
    "            processes = []\n",
    "            for input_fname in tasks:\n",
    "                p = multiprocessing.Process(target=convert_and_dump, args=(input_fname, txt_dir))\n",
    "                p.start()\n",
    "                processes.append(p)\n",
    "\n",
    "            for p in processes:\n",
    "                p.join()\n",
    "                \n",
    "            tasks = []\n",
    "    if tasks:\n",
    "        processes = []\n",
    "        for input_fname in tasks:\n",
    "            p = multiprocessing.Process(target=convert_and_dump, args=(input_fname, txt_dir))\n",
    "            p.start()\n",
    "            processes.append(p)\n",
    "\n",
    "        for p in processes:\n",
    "            p.join()\n",
    "\n",
    "        tasks = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rerun for originally faulty parsing\n",
    "\n",
    "### Original size > 500kb and parsed size < 50kb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorting files by size...\n",
      "Start processing pdfs...\n"
     ]
    }
   ],
   "source": [
    "max_parsed_size = 50000\n",
    "min_original_size = 500000\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tasks = []\n",
    "    workers = 96\n",
    "    print('Sorting files by size...')\n",
    "    fnames = sorted(glob.iglob(os.path.join(txt_dir, '*.pdf.txt')), key=lambda x: os.path.getsize(x))\n",
    "    print('Start processing pdfs...')\n",
    "    \n",
    "    for fname in fnames:\n",
    "        ofname = fname\n",
    "        fname = ofname.replace('.txt', '')\n",
    "        fname = os.path.join(pdf_dir, os.path.basename(fname))\n",
    "        \n",
    "        fsize = os.path.getsize(fname)\n",
    "        if os.path.isfile(ofname):\n",
    "            ofsize = os.path.getsize(ofname)\n",
    "            \n",
    "            if (fsize > min_original_size) and (ofsize < max_parsed_size):\n",
    "                pass\n",
    "            else:\n",
    "                logger.error(f'Document {os.path.basename(fname)} already processed...')\n",
    "                continue\n",
    "            \n",
    "        if len(tasks) < workers:\n",
    "            tasks.append(fname)\n",
    "        else:\n",
    "            processes = []\n",
    "            for input_fname in tasks:\n",
    "                p = multiprocessing.Process(target=convert_and_dump, args=(input_fname, txt_dir))\n",
    "                p.start()\n",
    "                processes.append(p)\n",
    "\n",
    "            for p in processes:\n",
    "                p.join()\n",
    "                \n",
    "            tasks = []\n",
    "    if tasks:\n",
    "        processes = []\n",
    "        for input_fname in tasks:\n",
    "            p = multiprocessing.Process(target=convert_and_dump, args=(input_fname, txt_dir))\n",
    "            p.start()\n",
    "            processes.append(p)\n",
    "\n",
    "        for p in processes:\n",
    "            p.join()\n",
    "\n",
    "        tasks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13954 9/14/2018 3:36 AM\n",
    "# 14020 9/14/2018 6:46 AM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f = 'e1c91148692198e96e7020b9e5919aa935308169.pdf'\n",
    "# f = '6497ba5f6ea18ca5c3ded8ef947510f562a3db89.pdf'\n",
    "f = '4c421ce6c630e5e77550d319e5cbd71f1c1c4842.pdf'\n",
    "fname = os.path.join('/R/Corpus/IMF/pdf/', f)  # 000dc1fcdc7b36b8014131971909b6b4952eb517.pdf'\n",
    "\n",
    "# t = auto_convert(fname)\n",
    "# t = convert(fname, page_timeout=10, verbose=True)  # , threads=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = auto_convert(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7977"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@2003 International Monetary F und March 2003\n",
      "IMF Country Repon No. 03/85\n",
      "\n",
      "Uganda: Report on Observance of Standards and Codes—\n",
      "Fiscal Transparency Module—Update\n",
      "\n",
      "This update to the Report on the Observance of Standards and Codes on Fiscal Transparency for\n",
      "Uganda was prepaIed by a staff team of the Intemational Monetary Fund as background documentation\n",
      "for the periodic consultation with the member country It is based on the information available at the time\n",
      "it was completed on J anuary 24, 2003. The views expressed in this docmnent are those of the staff team\n",
      "and do not necessan'ly reﬂect the views of the govemmem of Uganda or the Executive Board of the IMF.\n",
      "\n",
      "The policy of publication of staff reports and other docmnents by the IMF allows for the deletion of\n",
      "market-sensitive information.\n",
      "\n",
      "To assist the IMF in evaluating the publication policy, reader comments are invited and may be\n",
      "\n",
      "sent by e-mail to Eublicationgolic!@imf.urg.\n",
      "\n",
      "Copies of this report are available to the public from\n",
      "\n",
      "International Monetary Fund 0 Publication Services\n",
      "700 1ch Street. NW. 0 Washington, D.C. 20431\n",
      "Telephone: (202) 623 7430 I Telefax: (202) 623 7201\n",
      "\n",
      "E—mail: gublications@imf.org 0 Internet: http://www.imf.org\n",
      "\n",
      "International Monetary Fund\n",
      "Washington, D.C.\n",
      "\n",
      "\n",
      "1.\n",
      "\n",
      "INTERNATIONAL MONETARY FUND\n",
      "UGANDA\n",
      "\n",
      "Update to the Report on the Observance of Standards and Codes (ROSC)\n",
      "Fiscal Transparency Module\n",
      "\n",
      "Prepared by the African Department on the Basis of Infomiation\n",
      "Provided by the U gandan Authorities\n",
      "\n",
      "January 24, 2003\n",
      "\n",
      "I. INTRODUCTION\n",
      "\n",
      "The ﬁrst ROSC for Uganda was issued in August 1999.1 During the 2002 Atticle IV\n",
      "\n",
      "consultation mission, IMF staff reviewed developments in Uganda’s obs ervance of the ﬁscal\n",
      "transparency practices assessed in 1999, with a view to updating changes in current practices,\n",
      "reviewing progress in implementing the earlier ROSC’s recommendations, and identifying\n",
      "developments relevant for U ganda’s future observance of good ﬁscal transparency practices.\n",
      "\n",
      "2.\n",
      "\n",
      "11. DESCRIPTION OF PROGRESS AND RECENT DEVELOPMENTS\n",
      "\n",
      "Since the 1999 ROSC, Uganda has made signiﬁcant progress in enhancing\n",
      "\n",
      "transparency practices in the ﬁscal area. Actions have been taken under each of the four areas\n",
      "underlying the ﬁscal transparency code:\n",
      "\n",
      "Clarity of roles and responsibilities. The Ugandan authorities have made a number\n",
      "of improvements. They have inter alia (i) divested public enterprises, thereby\n",
      "reducing the scope for conducting off—budget quasi-ﬁscal operations; (ii) compiled\n",
      "statistics of line ministn'es’ revenue, bringing this revenue under the control of the\n",
      "Treasury; (iii) extended the budgeting framework to cover district and local\n",
      "government budget processes; and (iv) eliminated the possibility of government\n",
      "granting discretionary tax exemptions by modifying the income, customs, and value-\n",
      "added tax laws. Despite these improvements, at large number of recommendations\n",
      "contained in the original ROSC regarding broadening the coverage of the budget and\n",
      "including local governments in the ﬁscal management system have not been\n",
      "addressed.\n",
      "\n",
      "1 The “Experimental IMF Report on Observance of Standards and Codes: Uganda” is available at the\n",
      "IMF website: http://WWW.imf.org/extemal/np/rosc/rosc.asp. This report covered six areas: data\n",
      "dissemination, ﬁscal transparency, banking supervision, transparency of monetary policies, securities\n",
      "market regulation, and insurance market,\n",
      "\n",
      "\n",
      "0 Public availability of information. The Ugandan authorities have (i) produced\n",
      "annual, semiannual, and quarterly reports on the outtum 0f the central government, as\n",
      "well as monthly reports on the central government’s revenue outtum now available on\n",
      "the government’s website; and (ii) compiled and aggregated ﬁnal annual accounts for\n",
      "local governments. However, recommendations to include statements on the use and\n",
      "estimates of the cost of government guarantees, tax expenditures, and quasi-ﬁscal\n",
      "activities in the budget documentation, and to report on consolidated government\n",
      "accounts remain relevant.\n",
      "\n",
      "- Open budget preparation, execution, and reporting. The authorities have\n",
      "(i) increased the participation of legislature in ﬁscal management, particularly the\n",
      "budget, through the enactment of the 2001 Budget Act which allows the parliament to\n",
      "contribute to the budget during the preparation process; (ii) started piloting output-\n",
      "oriented budgeting for some of the maj or spending ministries, including education\n",
      "and water; (iii) introduced a commitment control system (CC S) for nonwage recur-\n",
      "rent and development expenditures that has greatly reduced, although not eliminated,\n",
      "the accumulation of new domestic arrears in these areas; and (iv) reformed the central\n",
      "tender board and passed a new procurement law, which has shifted procurement to\n",
      "the ministerial level in an attempt to improve budget execution and Value-for-money\n",
      "spending. Little progress, however, has been made on a number of recommendations\n",
      "in the original ROSC to improve the content of the budget document and the budget\n",
      "classiﬁcation, strengthen the monitoring of local expenditures, reconcile accounting\n",
      "data With budget appropriations and bank accounts, and fully enforce safeguards for\n",
      "monitoring expenditures and controlling arrears.\n",
      "\n",
      "- Independent assurances of integrity. Uganda has enhanced the technical capacity of\n",
      "the auditin g functions of the government by increasing budget resources and hirin g\n",
      "external technical experts. However, the effectiveness of the involved units is not\n",
      "regularly monitored, as recommended in the original ROSC.\n",
      "\n",
      "3. Since the completion of the 1999 ROSC, three lssues have emerged that are\n",
      "critical for Uganda’ s observance of ﬁscal transparency practices First, the extensive use\n",
      "of supplementary appropriations is undermining the budget process as an instrument for\n",
      "effective expenditure planning. Second, the assessment of the accumulated stock of domestic\n",
      "arrears is problematic, with the stock of veriﬁed arrears frequently changing. Third, the\n",
      "central government contingent liabilities are not fully reported, and their total amount is\n",
      "\n",
      "unknown.\n",
      "III. IMF STAFF COMMENTARY\n",
      "\n",
      "4. The authorities should be commended for addressing some of the critical areas where\n",
      "ﬁscal transparency was lacking in the 1999 ﬁscal ROSC. These include eliminating many\n",
      "sources of quasi—ﬁscal operations (through pn'vatization), enforcing regulations needed to\n",
      "reduce new budget meats, and discontinuing the practice of granting discretionary tax\n",
      "exemptions. However, a large number of the recommendations made in the 1999 ROSC on\n",
      "\n",
      "\n",
      "ﬁscal transparency remain valid, particularly those in the area of budget execution and\n",
      "reporting, both at the central and the local government levels Where progIess has been\n",
      "extremely limited. Furthermore, three additional issues have emerged that can adversely\n",
      "affect the Uganda’s ability to observe good ﬁscal transparency practices in the future,\n",
      "including the use of supplementary appropriations, the assessment of the stock of domestic\n",
      "arrears, and the level of the central governmenfs contingent liabilities. First, the regular use\n",
      "of supplementary appropriations suggests that urgent attention be given to the enactment of a\n",
      "new public ﬁnance bill to legislatively regulate and limit the use of supplementary\n",
      "appropriations by the executive. Second, the problem of assessing the current stock of arrears\n",
      "requires tighter requirements for the presentation of an anear and stricter enforcement in the\n",
      "arrears’ veriﬁcation process. Finally, the authon'ties should recognize that failure to monitor\n",
      "and control contingent liabilities can quickly escalate and threaten ﬁscal sustainability. To\n",
      "monitor the accumulation of contingent liabilities, an inventory should be completed to\n",
      "create a central database on contingent liabilities and the database should be regularly\n",
      "updated. A clear public policy on govemment guarantees should be developed that ideally\n",
      "tests the authority to grant a government guarantee solely with the Minister of Finance.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(t[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48575"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_image(fname, thread_count=20, page_delimiter='\\n', dpi=200, fmt='ppm', parallel_im2txt=True, cpu_count=None):\n",
    "    pages = convert_from_path(fname, fmt=fmt, dpi=dpi, thread_count=thread_count)\n",
    "\n",
    "    # print('Completed comvert_from_path...')\n",
    "    cpu_count = cpu_count if cpu_count is not None else (os.cpu_count() - FREE_CORES)\n",
    "    doc = ''\n",
    "    \n",
    "    if not parallel_im2txt:\n",
    "        for p in pages:\n",
    "            doc += tesserocr.image_to_text(p) + page_delimiter\n",
    "    else:\n",
    "        doc = parallel_image_to_text(pages, page_timeout=10, page_delimiter='\\n')\n",
    "#         page_txt = Parallel(n_jobs=cpu_count, backend='threading')(\n",
    "#             delayed(image_to_text_wrapper)(ix, p) for ix, p in enumerate(pages)\n",
    "#         )\n",
    "        \n",
    "#         page_txt = [p for ix, p in sorted(page_txt)]\n",
    "#         doc = page_delimiter.join(page_txt)\n",
    "        \n",
    "    return doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.6 s, sys: 91 ms, total: 26.7 s\n",
      "Wall time: 27.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "t = convert_image(fname, thread_count=100, parallel_im2txt=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.4 s, sys: 138 ms, total: 26.6 s\n",
      "Wall time: 27.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "t = convert_image(fname, parallel_im2txt=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 270 ms, sys: 113 ms, total: 383 ms\n",
      "Wall time: 27.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "t = convert_image(fname, parallel_im2txt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42594"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}