{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.data.path.append(\"/R/nltk_data\")\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "import gc\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecModel:\n",
    "    def __init__(\n",
    "        self,\n",
    "        corpus_id,\n",
    "        model_id,\n",
    "        doc_df=None,\n",
    "        dim=100,\n",
    "        window=10,\n",
    "        negative=10,\n",
    "        min_count=5,\n",
    "        sg=1,\n",
    "        workers=4,\n",
    "        model_path='./',\n",
    "        optimize_interval=0,\n",
    "        iter=10,\n",
    "        raise_empty_doc_status=True\n",
    "    ):\n",
    "        self.corpus_id = corpus_id\n",
    "        self.model_id = model_id\n",
    "        self.model = None\n",
    "        self.workers = workers\n",
    "        self.iter = iter\n",
    "        self.dim = dim\n",
    "        self.window = window\n",
    "        self.negative = negative\n",
    "        self.min_count = min_count\n",
    "        self.sg = sg\n",
    "        self.raise_empty_doc_status = raise_empty_doc_status\n",
    "\n",
    "        self.model_path = model_path\n",
    "        \n",
    "        self.model_data_id = f'{self.corpus_id.lower()}-w2vec_{self.model_id}'\n",
    "        self.vecs_path = os.path.join(self.model_path, f'{self.model_data_id}.hdf')\n",
    "        self.model_path = os.path.join(self.model_path, f'{self.model_data_id}.mm')\n",
    "\n",
    "        self.vecs = None\n",
    "        self.docs = doc_df.copy() if doc_df is not None else doc_df\n",
    "\n",
    "    def clear(self):\n",
    "        del(self.vecs)\n",
    "        del(self.docs)\n",
    "        del(self.model)\n",
    "\n",
    "        self.vecs = None\n",
    "        self.docs = None\n",
    "        self.model = None\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "    def save(self):\n",
    "        self.save_model()\n",
    "        self.save_vecs()\n",
    "\n",
    "    def load(self):\n",
    "        self.load_vecs()\n",
    "        self.load_model()\n",
    "\n",
    "        # Make sure that the dimensionality uses the one in the trained model.\n",
    "        if self.dim != self.model.wv.vector_size:\n",
    "            print('Warning: dimension declared is not aligned with loaded model. Using loaded model dim.')\n",
    "            self.dim = self.model.wv.vector_size\n",
    "\n",
    "    def save_vecs(self):\n",
    "        self.check_wvecs()\n",
    "        self.vecs.to_hdf(self.vecs_path, 'vecs')\n",
    "\n",
    "    def save_model(self):\n",
    "        self.check_model()\n",
    "        self.model.save(self.model_path)\n",
    "\n",
    "    def load_model(self):\n",
    "        self.model = Word2Vec.load(self.model_path)\n",
    "\n",
    "    def load_vecs(self):\n",
    "        self.vecs = pd.read_hdf(self.vecs_path, 'vecs')\n",
    "\n",
    "    def train_model(self):\n",
    "        if self.model is None:\n",
    "            self.model = Word2Vec(\n",
    "                self.docs.text.str.split().values,\n",
    "                size=self.dim, min_count=self.min_count,\n",
    "                workers=self.workers, iter=self.iter,\n",
    "                window=self.window, negative=self.negative, sg=self.sg\n",
    "            )\n",
    "        else:\n",
    "            print('Warning: Model already trained. Not doing anything...')\n",
    "                \n",
    "    def transform_doc(self, document):\n",
    "        # document: cleaned string\n",
    "\n",
    "        self.check_model()\n",
    "\n",
    "        try:\n",
    "            tokens = [i for i in document.split() if i in self.model.wv.vocab]\n",
    "            word_vecs = self.model.wv[tokens]\n",
    "            word_vecs = word_vecs.mean(axis=0).reshape(1, -1)\n",
    "\n",
    "        except Exception as e:\n",
    "            if self.raise_empty_doc_status:\n",
    "                raise(e)\n",
    "            else:\n",
    "                word_vecs = np.zeros(self.model.vector_size).reshape(1, -1)\n",
    "\n",
    "        return word_vecs\n",
    "\n",
    "    def build_doc_vecs(self, pool_workers=None):\n",
    "        self.vecs = self.docs[['id']]\n",
    "\n",
    "        if pool_workers is None:\n",
    "            self.vecs['wvecs'] = self.docs.text.map(self.transform_doc)\n",
    "        else:\n",
    "            pool = mp.Pool(processes=pool_workers)\n",
    "            self.vecs['wvecs'] = pool.map(self.transform_doc, self.docs.text)\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "\n",
    "#         if self.docs.index.max() != (self.docs.shape[0] - 1):\n",
    "#             self.docs = self.docs.reset_index(drop='index')\n",
    "\n",
    "        if self.vecs.index.max() != (self.vecs.shape[0] - 1):\n",
    "            self.vecs = self.vecs.reset_index(drop='index')\n",
    "\n",
    "    def get_similar_documents(self, document, topn=10, return_data='id', return_similarity=False, duplicate_threshold=0.98, show_duplicates=False, serialize=False):\n",
    "        # document: any text\n",
    "        # topn: number of returned related documents in the database\n",
    "        # return_data: string corresponding to a column in the docs or list of column names\n",
    "        # return_similarity: option if similarity scores are to be returned\n",
    "        # duplicate_threhold: threshold that defines a duplicate document based on similarity score\n",
    "        # show_duplicates: option if exact duplicates of documents are to be considered as return documents\n",
    "        self.check_wvecs()\n",
    "\n",
    "        doc_vec = self.transform_doc(document)\n",
    "\n",
    "        sim = cosine_similarity(doc_vec, np.vstack(self.vecs.wvecs)).flatten()\n",
    "\n",
    "        if not show_duplicates:\n",
    "            sim[sim > duplicate_threshold] = 0\n",
    "\n",
    "        payload = []\n",
    "        for rank, top_sim_ix in enumerate(sim.argsort()[-topn:][::-1], 1):\n",
    "            payload.append({'id': self.vecs.iloc[top_sim_ix][return_data], 'score': np.round(sim[top_sim_ix], decimals=5), 'rank': rank})\n",
    "\n",
    "        payload = sorted(payload, key=lambda x: x['rank'])\n",
    "        if serialize:\n",
    "            payload = pd.DataFrame(payload).to_json()\n",
    "\n",
    "        return payload\n",
    "\n",
    "    def get_similar_words(self, document, topn=10, return_similarity=False, serialize=False):\n",
    "\n",
    "        doc_vec = self.transform_doc(document)\n",
    "        sim = cosine_similarity(doc_vec, self.model.wv.vectors).flatten()\n",
    "\n",
    "        payload = []\n",
    "        for rank, top_sim_ix in enumerate(sim.argsort()[-topn:][::-1], 1):\n",
    "            payload.append({'word': self.model.wv.index2word[top_sim_ix], 'score': np.round(sim[top_sim_ix], decimals=5), 'rank': rank})\n",
    "\n",
    "        payload = sorted(payload, key=lambda x: x['rank'])\n",
    "        if serialize:\n",
    "            payload = pd.DataFrame(payload).to_json()\n",
    "\n",
    "        return payload\n",
    "\n",
    "    def get_similar_docs_by_id(self, doc_id, topn=10, return_data='id', return_similarity=False, duplicate_threshold=0.98, show_duplicates=False, serialize=False):\n",
    "        self.check_wvecs()\n",
    "        doc_vec = np.array(self.vecs[self.vecs['id'] == doc_id]['wvecs'].iloc[0]).reshape(1, -1)\n",
    "\n",
    "        sim = cosine_similarity(doc_vec, np.vstack(self.vecs.wvecs)).flatten()\n",
    "\n",
    "        if not show_duplicates:\n",
    "            sim[sim > duplicate_threshold] = 0\n",
    "\n",
    "        payload = []\n",
    "        for rank, top_sim_ix in enumerate(sim.argsort()[-topn:][::-1], 1):\n",
    "            payload.append({'id': self.vecs.iloc[top_sim_ix][return_data], 'score': np.round(sim[top_sim_ix], decimals=5), 'rank': rank})\n",
    "\n",
    "        payload = sorted(payload, key=lambda x: x['rank'])\n",
    "        if serialize:\n",
    "            payload = pd.DataFrame(payload).to_json()\n",
    "\n",
    "        return payload\n",
    "\n",
    "    def get_similar_words_by_id(self, doc_id, topn=10, return_data='id', return_similarity=False, duplicate_threshold=0.98, show_duplicates=False, serialize=False):\n",
    "        self.check_wvecs()\n",
    "        doc_vec = np.array(self.vecs[self.vecs['id'] == doc_id]['wvecs'].iloc[0]).reshape(1, -1)\n",
    "\n",
    "        sim = cosine_similarity(doc_vec, self.model.wv.vectors).flatten()\n",
    "\n",
    "        payload = []\n",
    "        for rank, top_sim_ix in enumerate(sim.argsort()[-topn:][::-1], 1):\n",
    "            payload.append({'word': self.model.wv.index2word[top_sim_ix], 'score': np.round(sim[top_sim_ix], decimals=5), 'rank': rank})\n",
    "\n",
    "        payload = sorted(payload, key=lambda x: x['rank'])\n",
    "        if serialize:\n",
    "            payload = pd.DataFrame(payload).to_json()\n",
    "\n",
    "        return payload\n",
    "\n",
    "    def check_model(self):\n",
    "        if self.model is None:\n",
    "            raise ValueError('Model not trained!')\n",
    "\n",
    "    def check_wvecs(self):\n",
    "        if 'wvecs' not in self.vecs.columns:\n",
    "            raise ValueError('Document vectors not available!')\n",
    "\n",
    "    def rescue_code(self, function):\n",
    "        # http://blog.rtwilson.com/how-to-rescue-lost-code-from-a-jupyteripython-notebook/\n",
    "        import inspect\n",
    "        get_ipython().set_next_input(\"\".join(inspect.getsourcelines(function)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def print_result_links(docs, id, similarity):\n",
    "    doc = docs.doclist[docs.doclist['id'] == id][['id', 'title', 'txt_url', 'pdf_url']].iloc[0]\n",
    "    p = f'id: {doc[\"id\"]} \\ntitle: {doc.title} \\nurl: {doc.txt_url} \\npdf_url: {doc.pdf_url}\\n'    \n",
    "    sim = f'similarity: {100 * similarity:0.2f}%\\n' if similarity is not None else ''\n",
    "\n",
    "    print(p + sim)\n",
    "    \n",
    "\n",
    "def close_docs(docs, doc_id, num_docs, report=False, **kwargs):\n",
    "    dt = kwargs['dt']\n",
    "    ed = euclidean_distances(dt.loc[doc_id].values.reshape(1, -1), dt.values)\n",
    "    doc_ids = dt.index[ed.argsort()[0, 0:num_docs]]\n",
    "    \n",
    "    if report:\n",
    "        for doc_id in doc_ids:\n",
    "            print_result_links(docs, doc_id, None)\n",
    "            \n",
    "    return doc_ids"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}