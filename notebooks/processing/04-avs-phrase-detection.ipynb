{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia-based vocabulary\n",
    "\n",
    "Part of the text cleaning is to recover misspelled tokens in documents. The base toolkit to implement the spell checking component is the [pyenchant](https://github.com/pyenchant/pyenchant) library.\n",
    "\n",
    "While the existing solution works, there are some issues that this implementation face. The most important of which is the detection of emerging or novel words. Recently, Covid has become a common term but the vocabulary of the dictionary that we're using doesn't contain it. This means that when a document containing this term is processed, it will be classified as misspelled and the pipeline will be try to \"fix\" it.\n",
    "\n",
    "To remedy this, we modify the solution by updating the standard vocabulary with the vocabulary from a dynamically updating corpus. In this case, we choose the [Wikipedia corpus](https://dumps.wikimedia.org/enwiki/latest/) as the source of our updated vocabulary.\n",
    "\n",
    "\n",
    "\n",
    "The solution for this is summarized as follows:\n",
    "\n",
    "1. Download the latest wikipedia corpus from https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2.\n",
    "2. Use gensim to process and collect the tokens in the corpus.\n",
    "\n",
    "        ```python\n",
    "        from gensim.corpora import WikiCorpus\n",
    "        from wb_nlp.dir_manager import get_data_dir\n",
    "        import os\n",
    "\n",
    "        wiki_dump = get_data_dir('raw', 'wiki', 'enwiki-latest-pages-articles.xml.bz2')\n",
    "        wiki_dict = get_data_dir('processed', 'wiki')\n",
    "        if not os.path.isdir(wiki_dict):\n",
    "            os.makedirs(wiki_dict)\n",
    "\n",
    "        wiki = WikiCorpus(\n",
    "                wiki_dump, processes=max(1, os.cpu_count() - 4),\n",
    "                lemmatize=False,\n",
    "                article_min_tokens=50, token_min_len=2,\n",
    "                token_max_len=50, lower=True)\n",
    "\n",
    "        wiki.dictionary.save(os.path.join(wiki_dict, 'wiki_en.gensim.dict.pickle'))\n",
    "        ```\n",
    "\n",
    "3. Filter the tokens using the `.cfs` and `.idf` attributes of the `wiki.dictionary`.\n",
    "4. Update use an updated dictionary.\n",
    "\n",
    "        ```\n",
    "        import enchant\n",
    "        en_dict = enchant.DictWithPWL(\"en_US\", \"wiki_en.txt\")\n",
    "        ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deduplicate documents based on hash similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wb_nlp.cleaning import cleaner\n",
    "from wb_nlp.extraction import phrase\n",
    "from wb_nlp import dir_manager\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "text": "\u001b[0;31mSignature:\u001b[0m\n\u001b[0mphrase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_phrases\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mdoc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDoc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mmin_token_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mtoken_func\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mtoken_container\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;31mDocstring:\u001b[0m <no docstring>\n\u001b[0;31mFile:\u001b[0m      /workspace/src/wb_nlp/extraction/phrase.py\n\u001b[0;31mType:\u001b[0m      function\n"
    }
   ],
   "source": [
    "phrase.get_phrases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_path = dir_manager.get_path_from_root('notebooks/archive/SCRIPTS/acronyms/imf_00ae75cce82e5c915d5bead7a7bb2165e9ef215a.txt')\n",
    "\n",
    "with open(txt_path) as fl:\n",
    "    text = fl.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_cleaner = cleaner.LDACleaner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "CPU times: user 3.78 s, sys: 2.66 s, total: 6.44 s\nWall time: 6.69 s\n"
    }
   ],
   "source": [
    "%%time\n",
    "tokens = lda_cleaner.get_tokens(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "CPU times: user 3.3 s, sys: 2.82 s, total: 6.12 s\nWall time: 6.25 s\n"
    }
   ],
   "source": [
    "%%time\n",
    "tokens_and_phrases = lda_cleaner.get_tokens_and_phrases(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(7831, 7831)"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "len(tokens), len(tokens_and_phrases['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "2298"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "len(tokens_and_phrases['phrases'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = phrase.get_phrases(nlp(re.sub('\\s+', ' ', text[:20000])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['second_review',\n 'year_arrangement',\n 'performance_criterion',\n 'staff_report',\n 'staff_team',\n 'economic_development',\n 'information_available',\n 'staff_report',\n 'staff_report',\n 'staff_team',\n 'staff_report',\n 'staff_report',\n 'other_document',\n 'market_sensitive_information',\n 'publication_policy',\n 'reader_comment',\n 'monetary_fund',\n 'other_department',\n 'second_review',\n 'mission_team',\n 'other_senior_government_official',\n 'donor_representative',\n 'amount_equivalent',\n 'first_review',\n 'time_director',\n 'additional_resource',\n 'food_relief_program',\n 'second_review',\n 'debt_relief',\n 'debt_relief',\n 'quantitative_performance_criterion',\n 'structural_performance_criterion',\n 'recent_development',\n 'exchange_rate',\n 'external_tariff',\n 'other_member',\n 'exchange_rate_policy',\n 'regional_level',\n 'proposed_schedule',\n 'common_indicator',\n 'executive_summary',\n 'severe_drought',\n 'strong_recovery',\n 'agriculture_sector',\n 'macroeconomic_performance',\n 'real_growth',\n 'record_agriculture_harvest',\n 'food_price',\n 'inflationary_pressure',\n 'end_year',\n 'food_insecurity',\n 'serious_concern',\n 'food_shortage_due',\n 'high_level',\n 'fiscal_target',\n 'high_revenue',\n 'food_security',\n 'limited_time_available',\n 'year_end',\n 'external_budget_support',\n 'structural_measure',\n 'performance_criterion',\n 'related_measure',\n 'taxpayer_audits',\n 'performance_criterion',\n 'domestic_pricing_mechanism',\n 'petroleum_product',\n 'retail_fuel_price',\n 'margin_payment',\n 'petroleum_company',\n 'domestic_price',\n 'volatile_world_market',\n 'macroeconomic_framework',\n 'food_security_need',\n 'priority_investment_project',\n 'domestic_petroleum_pricing_policy',\n 'real_growth',\n 'agricultural_output_return',\n 'end_period_basis',\n 'recent_decline',\n 'food_price',\n 'food_security',\n 'additional_donor_pledge',\n 'budgetary_resource',\n 'ment_project',\n 'food_security',\n 'medium_term',\n 'additional_resource',\n 'poverty_reduction_strategy_paper',\n 'domestic_resource',\n 'fiscal_program',\n 'food_security_need',\n 'fiscal_deficit',\n 'original_program',\n 'high_spending',\n 'food_security',\n 'high_financing_requirement',\n 'domestic_financing',\n 'unspent_resource',\n 'debt_relief',\n 'new_measure',\n 'fiscal_revenue',\n 'precise_policy',\n 'domestic_petroleum_pricing',\n 'structural_performance_criterion',\n 'timely_implementation',\n 'real_gdp',\n 'annual_percentage_change',\n 'right_scale',\n 'left_scale',\n 'metric_tonne',\n 'cereal_price_index',\n 'right_scale',\n 'inf_lation',\n 'serious_social',\n 'economic_challenge',\n 'combined_effect',\n 'locust_infestation',\n 'food_crisis',\n 'fuel_price',\n 'household_income',\n 'poor_socio_economic_condition',\n 'government_attempt',\n 'vat_exemption',\n 'key_food_item',\n 'widespread_social_unrest',\n 'avian_flu',\n 'fourth_quarter',\n 'cereal_harvest',\n 'cereal_price',\n 'pre_drought_level',\n 'overall_inflationary_pressure',\n 'external_current_account_deficit',\n 'export_receipt',\n 'high_uranium',\n 'gold_price',\n 'high_import',\n 'average_real_effective_exchange_rate',\n 'annual_percentage_change',\n 'right_scale',\n 'quantitative_performance_criterion',\n 'fiscal_revenue',\n 'high_trade',\n 'buoyant_import',\n 'food_relief',\n 'tax_administration',\n 'overall_expenditure',\n 'food_security',\n 'limited_time_available',\n 'year_end',\n 'external_budget_support',\n 'coordination_constraint',\n 'external_budget_support',\n 'immediate_need',\n 'food_security_spending',\n 'recent_good_harvest',\n 'overall_fiscal_deficit',\n 'net_domestic_financing',\n 'structural_reform',\n 'structural_performance_criterion',\n 'main_custom_office',\n 'joint_import_verification_team',\n 'petroleum_pricing_mechanism',\n 'domestic_fuel_price',\n 'margin_payment',\n 'structural_performance_criterion',\n 'turnover_threshold',\n 'large_taxpayer',\n 'medium_sized_taxpayer',\n 'targeted_number',\n 'recent_creation',\n 'audit_office',\n 'further_progress',\n 'expenditure_management',\n 'financial_sector',\n 'fiscal_target',\n 'entire_amount',\n 'food_security',\n 'margin_payment',\n 'key_macroeconomic_policy',\n 'macroeconomic_framework',\n 'food_security',\n 'extent_possible',\n 'unfinanced_priority_investment_project',\n 'revenue_performance',\n 'expenditure_management',\n 'domestic_petroleum_product',\n 'debt_sustainability',\n 'other_structural_reform',\n 'agricultural_production',\n 'recent_decline',\n 'domestic_food_price',\n 'year_inflation',\n 'external_current_account_deficit',\n 'food_security_increase',\n 'exchange_rate_policy',\n 'regional_level',\n 'exchange_rate_peg',\n 'inflation_low',\n 'medium_term_outlook',\n 'previous_staff',\n 'constant_price',\n 'current_account',\n 'fiscal_balance',\n 'cash_basis',\n 'external_public_debt',\n 'staff_estimate',\n 'annual_percentage_change',\n 'fiscal_program',\n 'food_security_need',\n 'new_priority_investment_project',\n 'economic_growth',\n 'new_real_estate_tax',\n 'total_revenue',\n 'total_expenditure',\n 'net_lending',\n 'basic_balance',\n 'MDRI_assistance',\n 'domestic_financing',\n 'financing_gap',\n 'financing_gap',\n 'staff_estimate']"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('This is the University of in the Philippines, Diliman.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('this', 'DET', ''),\n ('be', 'AUX', ''),\n ('the', 'DET', 'ORG'),\n ('University', 'PROPN', 'ORG'),\n ('of', 'ADP', 'ORG'),\n ('in', 'ADP', 'ORG'),\n ('the', 'DET', ''),\n ('Philippines', 'PROPN', 'GPE'),\n (',', 'PUNCT', ''),\n ('Diliman', 'PROPN', 'PERSON'),\n ('.', 'PUNCT', '')]"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "[(t.lemma_, t.pos_, t.ent_type_) for t in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[]"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "phrase.get_phrases(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wb_nlp",
   "language": "python",
   "name": "wb_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}