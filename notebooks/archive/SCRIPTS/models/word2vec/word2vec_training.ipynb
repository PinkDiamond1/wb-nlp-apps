{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for training word2vec models for IMF and WB corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Word2VecModel\n",
    "%run ./Word2vecModule.ipynb\n",
    "\n",
    "# DocsManager\n",
    "# build_docs\n",
    "%run ../../DocsManager.ipynb\n",
    "## Jupyter.notebook.save_checkpoint()\n",
    "\n",
    "# get_corpus_path\n",
    "# get_txt_clean_path\n",
    "%run ../../path_manager.ipynb\n",
    "\n",
    "# CorpusCleaner\n",
    "%run ../../DataCleanerModule.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_PATH = get_models_path('WORD2VEC')\n",
    "NUM_WORKERS = max(1, os.cpu_count() - 4)\n",
    "NUM_ITERS = 10\n",
    "MIN_TOKEN_COUNT = 50\n",
    "NGRAM_FILE = '../../whitelists/whitelist_ngrams_cleaned.csv'  # '../../whitelists/whitelist_ngrams_truncated_cleaned.csv'\n",
    "DOC_PROCESSING_WORKERS = max(1, os.cpu_count() - 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(MODELS_PATH):\n",
    "    os.makedirs(MODELS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import gc\n",
    "\n",
    "TRAINING_MODEL_ID = 'WORD2VEC'\n",
    "\n",
    "logging.basicConfig(filename=f'./{TRAINING_MODEL_ID.lower()}-iters_{NUM_ITERS}.log', format='%(asctime)s - %(levelname)s - %(message)s', level=logging.INFO)\n",
    "\n",
    "logger = logging.getLogger(f'{TRAINING_MODEL_ID.lower()}-logger')\n",
    "# logger.setLevel(logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/wb536061/wbes2474/NLP/MODELS/WORD2VEC'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODELS_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(207647, 3)\n"
     ]
    }
   ],
   "source": [
    "# for CORPUS_ID in ['IMF', 'WB']:\n",
    "for CORPUS_ID in ['WB']:\n",
    "    if CORPUS_ID == 'WB':\n",
    "        region_partitions = []  # ['AFR', 'EAP', 'ECA', 'LAC', 'MENA', 'RoW', 'SAR', 'WLD']\n",
    "        doctype_partitions = []  # ['BD', 'CF', 'ESW', 'PD', 'PR']\n",
    "        corpus_partitions = ['ALL'] + doctype_partitions + region_partitions\n",
    "    else:\n",
    "        corpus_partitions = ['ALL']\n",
    "\n",
    "    # num_topics = [50, 100]\n",
    "    num_topics = [128]\n",
    "\n",
    "    for CORPUS_PART in corpus_partitions:\n",
    "        docs = build_docs(\n",
    "            metadata_filename=os.path.join(get_corpus_path(CORPUS_ID), f'{CORPUS_ID.lower()}_metadata_complete.csv'),\n",
    "            cleaned_files_dir=get_txt_clean_path(CORPUS_ID),\n",
    "            model_output_dir=MODELS_PATH  # Use flat directory as discussed...\n",
    "        )\n",
    "\n",
    "        logger.info('Creating partitioned docs and loading files...')\n",
    "\n",
    "        # docs.set_ngram_mapper('../../whitelists/whitelist_ngrams_cleaned.csv', cleaner=None)\n",
    "        docs.set_ngram_mapper(NGRAM_FILE, cleaner=None)\n",
    "\n",
    "        docs.set_min_token_count(MIN_TOKEN_COUNT)\n",
    "        docs_filtered, meta = docs.filter_doclist(CORPUS_PART, CORPUS_ID, save=True, return_meta=True, pool_workers=DOC_PROCESSING_WORKERS)\n",
    "\n",
    "        print(docs_filtered.shape)\n",
    "        logger.info(f'Building model for {docs_filtered.shape[0]} documents...')\n",
    "        if docs_filtered.empty:\n",
    "            continue\n",
    "            \n",
    "        for NUM_TOPICS in num_topics:\n",
    "#             if (CORPUS_ID == 'WB') and (NUM_TOPICS == 50) and (CORPUS_PART == 'ALL'):\n",
    "#                 continue\n",
    "\n",
    "            MODEL_ID = f\"{CORPUS_PART}_{NUM_TOPICS}\"\n",
    "            MODEL_FOLDER = os.path.join(MODELS_PATH, f'{CORPUS_ID}-{MODEL_ID}')\n",
    "\n",
    "            MODEL_DATA_FOLDER = os.path.join(MODEL_FOLDER, 'data')\n",
    "\n",
    "            if not os.path.isdir(MODEL_DATA_FOLDER):\n",
    "                os.makedirs(MODEL_DATA_FOLDER)\n",
    "\n",
    "            # Set logging\n",
    "            lfh = logging.FileHandler(f'./{CORPUS_ID.lower()}-iters_{NUM_ITERS}-{MODEL_ID}.log')\n",
    "            lfh.setLevel(logging.INFO)\n",
    "            formatter = logging.Formatter('%(asctime)s %(levelname)-8s %(message)s')\n",
    "            lfh.setFormatter(formatter)\n",
    "            logger.addHandler(lfh)\n",
    "            # End logging setup\n",
    "\n",
    "            logger.info('Creating word2vec model...')\n",
    "\n",
    "            w2vec_model = Word2VecModel(\n",
    "                doc_df=docs_filtered,\n",
    "                corpus_id=CORPUS_ID,\n",
    "                model_id=MODEL_ID,\n",
    "                dim=NUM_TOPICS,\n",
    "                workers=NUM_WORKERS,\n",
    "                model_path=MODELS_PATH,\n",
    "                optimize_interval=0,\n",
    "                iter=NUM_ITERS\n",
    "            )\n",
    "\n",
    "            logger.info('Starting model training...')\n",
    "            w2vec_model.train_model()\n",
    "\n",
    "            logger.info('Starting document vectors creation...')\n",
    "            w2vec_model.build_doc_vecs(pool_workers=None)\n",
    "\n",
    "            logger.info('Saving model...')\n",
    "            w2vec_model.save_model()\n",
    "\n",
    "            logger.info('Saving model and vectors...')\n",
    "            w2vec_model.save()\n",
    "\n",
    "            logger.info(f'word2vec model for {CORPUS_ID} completed: {MODEL_ID}...')\n",
    "            logger.removeHandler(lfh)\n",
    "\n",
    "            w2vec_model.clear()\n",
    "            del(w2vec_model)\n",
    "            gc.collect()\n",
    "\n",
    "        del(docs_filtered)\n",
    "        del(docs.doclist)\n",
    "        del(docs)\n",
    "        del(meta)\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WB'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CORPUS_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/wb536061/wbes2474/NLP/MODELS/WORD2VEC'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODELS_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2vec_model.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}