{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import importlib\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from wb_nlp.interfaces import mongodb\n",
    "from wb_nlp.types import metadata as meta_type\n",
    "from wb_nlp.types import metadata_enums\n",
    "from wb_nlp import dir_manager\n",
    "\n",
    "importlib.reload(meta_type)\n",
    "importlib.reload(metadata_enums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# World Bank Metadata\n",
    "\n",
    "Load the metadata from the API scraper notebook: `wb_nlp/notebooks/scrapers/wb_api_scraper_latest.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fname = f\"wb_metadata-{datetime.now()}.csv\"\n",
    "# fname = \"wb_metadata-2021-03-27 19:17:44.764136.csv\"\n",
    "# wb_metadata = pd.read_csv(dir_manager.get_data_dir(\"corpus\", \"WB\", fname))\n",
    "\n",
    "# collection = mongodb.get_collection(\"test_nlp\", \"docs_metadata\")\n",
    "# errors_list = []\n",
    "# no_url = []\n",
    "# metadata_list = []\n",
    "# for i, row in wb_metadata.iterrows():\n",
    "#     if i % 10000 == 0:\n",
    "#         print(i)\n",
    "\n",
    "#     row[\"_id\"] = row[\"id\"]\n",
    "\n",
    "#     if row[\"url_txt\"] == \"not to be displayed--\":\n",
    "#         print(row[\"id\"])\n",
    "#         no_url.append(row)\n",
    "#         continue\n",
    "\n",
    "#     try:\n",
    "#         meta = json.loads(meta_type.make_metadata_model_from_nlp_schema(row.fillna(\"\")).json())\n",
    "#         meta[\"_id\"] = meta[\"id\"]\n",
    "#         metadata_list.append(meta)\n",
    "#     except:\n",
    "#         errors_list.append(row)\n",
    "\n",
    "# collection.insert_many(metadata_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"wb_metadata-2021-03-27 19:17:44.764136.csv\"\n",
    "wb_metadata = pd.read_csv(dir_manager.get_data_dir(\"corpus\", \"WB\", fname))\n",
    "\n",
    "collection = mongodb.get_collection(\"test_nlp\", \"docs_metadata\")\n",
    "errors_list = []\n",
    "no_url = []\n",
    "metadata_list = []\n",
    "ids = set()\n",
    "dup_ids = set()\n",
    "for i, row in wb_metadata.iterrows():\n",
    "    if i % 10000 == 0:\n",
    "        print(i)\n",
    "    row[\"_id\"] = row[\"id\"]\n",
    "    if row[\"url_txt\"] == \"not to be displayed--\":\n",
    "        print(row[\"id\"])\n",
    "        no_url.append(row)\n",
    "        continue\n",
    "    try:\n",
    "        meta = json.loads(meta_type.make_metadata_model_from_nlp_schema(row.fillna(\"\")).json())\n",
    "        if meta[\"path_original\"] is None or \"TXT_ORIG\" in meta[\"path_original\"]:\n",
    "            meta[\"path_original\"] = meta[\"path_original\"]\n",
    "        else:\n",
    "            p = Path(meta[\"path_original\"])\n",
    "            meta[\"path_original\"] = str(p.parent / \"TXT_ORIG\" / p.name)\n",
    "        meta[\"_id\"] = meta[\"id\"]\n",
    "        if meta[\"_id\"] in ids:\n",
    "            dup_ids.add(meta[\"_id\"])\n",
    "            continue\n",
    "        metadata_list.append(meta)\n",
    "        ids.add(meta[\"_id\"])\n",
    "    except:\n",
    "        errors_list.append(row)\n",
    "print(len(ids))\n",
    "filter_obj = {\"id\": {\"$in\": list(ids)}}\n",
    "del_res = collection.delete_many(filter_obj)\n",
    "ins_res = collection.insert_many(metadata_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_nlp_docs_from_docs_metadata(docs_metadata):\n",
    "    root_path = Path(dir_manager.get_path_from_root())\n",
    "    for ix, data in enumerate(docs_metadata):\n",
    "        if ix and ix % 10000 == 0:\n",
    "            print(ix)\n",
    "        doc_path = root_path / data[\"path_original\"]\n",
    "        if not doc_path.exists():\n",
    "            continue\n",
    "        # create and save and article\n",
    "        nlp_doc = elasticsearch.NLPDoc(meta={'id': data[\"_id\"]}, **data)\n",
    "        with open(doc_path, 'rb') as open_file:\n",
    "            doc = open_file.read().decode('utf-8', errors='ignore')\n",
    "            nlp_doc.body = doc\n",
    "        nlp_doc.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wb_nlp.interfaces import elasticsearch\n",
    "\n",
    "es = elasticsearch.get_client()\n",
    "es.indices.refresh(index_name)\n",
    "es.cat.count(index_name, params={\"format\": \"json\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes and scratch\n",
    "\n",
    "1. Copy raw data from sandbox server to gw1. Then copy to server7. Make sure the files are in the proper format <corpus_id>_<doc_id>.*\n",
    "2. Extract covers of pdf.\n",
    "3. Run filter by language script.\n",
    "4. Set up config for the cleaning and the models. Then load the configs to db.\n",
    "5. Define the scripts to run the models in model_runs/<model_name>/<model_name>-run-<run_number>.sh and also create the cleaning script (run_<model_name__cleaning.sh) for the intended model configuration.\n",
    "6. Execute cleaning and traning: bash run_<model_name__cleaning.sh && bash model_runs/<model_name>/<model_name>-run-<run_number>.sh\n",
    "7. Update permission of the data and models directories (inside the container).\n",
    "8. Tar the needed files and models.\n",
    "9. rsync to gw1, then rsync to sandbox/dev server.\n",
    "10. copy files to local directories, i.e., data/ and models/.\n",
    "11. Load the metadata to db.\n",
    "12. Make a softlink from EN_TXT_ORIG to TXT_ORIG\n",
    "13. Build the document vectors.\n",
    "14. Dump the documents to elasticsearch.\n",
    "15. Update the default model ids in app/app_kcp/src/config.js\n",
    "\n",
    "##############################\n",
    "\n",
    "https://stackoverflow.com/a/44683248\n",
    "\n",
    "```FROM ubuntu:xenial-20170214\n",
    "ARG UNAME=testuser\n",
    "ARG UID=1000\n",
    "ARG GID=1000\n",
    "RUN groupadd -g $GID -o $UNAME\n",
    "RUN useradd -m -u $UID -g $GID -o -s /bin/bash $UNAME\n",
    "USER $UNAME\n",
    "CMD /bin/bash\n",
    "\n",
    "\n",
    "docker build --build-arg UID=$(id -u) --build-arg GID=$(id -g) \\\n",
    "  -f bb.dockerfile -t testimg .\n",
    "```\n",
    "\n",
    "##############################\n",
    "\n",
    "```sudo docker-compose -f docker-compose.yml -f docker-compose.w0lxsnlp01.yml up -d --no-deps --build tika\n",
    "sudo docker-compose -f docker-compose.yml -f docker-compose.w0lxsnlp01.yml up -d --no-deps --build redis\n",
    "sudo docker-compose -f docker-compose.yml -f docker-compose.w0lxsnlp01.yml up -d --no-deps --build mongodb\n",
    "sudo docker-compose -f docker-compose.yml -f docker-compose.w0lxsnlp01.yml up -d --no-deps --build milvus\n",
    "\n",
    "sudo docker-compose -f docker-compose.yml -f docker-compose.w0lxsnlp01.yml up -d --no-deps --build nlp_api\n",
    "sudo docker-compose -f docker-compose.yml -f docker-compose.w0lxsnlp01.yml up -d --no-deps --build app_kcp\n",
    "\n",
    "Step 1: Copy nlp-metadata-wbes2474-20201007.json to data/raw\n",
    "Step 2: Place sample data in data/raw/sample_data/TXT_ORIG\n",
    "\n",
    "sudo docker exec -it wb_nlp_nlp_api_1 /bin/sh\n",
    "\n",
    "run scripts\n",
    "\n",
    "###################################\n",
    "To copy data from sandbox to gw1\n",
    "\n",
    "python ~/kcp-scripts/copy_raw_data.py\n",
    "\n",
    "# Manually rsync data for ADB\n",
    "rsync -avP w0lxsnlp01:/decfile2/Modeling/NLP/ihsn_scrapers/scrapers/adb/adb_files/full /Documentum/Aivin/corpus/ADB/\n",
    "mv /Documentum/Aivin/corpus/ADB/full /Documentum/Aivin/corpus/ADB/PDF_ORIG\n",
    "\n",
    "\n",
    "###################################\n",
    "Generate wikipedia vocab\n",
    "Download data from: https://dumps.wikimedia.org/enwiki/latest/\n",
    "enwiki-latest-pages-articles.xml.bz2: https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2\n",
    "\n",
    "cd /data/wb536061/wikipedia-word-frequency\n",
    "./gather_wordfreq.py ../wb_nlp/data/external/wikipedia/enwiki-latest-pages-articles.xml.bz2 > wordfreq-enwiki-latest-pages-articles.xml.bz2.txt\n",
    "\n",
    "\n",
    "sudo /usr/local/bin/docker-compose -f docker-compose.yml -f docker-compose.server7.yml up -d --no-deps --build tika\n",
    "sudo /usr/local/bin/docker-compose -f docker-compose.yml -f docker-compose.server7.yml up -d --no-deps --build redis\n",
    "sudo /usr/local/bin/docker-compose -f docker-compose.yml -f docker-compose.server7.yml up -d --no-deps --build mongodb\n",
    "sudo /usr/local/bin/docker-compose -f docker-compose.yml -f docker-compose.server7.yml up -d --no-deps --build milvus\n",
    "\n",
    "sudo /usr/local/bin/docker-compose -f docker-compose.yml -f docker-compose.server7.yml up -d --no-deps --build nlp_api\n",
    "sudo /usr/local/bin/docker-compose -f docker-compose.yml -f docker-compose.server7.yml up -d --no-deps --build app_kcp\n",
    "\n",
    "sudo docker exec -it wb_nlp_nlp_api_1 /bin/bash\n",
    "\n",
    "python -u ./scripts/cleaning/convert_pdf2txt_corpus.py --input-dir ./data/corpus --recursive -vv |& tee ./logs/convert_pdf2txt_corpus.py.log\n",
    "\n",
    "bash run_word2vec_cleaning.sh && bash model_runs/word2vec/word2vec-run-01.sh\n",
    "\n",
    "### Build a tar of word2vec model and WB data\n",
    "tar -czf KCP_WB_DATA_MODELS.tar.gz data/corpus/WB/COVER data/corpus/WB/wb_metadata-2021-03-27\\ 19\\:17\\:44.764136.csv data/corpus/WB/EN_TXT_ORIG data/corpus/cleaned/229abf370f281efa7c9f3c4ddc20159d/WB models/word2vec models/lda\n",
    "\n",
    "from wb_nlp.models import word2vec_base\n",
    "import logging\n",
    "\n",
    "wvec_model = word2vec_base.Word2VecModel(\n",
    "    model_config_id=\"702984027cfedde344961b8b9461bfd3\",\n",
    "    cleaning_config_id=\"229abf370f281efa7c9f3c4ddc20159d\",\n",
    "    model_run_info_id=\"854ae5f9cdda093265212c435d1ddfd4\",\n",
    "    raise_empty_doc_status=False,\n",
    "    log_level=logging.DEBUG)\n",
    "\n",
    "wvec_model.build_doc_vecs(pool_workers=22)\n",
    "\n",
    "\n",
    "###################### WDI\n",
    "import pandas as pd\n",
    "import logging\n",
    "from wb_nlp import dir_manager\n",
    "from wb_nlp.models import word2vec_base\n",
    "\n",
    "\n",
    "wvec_model = word2vec_base.Word2VecModel(\n",
    "    model_config_id=\"702984027cfedde344961b8b9461bfd3\",\n",
    "    cleaning_config_id=\"229abf370f281efa7c9f3c4ddc20159d\",\n",
    "    model_run_info_id=\"854ae5f9cdda093265212c435d1ddfd4\",\n",
    "    raise_empty_doc_status=False,\n",
    "    log_level=logging.DEBUG)\n",
    "\n",
    "wdi_df = pd.read_csv(dir_manager.get_data_dir(\n",
    "    \"preprocessed\", \"timeseries\", \"wdi_time_series_metadata.csv\"))\n",
    "\n",
    "wdi_df[\"text\"] = wdi_df[\"txt_meta\"].map(wvec_model.clean_text)\n",
    "\n",
    "wdi_df[\"vector\"] = wdi_df.apply(\n",
    "    lambda x: wvec_model.process_doc(x)[\"doc_vec\"], axis=1)\n",
    "\n",
    "wdi_df.to_pickle(\n",
    "    f\"/workspace/models/wdi/wdi_time_series_metadata-{wvec_model.model_id}.pickle\")```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wb_nlp",
   "language": "python",
   "name": "wb_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
