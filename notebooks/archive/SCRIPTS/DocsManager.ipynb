{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"text-align:center\">World Bank Documents and Reports Manager</div>\n",
    "\n",
    "This notebook implements the manager class for the data from the **Documents and Reports API**. This document manager provides filtering options for the documents in the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from ngrams.ngrams import NGramMapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class DocsManager:\n",
    "    # Corpus by region\n",
    "    adm_region_region_map = {\n",
    "        'Africa': 'AFR',\n",
    "        'East Asia and Pacific':'EAP',\n",
    "        'Europe and Central Asia': 'ECA',\n",
    "        'Latin America & Caribbean': 'LAC',\n",
    "        'Middle East and North Africa': 'MENA',\n",
    "        'Rest Of The World': 'RoW',\n",
    "        'South Asia': 'SAR',\n",
    "        'The world Region': 'WLD',\n",
    "        np.nan: 'M_U'\n",
    "    }\n",
    "\n",
    "    # Corpus by document type\n",
    "    major_doc_type_mdtype_map = {\n",
    "        '': 'N.D',\n",
    "        'Board Documents': 'BD',\n",
    "        'Country Focus': 'CF',\n",
    "        'Economic & Sector Work': 'ESW',\n",
    "        'Project Documents': 'PD',\n",
    "        'Publications & Research': 'PR',\n",
    "        np.nan: 'N.D'\n",
    "    }\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        metadata_filename, \n",
    "        cleaned_files_dir,\n",
    "        model_output_dir,\n",
    "        from_year=1950,\n",
    "        to_year=pd.datetime.now().year,  # Get the current year\n",
    "        min_token_count = 100,\n",
    "        ngram_whitelist_file=None\n",
    "    ):\n",
    "        '''\n",
    "        metadata_filename: full path to the metadata file generated from scraping the docs api\n",
    "        cleaned_files_output_dir: full path to the output directory of the cleaning pipeline\n",
    "        model_output_dir: full path to the output directory where models will be stored\n",
    "        from_year: default minimum year filter on the publication date\n",
    "        to_year: default maximum year filter on the publication date\n",
    "        min_token_count: threshold for the minimum number of useful words in the document\n",
    "        '''\n",
    "        \n",
    "        self.metadata_filename = metadata_filename\n",
    "        self.cleaned_files_dir = cleaned_files_dir\n",
    "        self.model_output_dir = model_output_dir\n",
    "        self.set_ngram_mapper(ngram_whitelist_file)\n",
    "                \n",
    "        self.doclist = pd.read_csv(self.metadata_filename, low_memory=False, index_col=0)\n",
    "        self.doclist.index.name = 'id'\n",
    "        self.doclist = self.doclist.reset_index()\n",
    "        \n",
    "        self.doclist['region'] = self.doclist.adm_region.map(self.adm_region_region_map).fillna('M_U')\n",
    "        self.doclist['mdtype'] = self.doclist.major_doc_type.map(self.major_doc_type_mdtype_map).fillna('N.D')\n",
    "        \n",
    "        self.corpus_parts = (\n",
    "            ['ALL'] + \n",
    "            [i for i in self.adm_region_region_map.values()] + \n",
    "            [i for i in self.major_doc_type_mdtype_map.values()]\n",
    "        )\n",
    "        \n",
    "        # Selected parameters to apply to all models\n",
    "        self.set_from_year(from_year)\n",
    "        self.set_to_year(to_year)\n",
    "        self.set_min_token_count(min_token_count)\n",
    "        \n",
    "    def set_from_year(self, from_year):\n",
    "        self.from_year = from_year\n",
    "    \n",
    "    def set_to_year(self, to_year):\n",
    "        self.to_year = to_year\n",
    "    \n",
    "    def set_ngram_mapper(self, ngram_whitelist_file, cleaner=None):\n",
    "        self.ngram_mapper = NGramMapper(whitelist_file=ngram_whitelist_file, cleaner=cleaner) if ngram_whitelist_file is not None else None\n",
    "\n",
    "    def set_min_token_count(self, min_token_count):\n",
    "        self.min_token_count = min_token_count\n",
    "        \n",
    "    def load_text(self, path):\n",
    "        if os.path.isfile(path):\n",
    "            try:\n",
    "                with open(path) as fl:\n",
    "                    data = fl.read()\n",
    "            except Exception as e1:\n",
    "                try:\n",
    "                    with open(path, 'rb') as fl:\n",
    "                        data = fl.read().decode('utf-8', errors='ignore')\n",
    "                except:\n",
    "                    raise(e1)\n",
    "        else:\n",
    "            data = None\n",
    "        \n",
    "        if self.ngram_mapper is not None:\n",
    "            data = self.ngram_mapper.replace_ngrams(data)\n",
    "\n",
    "        return data\n",
    "    \n",
    "    def filter_doclist(self, corpus_part, corpus_id, docs_filtered=None, verbose=False, save=False, return_meta=False, pool_workers=None):\n",
    "        if docs_filtered is None:\n",
    "            docs_filtered = self.doclist.copy()\n",
    "        else:\n",
    "            docs_filtered = docs_filtered.copy()\n",
    "        docs_filtered = docs_filtered.reset_index()\n",
    "\n",
    "        if corpus_part in self.major_doc_type_mdtype_map.values():\n",
    "            docs_filtered = docs_filtered[docs_filtered.mdtype == corpus_part]\n",
    "        if corpus_part in self.adm_region_region_map.values():\n",
    "            docs_filtered = docs_filtered[docs_filtered.region == corpus_part]\n",
    "\n",
    "        dfr_params = ['id', 'title', 'author', 'digital_identifier', 'language_detected', 'year']  # Note: `guid` removed\n",
    "\n",
    "        docs_filtered = docs_filtered[\n",
    "            (docs_filtered.year.between(\n",
    "                self.from_year, self.to_year\n",
    "            )) &\n",
    "            (docs_filtered.language_src == 'English') &\n",
    "            (docs_filtered.language_detected == 'en') &\n",
    "            (docs_filtered.language_score >= 0.98) &\n",
    "            (docs_filtered.tokens >= self.min_token_count)\n",
    "        ][dfr_params]  # Add pages here\n",
    "\n",
    "        docs_filtered['author'] = docs_filtered['author'].fillna('[Anon.]')\n",
    "        docs_filtered['pages'] = \"\"\n",
    "\n",
    "        if verbose:\n",
    "            print(f'Number of documents in selected corpus: {docs_filtered.shape[0]}')\n",
    "\n",
    "        file_name = os.path.join(self.model_output_dir, f'{corpus_id.lower()}-meta_{corpus_part}.csv')\n",
    "        if save:\n",
    "            docs_filtered.to_csv(\n",
    "                file_name,\n",
    "                index=False, header=False\n",
    "            )\n",
    "\n",
    "        if return_meta:\n",
    "            meta = docs_filtered[docs_filtered.columns]\n",
    "\n",
    "        docs_filtered = docs_filtered[['id']]\n",
    "        docs_filtered['filename'] = docs_filtered['id'].map(\n",
    "            lambda x: os.path.join(self.cleaned_files_dir, f'{x}.txt')\n",
    "        )\n",
    "\n",
    "        # Create a new column for text data and add content of text files in it \n",
    "        docs_filtered[\"text\"] = 'not-set'\n",
    "\n",
    "        if pool_workers is None:\n",
    "            docs_filtered['text'] = docs_filtered.filename.map(self.load_text)\n",
    "        else:\n",
    "            pool = mp.Pool(processes=pool_workers)\n",
    "            docs_filtered['text'] = pool.map(self.load_text, docs_filtered.filename)\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "            \n",
    "        docs_filtered = docs_filtered.dropna(subset=['text'])\n",
    "        \n",
    "        if return_meta:\n",
    "            return docs_filtered, meta\n",
    "        else:\n",
    "            return docs_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def build_docs(metadata_filename, cleaned_files_dir, model_output_dir):\n",
    "    docs = DocsManager(\n",
    "        metadata_filename=metadata_filename,\n",
    "        cleaned_files_dir=cleaned_files_dir,\n",
    "        model_output_dir=model_output_dir\n",
    "    )\n",
    "    \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Use as:\n",
    "\n",
    "%%capture\n",
    "%run <path to WBDocs>/WBdDocs.ipybn\n",
    "\n",
    "# model_output_dir = './'  # '../../data/model/'\n",
    "# cleaned_files_output_dir = '../cleaner/processed_files'  # '../../data/cleaner/cleaned_lemmatized_respelled'\n",
    "# metadata_file = pd.read_csv('../cleaner/complete_metadata_file.csv')  # '../../data/scraper/WBCorpus_normalized_metadata_05-31-2018.csv')\n",
    "\n",
    "wbdocs = WBDocs(\n",
    "    metadata_filename='../cleaner/complete_metadata_file.csv', \n",
    "    cleaned_files_output_dir='../cleaner/processed_files',\n",
    "    model_output_dir='./'\n",
    ")\n",
    "\n",
    "#     def train(self, corpus):\n",
    "#         self.convert_input(corpus, infer=False)\n",
    "#         cmd = self.mallet_path + ' train-topics --input %s --num-topics %s  --alpha %s --optimize-interval %s '\\\n",
    "#             '--num-threads %s --output-state %s --output-doc-topics %s --output-topic-keys %s '\\\n",
    "#             '--num-iterations %s --inferencer-filename %s --doc-topics-threshold %s'\n",
    "#         cmd = cmd % (\n",
    "#             self.fcorpusmallet(), self.num_topics, self.alpha, self.optimize_interval,\n",
    "#             self.workers, self.fstate(), self.fdoctopics(), self.ftopickeys(), self.iterations,\n",
    "#             self.finferencer(), self.topic_threshold\n",
    "#         )\n",
    "#         # NOTE \"--keep-sequence-bigrams\" / \"--use-ngrams true\" poorer results + runs out of memory\n",
    "#         logger.info(\"training MALLET LDA with %s\", cmd)\n",
    "#         check_output(args=cmd, shell=True)\n",
    "#         self.word_topics = self.load_word_topics()\n",
    "#         # NOTE - we are still keeping the wordtopics variable to not break backward compatibility.\n",
    "#         # word_topics has replaced wordtopics throughout the code;\n",
    "#         # wordtopics just stores the values of word_topics when train is called.\n",
    "#         self.wordtopics = self.word_topics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}